{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d2d064",
   "metadata": {},
   "source": [
    "# Hackable AlphaFold 3 without Docker or MSAs!\n",
    "\n",
    "This Jupyter notebook provides a lightweight, hackable way to run AlphaFold 3. Experiment with structure prediction directly on your laptop or single GPU server, without the overhead of massive MSA databases or Docker. Just define your sequences and start predicting!\n",
    "\n",
    "**This notebook allows you to:**\n",
    "- Easily change input parameters and the molecular system definition.\n",
    "- Step through the configuration and execution.\n",
    "- Inspect intermediate variables by adding print statements or debugging code.\n",
    "- Choose whether to run the data pipeline, inference, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02267b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Current XLA_FLAGS:** ` --xla_gpu_enable_triton_gemm=false --xla_gpu_enable_triton_gemm=false`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Note:** If you need to change `XLA_FLAGS` for your GPU, it's best to set it in your shell *before* starting Jupyter or restart the kernel after setting it with `os.environ`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Environment Variables Reminder\n",
    "# IMPORTANT: Set necessary environment variables for JAX/XLA before running.\n",
    "# These are usually set in your shell environment *before* launching Jupyter.\n",
    "# If you need to set them for the current session (less ideal), you can use os.environ:\n",
    "\n",
    "import os\n",
    "\n",
    "# Example for NVIDIA A100/H100 (Compute Capability 8.0+):\n",
    "os.environ['XLA_FLAGS'] = os.environ.get('XLA_FLAGS', \"\") + \" --xla_gpu_enable_triton_gemm=false\"\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"true\"\n",
    "os.environ['XLA_CLIENT_MEM_FRACTION'] = \"0.95\"\n",
    "\n",
    "# Example for NVIDIA V100 (Compute Capability 7.x):\n",
    "# os.environ['XLA_FLAGS'] = os.environ.get('XLA_FLAGS', \"\") + \" --xla_disable_hlo_passes=custom-kernel-fusion-rewriter\"\n",
    "\n",
    "# It's best to set these *before* JAX initializes its backend.\n",
    "# Restart the kernel if you change these here and JAX has already been imported.\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"**Current XLA_FLAGS:** `{os.environ.get('XLA_FLAGS')}`\"))\n",
    "display(Markdown(\n",
    "    \"**Note:** If you need to change `XLA_FLAGS` for your GPU, \"\n",
    "    \"it's best to set it in your shell *before* starting Jupyter \"\n",
    "    \"or restart the kernel after setting it with `os.environ`.\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408d7a1",
   "metadata": {},
   "source": [
    "# Imports and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45594e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import datetime\n",
    "import functools\n",
    "import json\n",
    "import textwrap\n",
    "import time\n",
    "import csv\n",
    "import dataclasses\n",
    "from typing import Sequence, Callable, overload\n",
    "\n",
    "# AlphaFold 3 specific imports\n",
    "from alphafold3.common import folding_input\n",
    "from alphafold3.common import resources\n",
    "from alphafold3.constants import chemical_components\n",
    "import alphafold3.cpp\n",
    "from alphafold3.data import featurisation\n",
    "from alphafold3.data import pipeline\n",
    "from alphafold3.jax.attention import attention\n",
    "from alphafold3.model import features\n",
    "from alphafold3.model import model\n",
    "from alphafold3.model import params\n",
    "from alphafold3.model import post_processing\n",
    "from alphafold3.model.components import utils\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# --- JAX Configuration (Optional) ---\n",
    "# Set JAX platform (e.g., 'gpu' or 'cpu').\n",
    "# jax.config.update('jax_platform_name', 'gpu')\n",
    "# Enable x64 for certain operations if needed, though AF3 typically uses float32.\n",
    "# jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977db8c",
   "metadata": {},
   "source": [
    "# Configuration Parameters (Mimicking command-line flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaa14170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Input and Output Paths ---\n",
    "# If providing input directly in this cell, `json_path_notebook` can be None.\n",
    "# If loading from a directory of JSONs, set `input_dir_notebook`.\n",
    "json_path_notebook = None  # Or set to a file path string e.g., \"my_input.json\"\n",
    "input_dir_notebook = None  # Or set to a directory path string e.g., \"my_af_inputs/\"\n",
    "output_dir_notebook = \"af_output_notebook\" # REQUIRED: Where all outputs will be saved.\n",
    "\n",
    "# Path to the AlphaFold 3 model parameters\n",
    "_DEFAULT_MODEL_DIR_NB = pathlib.Path.cwd() / 'models'\n",
    "model_dir_notebook = _DEFAULT_MODEL_DIR_NB.as_posix()\n",
    "\n",
    "# --- Control which stages to run ---\n",
    "run_data_pipeline_notebook = True  # Set to False if MSAs/templates are in input_dict or JSON\n",
    "run_inference_notebook = True    # Set to False to only run data pipeline\n",
    "\n",
    "# --- Binary Paths (for data pipeline if run_data_pipeline_notebook is True) ---\n",
    "jackhmmer_binary_path_notebook = shutil.which('jackhmmer')\n",
    "nhmmer_binary_path_notebook = shutil.which('nhmmer')\n",
    "hmmalign_binary_path_notebook = shutil.which('hmmalign')\n",
    "hmmsearch_binary_path_notebook = shutil.which('hmmsearch')\n",
    "hmmbuild_binary_path_notebook = shutil.which('hmmbuild')\n",
    "\n",
    "# --- Database Paths (for data pipeline if run_data_pipeline_notebook is True) ---\n",
    "_DEFAULT_DB_DIR_NB = pathlib.Path(os.environ.get('HOME', '.')) / 'public_databases'\n",
    "db_dirs_notebook = [_DEFAULT_DB_DIR_NB.as_posix()] # List of paths to search for DBs\n",
    "\n",
    "# These paths use a placeholder ${DB_DIR} which will be replaced by one of the paths in db_dirs_notebook.\n",
    "small_bfd_database_path_notebook = '${DB_DIR}/bfd-first_non_consensus_sequences.fasta'\n",
    "mgnify_database_path_notebook = '${DB_DIR}/mgy_clusters_2022_05.fa'\n",
    "uniprot_cluster_annot_database_path_notebook = '${DB_DIR}/uniprot_all_2021_04.fa'\n",
    "uniref90_database_path_notebook = '${DB_DIR}/uniref90_2022_05.fa'\n",
    "ntrna_database_path_notebook = '${DB_DIR}/nt_rna_2023_02_23_clust_seq_id_90_cov_80_rep_seq.fasta'\n",
    "rfam_database_path_notebook = '${DB_DIR}/rfam_14_9_clust_seq_id_90_cov_80_rep_seq.fasta'\n",
    "rna_central_database_path_notebook = '${DB_DIR}/rnacentral_active_seq_id_90_cov_80_linclust.fasta'\n",
    "pdb_database_path_notebook = '${DB_DIR}/mmcif_files' # Directory of mmCIFs\n",
    "seqres_database_path_notebook = '${DB_DIR}/pdb_seqres_2022_09_28.fasta'\n",
    "\n",
    "# --- CPU Counts for MSA tools (for data pipeline) ---\n",
    "jackhmmer_n_cpu_notebook = min(multiprocessing.cpu_count(), 8)\n",
    "nhmmer_n_cpu_notebook = min(multiprocessing.cpu_count(), 8)\n",
    "\n",
    "# --- Data Pipeline Configuration ---\n",
    "resolve_msa_overlaps_notebook = True\n",
    "max_template_date_notebook = '2021-09-30' # YYYY-MM-DD\n",
    "conformer_max_iterations_notebook = None # Use RDKit default\n",
    "\n",
    "# --- JAX Inference Performance Tuning ---\n",
    "jax_compilation_cache_dir_notebook = None # Or path to a JAX cache directory e.g., \"./jax_cache\"\n",
    "gpu_device_notebook = 0 # Index of the GPU to use\n",
    "buckets_notebook = [256, 512, 768, 1024, 1280, 1536, 2048, 2560, 3072, 3584, 4096, 4608, 5120] # List of ints\n",
    "flash_attention_implementation_notebook: attention.Implementation = 'xla' # 'triton', 'cudnn', or 'xla'\n",
    "num_recycles_notebook = 10\n",
    "num_diffusion_samples_notebook = 5\n",
    "num_seeds_notebook = None # If int, generates N seeds from the first seed in input. If None, uses seeds from input.\n",
    "\n",
    "# --- Output Controls ---\n",
    "save_embeddings_notebook = False\n",
    "save_distogram_notebook = False\n",
    "force_output_dir_notebook = False # If True, reuses output_dir_notebook even if non-empty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668fd6c",
   "metadata": {},
   "source": [
    "# Input Definition (Define your input as a Python dictionary here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06ca510f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created input object: 2PV7_notebook_example\n",
      "  Number of chains: 1\n",
      "  RNG Seeds: (1,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This example is the same as the 2PV7 example from the README\n",
    "# Modify this dictionary to define your prediction target.\n",
    "input_dict = {\n",
    "  \"name\": \"2PV7_notebook_example\", # It's good to give a unique name\n",
    "  \"sequences\": [\n",
    "    {\n",
    "      \"protein\": {\n",
    "        # For multiple identical protein chains, provide a list of IDs: e.g., \"id\": [\"A\", \"B\"],\n",
    "        \"id\": \"A\",\n",
    "        \"sequence\": \"GMRESYANENQFGFKTINSDIHKIVIVGGYGKLGGLFARYLRASGYPISILDREDWAVAESILANADVVIVSVPINLTLETIERLKPYLTENMLLADLTSVKREPLAKMLEVHTGAVLGLHPMFGADIASMAKQVVVRCDGRFPERYEWLLEQIQIWGAKIYQTNATEHDHNMTYIQALRHFSTFANGLHLSKQPINLANLLALSSPIYRLELAMIGRLFAQDAELYADIIMDKSENLAVIETLKQTYDEALTFFENNDRQGFIDAFHKVRDWFGDYSEQFLKESRQLLQQANDLKQG\",\n",
    "        # Optional: MSA-free, template-free prediction\n",
    "        \"unpairedMsa\": \"\",\n",
    "        \"pairedMsa\": \"\",\n",
    "        \"templates\": [],\n",
    "        # Optional: Provide precomputed MSAs or templates if run_data_pipeline_notebook is False\n",
    "        # \"unpairedMsa\": \"CONTENTS_OF_MSA_FILE_OR_PATH_TO_MSA_FILE\",\n",
    "        # \"pairedMsa\": \"CONTENTS_OF_MSA_FILE_OR_PATH_TO_MSA_FILE\",\n",
    "        # \"templates\": [\n",
    "        #   {\n",
    "        #     \"mmcif\": \"CONTENTS_OF_CIF_FILE_OR_PATH_TO_CIF_FILE\",\n",
    "        #     \"queryIndices\": [0, 1, 2],    # 0-indexed query residue indices matching template\n",
    "        #     \"templateIndices\": [10, 11, 12] # 0-indexed template residue indices\n",
    "        #   }\n",
    "        # ]\n",
    "        # Optional: PTMs\n",
    "        # \"modifications\": [\n",
    "        #    {\"ptmType\": \"ACE\", \"ptmPosition\": 1}, # N-terminal acetylation at residue 1\n",
    "        #    {\"ptmType\": \"PHO\", \"ptmPosition\": 15} # Phosphorylation at residue 15\n",
    "        # ]\n",
    "      }\n",
    "    }\n",
    "    # --- Other examples of sequence types ---\n",
    "    # {\n",
    "    #   \"ligand\": {\n",
    "    #     \"id\": \"L\",\n",
    "    #     \"ccdCodes\": [\"ATP\"] # Can be a list for multi-component ligands\n",
    "    #     # Or use SMILES:\n",
    "    #     # \"smiles\": \"Cc1ccccc1\"\n",
    "    #   }\n",
    "    # },\n",
    "    # {\n",
    "    #   \"rna\": {\n",
    "    #     \"id\": \"R\",\n",
    "    #     \"sequence\": \"AUGGCUAG\",\n",
    "    #     # \"modifications\": [\n",
    "    #     #    {\"modificationType\": \"PSU\", \"basePosition\": 3} # Pseudouridine at base 3\n",
    "    #     # ]\n",
    "    #   }\n",
    "    # },\n",
    "    # {\n",
    "    #   \"dna\": {\n",
    "    #     \"id\": \"D\",\n",
    "    #     \"sequence\": \"ATGCGTTA\",\n",
    "    #     # \"modifications\": [...]\n",
    "    #   }\n",
    "    # }\n",
    "  ],\n",
    "  \"modelSeeds\": [1], # A list of one or more integer seeds. If num_seeds_notebook is set, only the first seed is used as a base.\n",
    "  \"dialect\": \"alphafold3\",\n",
    "  \"version\": folding_input.JSON_VERSION # Uses the latest version from the library\n",
    "  # Optional: Define inter-chain bonds\n",
    "  # \"bondedAtomPairs\": [\n",
    "  #   [[\"A\", 10, \"SG\"], [\"L\", 1, \"C1\"]] # Bond between CYS 10 (chain A) SG and Ligand (chain L) C1\n",
    "  # ]\n",
    "}\n",
    "\n",
    "# Optional: Define a user-specific Chemical Component Dictionary (CCD)\n",
    "# This should be a string in mmCIF format.\n",
    "user_ccd_string_notebook = None\n",
    "# Example:\n",
    "# user_ccd_string_notebook = \"\"\"\n",
    "# data_MYLIGAND\n",
    "# _chem_comp.id                                    MYLIGAND\n",
    "# _chem_comp.name                                  \"MY CUSTOM LIGAND\"\n",
    "# _chem_comp.type                                  NON-POLYMER\n",
    "# _chem_comp.formula                               \"C6 H12 O6\"\n",
    "# _chem_comp.mon_nstd_parent_comp_id               ?\n",
    "# _chem_comp.pdbx_synonyms                         ?\n",
    "# _chem_comp.formula_weight                        180.156\n",
    "# # ... (atoms and bonds for MYLIGAND) ...\n",
    "# \"\"\"\n",
    "\n",
    "# --- Convert Python dict to folding_input.Input object ---\n",
    "# This section prepares the `current_fold_input` object which will be processed.\n",
    "# If `json_path_notebook` or `input_dir_notebook` is set later, this `current_fold_input` might be overridden.\n",
    "current_fold_input = None\n",
    "if input_dict:\n",
    "    try:\n",
    "        input_json_string = json.dumps(input_dict)\n",
    "        current_fold_input = folding_input.Input.from_json(input_json_string)\n",
    "        if user_ccd_string_notebook:\n",
    "            current_fold_input = dataclasses.replace(current_fold_input, user_ccd=user_ccd_string_notebook)\n",
    "\n",
    "        # If num_seeds_notebook is set globally, adjust the current_fold_input\n",
    "        if num_seeds_notebook is not None and current_fold_input:\n",
    "            if len(current_fold_input.rng_seeds) != 1:\n",
    "                raise ValueError(\n",
    "                    \"If num_seeds_notebook is set, the input_dict should contain only one seed in 'modelSeeds'.\"\n",
    "                )\n",
    "            print(f\"Expanding input '{current_fold_input.name}' to {num_seeds_notebook} seeds based on global setting.\")\n",
    "            current_fold_input = current_fold_input.with_multiple_seeds(num_seeds_notebook)\n",
    "\n",
    "        print(f\"Successfully created input object: {current_fold_input.name}\")\n",
    "        print(f\"  Number of chains: {len(current_fold_input.chains)}\")\n",
    "        print(f\"  RNG Seeds: {current_fold_input.rng_seeds}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating folding_input.Input from input_dict: {e}\")\n",
    "        current_fold_input = None # Ensure it's None if creation fails\n",
    "else:\n",
    "    print(\"No input_dict provided in this cell. Ensure json_path_notebook or input_dir_notebook is set if you intend to run.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c569f2d",
   "metadata": {},
   "source": [
    "# Helper Functions & Classes (Ported from run_alphafold.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f741b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ModelRunner Class ---\n",
    "class ModelRunner:\n",
    "  \"\"\"Helper class to run structure prediction stages.\"\"\"\n",
    "  def __init__(\n",
    "      self,\n",
    "      config: model.Model.Config,\n",
    "      device: jax.Device,\n",
    "      model_dir: pathlib.Path,\n",
    "  ):\n",
    "    self._model_config = config\n",
    "    self._device = device\n",
    "    self._model_dir = model_dir\n",
    "\n",
    "  @functools.cached_property\n",
    "  def model_params(self) -> hk.Params:\n",
    "    \"\"\"Loads model parameters from the model directory.\"\"\"\n",
    "    return params.get_model_haiku_params(model_dir=self._model_dir)\n",
    "\n",
    "  @functools.cached_property\n",
    "  def _model(\n",
    "      self,\n",
    "  ) -> Callable[[jnp.ndarray, features.BatchDict], model.ModelResult]:\n",
    "    \"\"\"Loads model parameters and returns a jitted model forward pass.\"\"\"\n",
    "    @hk.transform\n",
    "    def forward_fn(batch):\n",
    "      return model.Model(self._model_config)(batch)\n",
    "    return functools.partial(\n",
    "        jax.jit(forward_fn.apply, device=self._device), self.model_params\n",
    "    )\n",
    "\n",
    "  def run_inference(\n",
    "      self, featurised_example: features.BatchDict, rng_key: jnp.ndarray\n",
    "  ) -> model.ModelResult:\n",
    "    \"\"\"Computes a forward pass of the model on a featurised example.\"\"\"\n",
    "    featurised_example = jax.device_put(\n",
    "        jax.tree_util.tree_map(\n",
    "            jnp.asarray, utils.remove_invalidly_typed_feats(featurised_example)\n",
    "        ),\n",
    "        self._device,\n",
    "    )\n",
    "    result = self._model(rng_key, featurised_example)\n",
    "    result = jax.tree.map(np.asarray, result)\n",
    "    result = jax.tree.map(\n",
    "        lambda x: x.astype(jnp.float32) if x.dtype == jnp.bfloat16 else x,\n",
    "        result,\n",
    "    )\n",
    "    result = dict(result)\n",
    "    identifier = self.model_params['__meta__']['__identifier__'].tobytes()\n",
    "    result['__identifier__'] = identifier\n",
    "    return result\n",
    "\n",
    "  def extract_inference_results(\n",
    "      self,\n",
    "      batch: features.BatchDict,\n",
    "      result: model.ModelResult,\n",
    "      target_name: str,\n",
    "  ) -> list[model.InferenceResult]:\n",
    "    \"\"\"Extracts inference results from model outputs.\"\"\"\n",
    "    return list(\n",
    "        model.Model.get_inference_result(\n",
    "            batch=batch, result=result, target_name=target_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "  def extract_embeddings(\n",
    "      self, result: model.ModelResult, num_tokens: int\n",
    "  ) -> dict[str, np.ndarray] | None:\n",
    "    \"\"\"Extracts embeddings from model outputs.\"\"\"\n",
    "    embeddings = {}\n",
    "    if 'single_embeddings' in result:\n",
    "      embeddings['single_embeddings'] = result['single_embeddings'][\n",
    "          :num_tokens\n",
    "      ].astype(np.float16)\n",
    "    if 'pair_embeddings' in result:\n",
    "      embeddings['pair_embeddings'] = result['pair_embeddings'][\n",
    "          :num_tokens, :num_tokens\n",
    "      ].astype(np.float16)\n",
    "    return embeddings or None\n",
    "\n",
    "  def extract_distogram(\n",
    "      self, result: model.ModelResult, num_tokens: int\n",
    "  ) -> np.ndarray | None:\n",
    "    \"\"\"Extracts distogram from model outputs.\"\"\"\n",
    "    if 'distogram' not in result['distogram']:\n",
    "      return None\n",
    "    distogram = result['distogram']['distogram'][:num_tokens, :num_tokens, :]\n",
    "    return distogram\n",
    "\n",
    "# --- ResultsForSeed Dataclass ---\n",
    "@dataclasses.dataclass(frozen=True, slots=True, kw_only=True)\n",
    "class ResultsForSeed:\n",
    "  seed: int\n",
    "  inference_results: Sequence[model.InferenceResult]\n",
    "  full_fold_input: folding_input.Input\n",
    "  embeddings: dict[str, np.ndarray] | None = None\n",
    "  distogram: np.ndarray | None = None\n",
    "\n",
    "# --- make_model_config_notebook ---\n",
    "def make_model_config_notebook(\n",
    "    *,\n",
    "    flash_attention_implementation: attention.Implementation = 'triton',\n",
    "    num_diffusion_samples: int = 5,\n",
    "    num_recycles: int = 10,\n",
    "    return_embeddings: bool = False,\n",
    "    return_distogram: bool = False,\n",
    ") -> model.Model.Config:\n",
    "  config = model.Model.Config()\n",
    "  config.global_config.flash_attention_implementation = flash_attention_implementation\n",
    "  config.heads.diffusion.eval.num_samples = num_diffusion_samples\n",
    "  config.num_recycles = num_recycles\n",
    "  config.return_embeddings = return_embeddings\n",
    "  config.return_distogram = return_distogram\n",
    "  return config\n",
    "\n",
    "# --- predict_structure_notebook ---\n",
    "def predict_structure_notebook(\n",
    "    fold_input: folding_input.Input,\n",
    "    model_runner: ModelRunner,\n",
    "    buckets: Sequence[int] | None = None,\n",
    "    ref_max_modified_date: datetime.date | None = None,\n",
    "    conformer_max_iterations: int | None = None,\n",
    "    resolve_msa_overlaps: bool = True,\n",
    ") -> Sequence[ResultsForSeed]:\n",
    "  print(f'Featurising data with {len(fold_input.rng_seeds)} seed(s)...')\n",
    "  featurisation_start_time = time.time()\n",
    "  ccd = chemical_components.cached_ccd(user_ccd=fold_input.user_ccd)\n",
    "  featurised_examples = featurisation.featurise_input(\n",
    "      fold_input=fold_input,\n",
    "      buckets=buckets,\n",
    "      ccd=ccd,\n",
    "      verbose=True,\n",
    "      ref_max_modified_date=ref_max_modified_date,\n",
    "      conformer_max_iterations=conformer_max_iterations,\n",
    "      resolve_msa_overlaps=resolve_msa_overlaps,\n",
    "  )\n",
    "  print(f'Featurising data took {time.time() - featurisation_start_time:.2f} seconds.')\n",
    "\n",
    "  print(f'Running model inference for {len(fold_input.rng_seeds)} seed(s)...')\n",
    "  all_inference_start_time = time.time()\n",
    "  all_results_for_seeds = []\n",
    "  for seed, example in zip(fold_input.rng_seeds, featurised_examples):\n",
    "    print(f'  Running inference for seed {seed}...')\n",
    "    inference_start_time = time.time()\n",
    "    rng_key = jax.random.PRNGKey(seed)\n",
    "    result = model_runner.run_inference(example, rng_key)\n",
    "    print(f'  Inference for seed {seed} took {time.time() - inference_start_time:.2f} seconds.')\n",
    "\n",
    "    print(f'  Extracting results for seed {seed}...')\n",
    "    extract_time = time.time()\n",
    "    inference_results_list = model_runner.extract_inference_results(\n",
    "        batch=example, result=result, target_name=fold_input.name\n",
    "    )\n",
    "    num_tokens = len(inference_results_list[0].metadata['token_chain_ids'])\n",
    "    embeddings = model_runner.extract_embeddings(result=result, num_tokens=num_tokens)\n",
    "    distogram = model_runner.extract_distogram(result=result, num_tokens=num_tokens)\n",
    "    print(f'  Extraction for seed {seed} took {time.time() - extract_time:.2f} seconds.')\n",
    "\n",
    "    all_results_for_seeds.append(\n",
    "        ResultsForSeed(\n",
    "            seed=seed,\n",
    "            inference_results=inference_results_list,\n",
    "            full_fold_input=fold_input, # Store the (potentially data-pipelined) input\n",
    "            embeddings=embeddings,\n",
    "            distogram=distogram,\n",
    "        )\n",
    "    )\n",
    "  print(f'Total model inference and extraction took {time.time() - all_inference_start_time:.2f} seconds.')\n",
    "  return all_results_for_seeds\n",
    "\n",
    "# --- write_fold_input_json_notebook ---\n",
    "def write_fold_input_json_notebook(\n",
    "    fold_input: folding_input.Input,\n",
    "    output_dir: os.PathLike[str] | str,\n",
    ") -> None:\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  path = os.path.join(output_dir, f'{fold_input.sanitised_name()}_data.json')\n",
    "  print(f'Writing model input JSON to {path}')\n",
    "  with open(path, 'wt') as f:\n",
    "    f.write(fold_input.to_json())\n",
    "\n",
    "# --- write_outputs_notebook ---\n",
    "def write_outputs_notebook(\n",
    "    all_results_for_seeds: Sequence[ResultsForSeed],\n",
    "    output_dir: os.PathLike[str] | str,\n",
    "    job_name: str,\n",
    ") -> None:\n",
    "  ranking_scores = []\n",
    "  max_ranking_score = None\n",
    "  max_ranking_result = None\n",
    "  output_terms_path = pathlib.Path(alphafold3.cpp.__file__).parent / 'OUTPUT_TERMS_OF_USE.md'\n",
    "  output_terms = output_terms_path.read_text() if output_terms_path.exists() else \"# AlphaFold 3 Output Terms of Use\\n...\"\n",
    "\n",
    "\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  for results_for_seed_item in all_results_for_seeds:\n",
    "    seed = results_for_seed_item.seed\n",
    "    for sample_idx, result_item in enumerate(results_for_seed_item.inference_results):\n",
    "      sample_dir = os.path.join(output_dir, f'seed-{seed}_sample-{sample_idx}')\n",
    "      os.makedirs(sample_dir, exist_ok=True)\n",
    "      post_processing.write_output(\n",
    "          inference_result=result_item,\n",
    "          output_dir=sample_dir,\n",
    "          name=f'{job_name}_seed-{seed}_sample-{sample_idx}',\n",
    "      )\n",
    "      ranking_score = float(result_item.metadata['ranking_score'])\n",
    "      ranking_scores.append((seed, sample_idx, ranking_score))\n",
    "      if max_ranking_score is None or ranking_score > max_ranking_score:\n",
    "        max_ranking_score = ranking_score\n",
    "        max_ranking_result = result_item\n",
    "\n",
    "    if embeddings_item := results_for_seed_item.embeddings:\n",
    "      embeddings_dir = os.path.join(output_dir, f'seed-{seed}_embeddings')\n",
    "      os.makedirs(embeddings_dir, exist_ok=True)\n",
    "      post_processing.write_embeddings(\n",
    "          embeddings=embeddings_item,\n",
    "          output_dir=embeddings_dir,\n",
    "          name=f'{job_name}_seed-{seed}',\n",
    "      )\n",
    "\n",
    "    if (distogram_item := results_for_seed_item.distogram) is not None:\n",
    "      distogram_dir = os.path.join(output_dir, f'seed-{seed}_distogram')\n",
    "      os.makedirs(distogram_dir, exist_ok=True)\n",
    "      distogram_path = os.path.join(distogram_dir, f'{job_name}_seed-{seed}_distogram.npz')\n",
    "      with open(distogram_path, 'wb') as f:\n",
    "        np.savez_compressed(f, distogram=distogram_item.astype(np.float16))\n",
    "\n",
    "  if max_ranking_result is not None:\n",
    "    post_processing.write_output(\n",
    "        inference_result=max_ranking_result,\n",
    "        output_dir=output_dir,\n",
    "        terms_of_use=output_terms,\n",
    "        name=job_name,\n",
    "    )\n",
    "    with open(os.path.join(output_dir, f'{job_name}_ranking_scores.csv'), 'wt') as f:\n",
    "      writer = csv.writer(f)\n",
    "      writer.writerow(['seed', 'sample', 'ranking_score'])\n",
    "      writer.writerows(ranking_scores)\n",
    "\n",
    "# --- replace_db_dir_notebook ---\n",
    "import string # Ensure string is imported\n",
    "def replace_db_dir_notebook(path_with_db_dir: str, db_dirs: Sequence[str]) -> str:\n",
    "  template = string.Template(path_with_db_dir)\n",
    "  if 'DB_DIR' in template.get_identifiers():\n",
    "    for db_dir in db_dirs:\n",
    "      path = template.substitute(DB_DIR=db_dir)\n",
    "      if os.path.exists(path):\n",
    "        return path\n",
    "    raise FileNotFoundError(f'{path_with_db_dir} with ${{DB_DIR}} not found in any of {db_dirs}.')\n",
    "  if not os.path.exists(path_with_db_dir):\n",
    "    raise FileNotFoundError(f'{path_with_db_dir} does not exist.')\n",
    "  return path_with_db_dir\n",
    "\n",
    "# --- process_fold_input_notebook (with overloads) ---\n",
    "@overload\n",
    "def process_fold_input_notebook(\n",
    "    fold_input: folding_input.Input,\n",
    "    data_pipeline_config: pipeline.DataPipelineConfig | None,\n",
    "    model_runner: None,\n",
    "    output_dir: os.PathLike[str] | str,\n",
    "    buckets: Sequence[int] | None = None,\n",
    "    ref_max_modified_date: datetime.date | None = None,\n",
    "    conformer_max_iterations: int | None = None,\n",
    "    resolve_msa_overlaps: bool = True,\n",
    "    force_output_dir: bool = False,\n",
    ") -> folding_input.Input: ...\n",
    "\n",
    "@overload\n",
    "def process_fold_input_notebook(\n",
    "    fold_input: folding_input.Input,\n",
    "    data_pipeline_config: pipeline.DataPipelineConfig | None,\n",
    "    model_runner: ModelRunner,\n",
    "    output_dir: os.PathLike[str] | str,\n",
    "    buckets: Sequence[int] | None = None,\n",
    "    ref_max_modified_date: datetime.date | None = None,\n",
    "    conformer_max_iterations: int | None = None,\n",
    "    resolve_msa_overlaps: bool = True,\n",
    "    force_output_dir: bool = False,\n",
    ") -> Sequence[ResultsForSeed]: ...\n",
    "\n",
    "def process_fold_input_notebook(\n",
    "    fold_input: folding_input.Input,\n",
    "    data_pipeline_config: pipeline.DataPipelineConfig | None,\n",
    "    model_runner: ModelRunner | None,\n",
    "    output_dir: os.PathLike[str] | str,\n",
    "    buckets: Sequence[int] | None = None,\n",
    "    ref_max_modified_date: datetime.date | None = None,\n",
    "    conformer_max_iterations: int | None = None,\n",
    "    resolve_msa_overlaps: bool = True,\n",
    "    force_output_dir: bool = False,\n",
    ") -> folding_input.Input | Sequence[ResultsForSeed]:\n",
    "  print(f'\\nRunning fold job {fold_input.name}...')\n",
    "  if not fold_input.chains:\n",
    "    raise ValueError('Fold input has no chains.')\n",
    "\n",
    "  if (not force_output_dir and os.path.exists(output_dir) and os.listdir(output_dir)):\n",
    "    new_output_dir = f'{output_dir}_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    print(f'Output will be written in {new_output_dir} since {output_dir} is non-empty.')\n",
    "    output_dir = new_output_dir\n",
    "  else:\n",
    "    print(f'Output will be written in {output_dir}')\n",
    "\n",
    "  processed_fold_input = fold_input # Keep a reference to the potentially modified input\n",
    "  if data_pipeline_config is None:\n",
    "    print('Skipping data pipeline...')\n",
    "  else:\n",
    "    print('Running data pipeline...')\n",
    "    data_pipeline_start_time = time.time()\n",
    "    processed_fold_input = pipeline.DataPipeline(data_pipeline_config).process(fold_input)\n",
    "    print(f'Data pipeline took {time.time() - data_pipeline_start_time:.2f} seconds.')\n",
    "\n",
    "  write_fold_input_json_notebook(processed_fold_input, output_dir) # Write the (possibly augmented) input\n",
    "\n",
    "  if model_runner is None:\n",
    "    print('Skipping model inference...')\n",
    "    final_output = processed_fold_input\n",
    "  else:\n",
    "    all_inference_results = predict_structure_notebook(\n",
    "        fold_input=processed_fold_input, # Use the processed input\n",
    "        model_runner=model_runner,\n",
    "        buckets=buckets,\n",
    "        ref_max_modified_date=ref_max_modified_date,\n",
    "        conformer_max_iterations=conformer_max_iterations,\n",
    "        resolve_msa_overlaps=resolve_msa_overlaps,\n",
    "    )\n",
    "    print(f'Writing outputs for {len(processed_fold_input.rng_seeds)} seed(s)...')\n",
    "    write_outputs_notebook(\n",
    "      all_results_for_seeds=all_inference_results,\n",
    "        output_dir=output_dir,\n",
    "        job_name=processed_fold_input.sanitised_name(),\n",
    "    )\n",
    "    final_output = all_inference_results\n",
    "\n",
    "  print(f'Fold job {processed_fold_input.name} done, output written to {os.path.abspath(output_dir)}\\n')\n",
    "  return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e6ec7",
   "metadata": {},
   "source": [
    "# Main Execution Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b08f40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global output directory: /home/ckj24/alphafold3/af_output_notebook\n",
      "Setting up Data Pipeline Configuration...\n",
      "ERROR: Database or binary path not found: ${DB_DIR}/bfd-first_non_consensus_sequences.fasta with ${DB_DIR} not found in any of ['/home/ckj24/public_databases'].\n",
      "Please ensure all database and binary paths in Cell 3 are correct and accessible.\n",
      "Setting up Model Runner...\n",
      "Using GPU: cuda:0\n",
      "  GPU Compute Capability: 8.0\n",
      "Building model and loading parameters...\n",
      "Model parameters loaded successfully.\n",
      "Using input defined in notebook cell: 2PV7_notebook_example\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Running AlphaFold 3. Please note that standard AlphaFold 3 model\n",
       "parameters are only available under terms of use provided at\n",
       "https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md.\n",
       "If you do not agree to these terms and are using AlphaFold 3 derived\n",
       "model parameters, cancel execution of AlphaFold 3 inference with\n",
       "CTRL-C (or stop button in Jupyter), and do not use the model parameters.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running fold job 2PV7_notebook_example...\n",
      "Output will be written in af_output_notebook/2pv7_notebook_example\n",
      "Skipping data pipeline...\n",
      "Writing model input JSON to af_output_notebook/2pv7_notebook_example/2pv7_notebook_example_data.json\n",
      "Featurising data with 1 seed(s)...\n",
      "Featurising data with seed 1.\n",
      "Featurising data with seed 1 took 4.76 seconds.\n",
      "Featurising data took 4.76 seconds.\n",
      "Running model inference for 1 seed(s)...\n",
      "  Running inference for seed 1...\n",
      "  Inference for seed 1 took 72.25 seconds.\n",
      "  Extracting results for seed 1...\n",
      "  Extraction for seed 1 took 0.39 seconds.\n",
      "Total model inference and extraction took 72.64 seconds.\n",
      "Writing outputs for 1 seed(s)...\n",
      "Fold job 2PV7_notebook_example done, output written to /home/ckj24/alphafold3/af_output_notebook/2pv7_notebook_example\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ✅ Done running 1 fold job(s)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Outputs saved to subdirectories within: `/home/ckj24/alphafold3/af_output_notebook`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- JAX Compilation Cache ---\n",
    "if jax_compilation_cache_dir_notebook:\n",
    "    jax.config.update('jax_compilation_cache_dir', jax_compilation_cache_dir_notebook)\n",
    "    print(f\"JAX compilation cache enabled at: {jax_compilation_cache_dir_notebook}\")\n",
    "\n",
    "# --- Validate Prerequisite Configurations ---\n",
    "if not run_inference_notebook and not run_data_pipeline_notebook:\n",
    "    raise ValueError(\"At least one of run_inference_notebook or run_data_pipeline_notebook must be True.\")\n",
    "\n",
    "# --- Create Output Directory ---\n",
    "# The actual job-specific output directory will be a subdirectory of this.\n",
    "# This global output_dir_notebook is where all job folders will reside.\n",
    "try:\n",
    "    os.makedirs(output_dir_notebook, exist_ok=True)\n",
    "    print(f\"Global output directory: {os.path.abspath(output_dir_notebook)}\")\n",
    "except OSError as e:\n",
    "    print(f\"Failed to create global output directory {output_dir_notebook}: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Setup Data Pipeline Config (if running data pipeline) ---\n",
    "data_pipeline_config_obj = None\n",
    "if run_data_pipeline_notebook:\n",
    "    print(\"Setting up Data Pipeline Configuration...\")\n",
    "    max_template_date_obj = datetime.date.fromisoformat(max_template_date_notebook)\n",
    "    expand_path = lambda p: replace_db_dir_notebook(p, db_dirs_notebook)\n",
    "    try:\n",
    "        data_pipeline_config_obj = pipeline.DataPipelineConfig(\n",
    "            jackhmmer_binary_path=jackhmmer_binary_path_notebook,\n",
    "            nhmmer_binary_path=nhmmer_binary_path_notebook,\n",
    "            hmmalign_binary_path=hmmalign_binary_path_notebook,\n",
    "            hmmsearch_binary_path=hmmsearch_binary_path_notebook,\n",
    "            hmmbuild_binary_path=hmmbuild_binary_path_notebook,\n",
    "            small_bfd_database_path=expand_path(small_bfd_database_path_notebook),\n",
    "            mgnify_database_path=expand_path(mgnify_database_path_notebook),\n",
    "            uniprot_cluster_annot_database_path=expand_path(uniprot_cluster_annot_database_path_notebook),\n",
    "            uniref90_database_path=expand_path(uniref90_database_path_notebook),\n",
    "            ntrna_database_path=expand_path(ntrna_database_path_notebook),\n",
    "            rfam_database_path=expand_path(rfam_database_path_notebook),\n",
    "            rna_central_database_path=expand_path(rna_central_database_path_notebook),\n",
    "            pdb_database_path=expand_path(pdb_database_path_notebook),\n",
    "            seqres_database_path=expand_path(seqres_database_path_notebook),\n",
    "            jackhmmer_n_cpu=jackhmmer_n_cpu_notebook,\n",
    "            nhmmer_n_cpu=nhmmer_n_cpu_notebook,\n",
    "            max_template_date=max_template_date_obj,\n",
    "        )\n",
    "        print(\"Data Pipeline Configuration ready.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Database or binary path not found: {e}\")\n",
    "        print(\"Please ensure all database and binary paths in Cell 3 are correct and accessible.\")\n",
    "        data_pipeline_config_obj = None # Prevent further execution if paths are wrong\n",
    "        # raise # Optionally re-raise to stop execution\n",
    "\n",
    "# --- Setup Model Runner (if running inference) ---\n",
    "model_runner_obj = None\n",
    "if run_inference_notebook:\n",
    "    print(\"Setting up Model Runner...\")\n",
    "    local_devices = jax.local_devices()\n",
    "    if not local_devices:\n",
    "        raise RuntimeError(\"No JAX devices found. Ensure JAX is installed correctly for your hardware (CPU/GPU).\")\n",
    "\n",
    "    # Try to use GPU if available, otherwise CPU (with a warning)\n",
    "    gpu_devices = [d for d in local_devices if d.platform.upper() == 'GPU']\n",
    "    if gpu_devices:\n",
    "        if gpu_device_notebook >= len(gpu_devices):\n",
    "            print(f\"Warning: GPU device index {gpu_device_notebook} out of range. Found {len(gpu_devices)} GPUs. Using GPU 0.\")\n",
    "            selected_gpu_idx = 0\n",
    "        else:\n",
    "            selected_gpu_idx = gpu_device_notebook\n",
    "        selected_device = gpu_devices[selected_gpu_idx]\n",
    "        print(f\"Using GPU: {selected_device}\")\n",
    "    else:\n",
    "        print(\"WARNING: No GPU detected by JAX. Inference will run on CPU and may be very slow or OOM.\")\n",
    "        cpu_devices = [d for d in local_devices if d.platform.upper() == 'CPU']\n",
    "        if not cpu_devices:\n",
    "             raise RuntimeError(\"No CPU devices found by JAX.\")\n",
    "        selected_device = cpu_devices[0] # Use the first CPU device\n",
    "        print(f\"Using CPU: {selected_device}\")\n",
    "\n",
    "\n",
    "    # GPU Compute Capability Check (informational)\n",
    "    if selected_device.platform.upper() == 'GPU':\n",
    "        try:\n",
    "            compute_capability = float(selected_device.compute_capability)\n",
    "            print(f\"  GPU Compute Capability: {compute_capability}\")\n",
    "            if compute_capability < 6.0:\n",
    "                display(Markdown(\"**WARNING:** AlphaFold 3 ideally requires GPU compute capability 6.0 or higher.\"))\n",
    "            elif 7.0 <= compute_capability < 8.0:\n",
    "                xla_flags = os.environ.get('XLA_FLAGS', \"\")\n",
    "                required_flag = '--xla_disable_hlo_passes=custom-kernel-fusion-rewriter'\n",
    "                if required_flag not in xla_flags:\n",
    "                    display(Markdown(f\"**WARNING:** For GPU compute capability 7.x, `XLA_FLAGS` should include `{required_flag}`.\"))\n",
    "                if flash_attention_implementation_notebook != 'xla':\n",
    "                    display(Markdown(\"**WARNING:** For GPU compute capability 7.x, `flash_attention_implementation_notebook` should be set to `'xla'`.\"))\n",
    "        except Exception as e:\n",
    "            print(f\"  Could not determine GPU compute capability: {e}\")\n",
    "\n",
    "    print(\"Building model and loading parameters...\")\n",
    "    model_runner_obj = ModelRunner(\n",
    "        config=make_model_config_notebook(\n",
    "            flash_attention_implementation=flash_attention_implementation_notebook,\n",
    "            num_diffusion_samples=num_diffusion_samples_notebook,\n",
    "            num_recycles=num_recycles_notebook,\n",
    "            return_embeddings=save_embeddings_notebook,\n",
    "            return_distogram=save_distogram_notebook,\n",
    "        ),\n",
    "        device=selected_device,\n",
    "        model_dir=pathlib.Path(model_dir_notebook),\n",
    "    )\n",
    "    try:\n",
    "        _ = model_runner_obj.model_params # This triggers loading\n",
    "        print(\"Model parameters loaded successfully.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: Model parameters not found at {model_dir_notebook}: {e}\")\n",
    "        print(\"Please ensure model_dir_notebook in Cell 3 points to the correct directory.\")\n",
    "        model_runner_obj = None # Prevent further execution\n",
    "        # raise # Optionally re-raise\n",
    "\n",
    "# --- Determine inputs to process ---\n",
    "fold_inputs_to_process = []\n",
    "if input_dir_notebook:\n",
    "    print(f\"Loading inputs from directory: {input_dir_notebook}\")\n",
    "    fold_inputs_to_process.extend(\n",
    "        folding_input.load_fold_inputs_from_dir(pathlib.Path(input_dir_notebook))\n",
    "    )\n",
    "elif json_path_notebook:\n",
    "    print(f\"Loading input from JSON file: {json_path_notebook}\")\n",
    "    fold_inputs_to_process.extend(\n",
    "        folding_input.load_fold_inputs_from_path(pathlib.Path(json_path_notebook))\n",
    "    )\n",
    "elif current_fold_input: # Input defined in Cell 4\n",
    "    print(f\"Using input defined in notebook cell: {current_fold_input.name}\")\n",
    "    fold_inputs_to_process.append(current_fold_input)\n",
    "\n",
    "if not fold_inputs_to_process:\n",
    "    if not (run_data_pipeline_notebook and data_pipeline_config_obj is None) and \\\n",
    "       not (run_inference_notebook and model_runner_obj is None): # Avoid error if setup failed\n",
    "        raise ValueError(\n",
    "            \"No inputs to process. Define `input_dict` in Cell 4, or set `json_path_notebook` or `input_dir_notebook` in Cell 3.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Skipping processing due to earlier setup errors.\")\n",
    "\n",
    "\n",
    "# --- Display AlphaFold 3 Usage Notice ---\n",
    "notice_md = textwrap.dedent(f\"\"\"\\\n",
    "    Running AlphaFold 3. Please note that standard AlphaFold 3 model\n",
    "    parameters are only available under terms of use provided at\n",
    "    https://github.com/google-deepmind/alphafold3/blob/main/WEIGHTS_TERMS_OF_USE.md.\n",
    "    If you do not agree to these terms and are using AlphaFold 3 derived\n",
    "    model parameters, cancel execution of AlphaFold 3 inference with\n",
    "    CTRL-C (or stop button in Jupyter), and do not use the model parameters.\n",
    "    \"\"\")\n",
    "display(Markdown(notice_md))\n",
    "\n",
    "# --- Process each input ---\n",
    "num_processed_fold_inputs = 0\n",
    "if (run_data_pipeline_notebook and data_pipeline_config_obj) or \\\n",
    "   (run_inference_notebook and model_runner_obj) or \\\n",
    "   (not run_data_pipeline_notebook and not run_inference_notebook): # Allow if both are false (just writes JSON)\n",
    "\n",
    "    for f_input_to_process in fold_inputs_to_process:\n",
    "        # Apply num_seeds_notebook if it was set globally and not already applied\n",
    "        # This handles cases where input comes from file/dir and num_seeds_notebook is active\n",
    "        if num_seeds_notebook is not None and f_input_to_process is not current_fold_input:\n",
    "            if len(f_input_to_process.rng_seeds) != 1:\n",
    "                 raise ValueError(\"If num_seeds_notebook is set, inputs from files/directories should also contain only one seed.\")\n",
    "            print(f\"Expanding input '{f_input_to_process.name}' to {num_seeds_notebook} seeds based on global setting.\")\n",
    "            f_input_to_process = f_input_to_process.with_multiple_seeds(num_seeds_notebook)\n",
    "\n",
    "        # Determine the specific output directory for this job, inside the global output_dir_notebook\n",
    "        job_specific_output_dir = os.path.join(output_dir_notebook, f_input_to_process.sanitised_name())\n",
    "\n",
    "        process_fold_input_notebook(\n",
    "            fold_input=f_input_to_process,\n",
    "            data_pipeline_config=data_pipeline_config_obj if run_data_pipeline_notebook else None,\n",
    "            model_runner=model_runner_obj if run_inference_notebook else None,\n",
    "            output_dir=job_specific_output_dir,\n",
    "            buckets=tuple(buckets_notebook) if buckets_notebook else None,\n",
    "            ref_max_modified_date=datetime.date.fromisoformat(max_template_date_notebook) if run_inference_notebook else None,\n",
    "            conformer_max_iterations=conformer_max_iterations_notebook if run_inference_notebook else None,\n",
    "            resolve_msa_overlaps=resolve_msa_overlaps_notebook if run_data_pipeline_notebook else True,\n",
    "            force_output_dir=force_output_dir_notebook,\n",
    "        )\n",
    "        num_processed_fold_inputs += 1\n",
    "else:\n",
    "    print(\"Skipping main processing loop due to errors in Data Pipeline or Model Runner setup.\")\n",
    "\n",
    "\n",
    "if num_processed_fold_inputs > 0:\n",
    "    display(Markdown(f\"### ✅ Done running {num_processed_fold_inputs} fold job(s).\"))\n",
    "    display(Markdown(f\"Outputs saved to subdirectories within: `{os.path.abspath(output_dir_notebook)}`\"))\n",
    "elif not fold_inputs_to_process:\n",
    "     display(Markdown(\"No input was processed.\"))\n",
    "else:\n",
    "    display(Markdown(\"Processing was skipped due to setup issues. Please check error messages above.\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphafold3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
