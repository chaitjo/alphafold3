Accurate structure prediction of biomolecular interactions with AlphaFold 3

Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J. Ballard, Joshua Bambrick, Sebastian W. Bodenstein, David A. Evans, Chia-Chun Hung, Michael O’Neill, David Reiman, Kathryn Tunyasuvunakool, Zachary Wu, Akvilė Žemgulytė, Eirini Arvaniti, Charles Beattie, Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov, Miles Congreve, Alexander I. Cowen-Rivers, Andrew Cowie, Michael Figurnov, Fabian B. Fuchs, Hannah Gladman, Rishub Jain, Yousuf A. Khan, Caroline M. R. Low, Kuba Perlin, Anna Potapenko, Pascal Savy, Sukhdeep Singh, Adrian Stecula, Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D. Zhong, Michal Zielinski, Augustin Žídek, Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis Hassabis & John M. Jumper

Abstract
The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2,3,4,5,6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein–ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein–nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody–antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.

Main
Accurate models of biological complexes are critical to our understanding of cellular functions and for the rational design of therapeutics2,3,4,9. Enormous progress has been achieved in protein structure prediction with the development of AlphaFold1, and the field has grown tremendously with a number of later methods that build on the ideas and techniques of AlphaFold 2 (AF2)10,11,12. Almost immediately after AlphaFold became available, it was shown that simple input modifications would enable surprisingly accurate protein interaction predictions13,14,15 and that training AF2 specifically for protein interaction prediction yielded a highly accurate system7.

These successes lead to the question of whether it is possible to accurately predict the structure of complexes containing a much wider range of biomolecules, including ligands, ions, nucleic acids and modified residues, within a deep-learning framework. A wide range of predictors for various specific interaction types has been developed16,17,18,19,20,21,22,23,24,25,26,27,28, as well as one generalist method developed concurrently with the present work29, but the accuracy of such deep-learning attempts has been mixed and often below that of physics-inspired methods30,31. Almost all of these methods are also highly specialized to particular interaction types and cannot predict the structure of general biomolecular complexes containing many types of entities.

Here we present AlphaFold 3 (AF3)—a model that is capable of high-accuracy prediction of complexes containing nearly all molecular types present in the Protein Data Bank32 (PDB) (Fig. 1a,b). In all but one category, it achieves a substantially higher performance than strong methods that specialize in just the given task (Fig. 1c and Extended Data Table 1), including higher accuracy at protein structure and the structure of protein–protein interactions.

Fig. 1: AF3 accurately predicts structures across biomolecular complexes.
a,b, Example structures predicted using AF3. a, Bacterial CRP/FNR family transcriptional regulator protein bound to DNA and cGMP (PDB 7PZB; full-complex LDDT47, 82.8; global distance test (GDT)48, 90.1). b, Human coronavirus OC43 spike protein, 4,665 residues, heavily glycosylated and bound by neutralizing antibodies (PDB 7PNM; full-complex LDDT, 83.0; GDT, 83.1). c, AF3 performance on PoseBusters (v.1, August 2023 release), our recent PDB evaluation set and CASP15 RNA. Metrics are as follows: percentage of pocket-aligned ligand r.m.s.d. < 2 Å for ligands and covalent modifications; interface LDDT for protein–nucleic acid complexes; LDDT for nucleic acid and protein monomers; and percentage DockQ > 0.23 for protein–protein and protein–antibody interfaces. All scores are reported from the top confidence-ranked sample out of five model seeds (each with five diffusion samples), except for protein–antibody scores, which were ranked across 1,000 model seeds for both models (each AF3 seed with five diffusion samples). Sampling and ranking details are provided in the Methods. For ligands, n indicates the number of targets; for nucleic acids, n indicates the number of structures; for modifications, n indicates the number of clusters; and for proteins, n indicates the number of clusters. The bar height indicates the mean; error bars indicate exact binomial distribution 95% confidence intervals for PoseBusters and by 10,000 bootstrap resamples for all others. Significance levels were calculated using two-sided Fisher’s exact tests for PoseBusters and using two-sided Wilcoxon signed-rank tests for all others; ***P < 0.001, **P < 0.01. Exact P values (from left to right) are as follows: 2.27 × 10−13, 2.57 × 10−3, 2.78 × 10−3, 7.28 × 10−12, 1.81 × 10−18, 6.54 × 10−5 and 1.74 × 10−34. AF-M 2.3, AlphaFold-Multimer v.2.3; dsDNA, double-stranded DNA. d, AF3 architecture for inference. The rectangles represent processing modules and the arrows show the data flow. Yellow, input data; blue, abstract network activations; green, output data. The coloured balls represent physical atom coordinates.

This is achieved by a substantial evolution of the AF2 architecture and training procedure (Fig. 1d) both to accommodate more general chemical structures and to improve the data efficiency of learning. The system reduces the amount of multiple-sequence alignment (MSA) processing by replacing the AF2 evoformer with the simpler pairformer module (Fig. 2a). Furthermore it directly predicts the raw atom coordinates with a diffusion module, replacing the AF2 structure module that operated on amino-acid-specific frames and side-chain torsion angles (Fig. 2b). The multiscale nature of the diffusion process (low noise levels induce the network to improve local structure) also enable us to eliminate stereochemical losses and most special handling of bonding patterns in the network, easily accommodating arbitrary chemical components.

Fig. 2: Architectural and training details.
a, The pairformer module. Input and output: pair representation with dimension (n, n, c) and single representation with dimension (n, c). n is the number of tokens (polymer residues and atoms); c is the number of channels (128 for the pair representation, 384 for the single representation). Each of the 48 blocks has an independent set of trainable parameters. b, The diffusion module. Input: coarse arrays depict per-token representations (green, inputs; blue, pair; red, single). Fine arrays depict per-atom representations. The coloured balls represent physical atom coordinates. Cond., conditioning; rand. rot. trans., random rotation and translation; seq., sequence. c, The training set-up (distogram head omitted) starting from the end of the network trunk. The coloured arrays show activations from the network trunk (green, inputs; blue, pair; red, single). The blue arrows show abstract activation arrays. The yellow arrows show ground-truth data. The green arrows show predicted data. The stop sign represents stopping of the gradient. Both depicted diffusion modules share weights. d, Training curves for initial training and fine-tuning stages, showing the LDDT on our evaluation set as a function of optimizer steps. The scatter plot shows the raw datapoints and the lines show the smoothed performance using a median filter with a kernel width of nine datapoints. The crosses mark the point at which the smoothed performance reaches 97% of its initial training maximum.

Network architecture and training
The overall structure of AF3 (Fig. 1d and Supplementary Methods 3) echoes that of AF2, with a large trunk evolving a pairwise representation of the chemical complex followed by a structure module that uses the pairwise representation to generate explicit atomic positions, but there are large differences in each major component. These modifications were driven both by the need to accommodate a wide range of chemical entities without excessive special casing and by observations of AF2 performance with different modifications. Within the trunk, MSA processing is substantially de-emphasized, with a much smaller and simpler MSA embedding block (Supplementary Methods 3.3). Compared with the original evoformer from AF2, the number of blocks is reduced to four, the processing of the MSA representation uses an inexpensive pair-weighted averaging and only the pair representation is used for later processing steps. The ‘pairformer’ (Fig. 2a and Supplementary Methods 3.6) replaces the evoformer of AF2 as the dominant processing block. It operates only on the pair representation and the single representation; the MSA representation is not retained and all information passes through the pair representation. The pair processing and the number of blocks (48) is largely unchanged from AF2. The resulting pair and single representation together with the input representation are passed to the new diffusion module (Fig. 2b) that replaces the structure module of AF2.

The diffusion module (Fig. 2b and Supplementary Methods 3.7) operates directly on raw atom coordinates, and on a coarse abstract token representation, without rotational frames or any equivariant processing. We had observed in AF2 that removing most of the complexity of the structure module had only a modest effect on the prediction accuracy, and maintaining the backbone frame and side-chain torsion representation add quite a bit of complexity for general molecular graphs. Similarly AF2 required carefully tuned stereochemical violation penalties during training to enforce chemical plausibility of the resulting structures. We use a relatively standard diffusion approach33 in which the diffusion model is trained to receive ‘noised’ atomic coordinates and then predict the true coordinates. This task requires the network to learn protein structure at a variety of length scales, whereby the denoising task at small noise emphasizes understanding very local stereochemistry and the denoising task at high noise emphasizes the large-scale structure of the system. At the inference time, random noise is sampled and then recurrently denoised to produce a final structure. Importantly, this is a generative training procedure that produces a distribution of answers. This means that, for each answer, the local structure will be sharply defined (for example, side-chain bond geometry) even when the network is uncertain about the positions. For this reason, we are able to avoid both torsion-based parametrizations of the residues and violation losses on the structure, while handling the full complexity of general ligands. Similarly to some recent work34, we find that no invariance or equivariance with respect to global rotations and translation of the molecule are required in the architecture and we therefore omit them to simplify the machine learning architecture.

The use of a generative diffusion approach comes with some technical challenges that we needed to address. The biggest issue is that generative models are prone to hallucination35, whereby the model may invent plausible-looking structure even in unstructured regions. To counteract this effect, we use a cross-distillation method in which we enrich the training data with structures predicted by AlphaFold-Multimer (v.2.3)7,8. In these structures, unstructured regions are typically represented by long extended loops instead of compact structures, and training on them ‘teaches’ AF3 to mimic this behaviour. This cross-distillation greatly reduced the hallucination behaviour of AF3 (Extended Data Fig. 1 for disorder prediction results on the CAID 236 benchmark set).

We also developed confidence measures that predict the atom-level and pairwise errors in our final structures. In AF2, this was done directly by regressing the error in the output of the structure module during training. However, this procedure is not applicable to diffusion training, as only a single step of the diffusion is trained instead of a full-structure generation (Fig. 2c). To remedy this, we developed a diffusion ‘rollout’ procedure for the full-structure prediction generation during training (using a larger step size than normal; Fig. 2c (mini-rollout)). This predicted structure is then used to permute the symmetric ground-truth chains and ligands, and to compute the performance metrics to train the confidence head. The confidence head uses the pairwise representation to predict a modified local distance difference test (pLDDT) and a predicted aligned error (PAE) matrix as in AF2, as well as a distance error matrix (PDE), which is the error in the distance matrix of the predicted structure as compared to the true structure (details are provided in Supplementary Methods 4.3).

Figure 2d shows that, during initial training, the model learns quickly to predict the local structures (all intrachain metrics go up quickly and reach 97% of the maximum performance within the first 20,000 training steps), while the model needs considerably longer to learn the global constellation (the interface metrics go up slowly and protein–protein interface LDDT passes the 97% bar only after 60,000 steps). During AF3 development, we observed that some model abilities topped out relatively early and started to decline (most likely due to overfitting to the limited number of training samples for this capability), while other abilities were still undertrained. We addressed this by increasing or decreasing the sampling probability for the corresponding training sets (Supplementary Methods 2.5.1) and by performing early stopping using a weighted average of all of the above metrics and some additional metrics to select the best model checkpoint (Supplementary Table 7). The fine-tuning stages with the larger crop sizes improve the model on all metrics with an especially high uplift on protein–protein interfaces (Extended Data Fig. 2).

Accuracy across complex types
AF3 can predict structures from input polymer sequences, residue modifications and ligand SMILES (simplified molecular-input line-entry system). In Fig. 3 we show a selection of examples highlighting the ability of the model to generalize to a number of biologically important and therapeutically relevant modalities. In selecting these examples, we considered novelty in terms of the similarity of individual chains and interfaces to the training set (additional information is provided in Supplementary Methods 8.1).

Fig. 3: Examples of predicted complexes.
Selected structure predictions from AF3. Predicted protein chains are shown in blue (predicted antibody in green), predicted ligands and glycans in orange, predicted RNA in purple and the ground truth is shown in grey. a, Human 40S small ribosomal subunit (7,663 residues) including 18S ribosomal RNA and Met-tRNAiMet (opaque purple) in a complex with translation initiation factors eIF1A and eIF5B (opaque blue; PDB 7TQL; full-complex LDDT, 87.7; GDT, 86.9). b, The glycosylated globular portion of an EXTL3 homodimer (PDB 7AU2; mean pocket-aligned r.m.s.d., 1.10 Å). c, Mesothelin C-terminal peptide bound to the monoclonal antibody 15B6 (PDB 7U8C; DockQ, 0.85). d, LGK974, a clinical-stage inhibitor, bound to PORCN in a complex with the WNT3A peptide (PDB 7URD; ligand r.m.s.d., 1.00 Å). e, (5S,6S)-O7-sulfo DADH bound to the AziU3/U2 complex with a novel fold (PDB 7WUX; ligand r.m.s.d., 1.92 Å). f, Analogue of NIH-12848 bound to an allosteric site of PI5P4Kγ (PDB 7QIE; ligand r.m.s.d., 0.37 Å).

We evaluated the performance of the system on recent interface-specific benchmarks for each complex type (Fig. 1c and Extended Data Table 1). Performance on protein–ligand interfaces was evaluated on the PoseBusters benchmark set, which is composed of 428 protein–ligand structures released to the PDB in 2021 or later. As our standard training cut-off date is in 2021, we trained a separate AF3 model with an earlier training-set cutoff (Methods). Accuracy on the PoseBusters set is reported as the percentage of protein–ligand pairs with pocket-aligned ligand root mean squared deviation (r.m.s.d.) of less than 2 Å. The baseline models come in two categories: those that use only protein sequence and ligand SMILES as an input and those that additionally leak information from the solved protein–ligand test structure. Traditional docking methods use the latter privileged information, even though that information would not be available in real-world use cases. Even so, AF3 greatly outperforms classical docking tools such as Vina37,38 even while not using any structural inputs (Fisher’s exact test, P = 2.27 × 10−13) and greatly outperforms all other true blind docking like RoseTTAFold All-Atom (P = 4.45 × 10−25). Extended Data Fig. 3 shows three examples in which AF3 achieves accurate predictions but docking tools Vina and Gold do not37. PoseBusters analysis was performed using a training cut-off of 30 September 2019 for AF3 to ensure that the model was not trained on any PoseBusters structures. To compare with the RoseTTAFold All-Atom results, we used PoseBusters version 1. Version 2 (crystal contacts removed from the benchmark set) results including quality metrics are shown in Extended Data Fig. 4b–f and Extended Data Table 1. We use multiple seeds to ensure correct chirality and avoid slight protein–ligand clashing (as opposed to a method like diffusion guidance to enforce) but we are typically able to produce high-quality stereochemistry. Separately, we also train a version of AF3 that receives the ‘pocket information’ as used in some recent deep-learning work24,26 (the results are shown in Extended Data Fig. 4a).

AF3 predicts protein–nucleic complexes and RNA structures with higher accuracy than RoseTTAFold2NA15 (Fig. 1c (second plot)). As RoseTTAFold2NA is validated only on structures below 1,000 residues, we use only structures below 1,000 residues from our recent PDB evaluation set for this comparison (Methods). AF3 is able to predict protein–nucleic structures with thousands of residues, an example of which is shown in Fig. 3a. Note that we do not compare directly to RoseTTAFold All-Atom, but benchmarks indicate that RoseTTAFold All-Atom is slightly less accurate than RoseTTAFold2NA for nucleic acid predictions29.

We also evaluated AF3 performance on the ten publicly available Critical Assessment of Structure Prediction 15 (CASP15) RNA targets: we achieve a higher average performance than RoseTTAFold2NA and AIchemy_RNA27 (the best AI-based submission in CASP1518,31) on the respective common subsets of our and their predictions (detailed results are shown in Extended Data Fig. 5a). We did not reach the performance of the best human-expert-aided CASP15 submission AIchemy_RNA239 (Fig. 1c (centre left)). Owing to limited dataset sizes, we do not report significance test statistics here. Further analysis of the accuracy of predicting nucleic acids alone (without proteins) is shown in Extended Data Fig. 5b.

Covalent modifications (bonded ligands, glycosylation, and modified protein residues and nucleic acid bases) are also accurately predicted by AF3 (Fig. 1c (centre right)). Modifications include those to any polymer residue (protein, RNA or DNA). We report accuracy as the percentage of successful predictions (pocket r.m.s.d. < 2 Å). We apply quality filters to the bonded ligands and glycosylation dataset (as does PoseBusters): we include only ligands with high-quality experimental data (ranking_model_fit > 0.5, according to the RCSB structure validation report, that is, X-ray structures with a model quality above the median). As with the PoseBusters set, the bonded ligands and glycosylation datasets are not filtered by homology to the training dataset. Filtering on the basis of the bound polymer chain homology (using polymer template similarity < 40) yielded only five clusters for bonded ligands and seven clusters for glycosylation. We exclude multi-residue glycans here because the RCSB validation report does not provide a ranking_model_fit value for them. The percentage of successful predictions (pocket r.m.s.d. < 2 Å) for multi-residue glycans on all-quality experimental data is 42.1% (n = 131 clusters), which is slightly lower than the success rate for single-residue glycans on all-quality experimental data of 46.1% (n = 167). The modified residues dataset is filtered similarly to our other polymer test sets: it contains only modified residues in polymer chains with low homology to the training set (Methods). See Extended Data Table 1 for detailed results, and Extended Data Fig. 6 for examples of predicted protein, DNA and RNA structures with covalent modifications, including analysis of the impact of phosphorylation on predictions.

While expanding in modelling abilities, AF3 has also improved in protein complex accuracy relative to AlphaFold-Multimer (v.2.3)7,8. Generally, protein–protein prediction success (DockQ > 0.23)40 has increased (paired Wilcoxon signed-rank test, P = 1.8 × 10−18), with antibody–protein interaction prediction in particular showing a marked improvement (Fig. 1c (right); paired Wilcoxon signed-rank test, P = 6.5 × 10−5, predictions top-ranked from 1,000 rather than the typical 5 seeds; further details are provided in Fig. 5a). Protein monomer LDDT improvement is also significant (paired Wilcoxon signed-rank test, P = 1.7 × 10−34). AF3 has a very similar dependence on MSA depth to AlphaFold-Multimer v.2.3; proteins with shallow MSAs are predicted with lower accuracy (a comparison of the dependence of single-chain LDDT on MSA depth is shown in Extended Data Fig. 7a).

Predicted confidences track accuracy
As with AF2, AF3 confidence measures are well calibrated with accuracy. Our confidence analysis is performed on the recent PDB evaluation set, with no homology filtering and including peptides. The ligands category is filtered to high-quality experimental structures as described above, and considers standard non-bonded ligands only. See Extended Data Fig. 8 for a similar assessment on bonded ligand and other interfaces. All statistics are cluster-weighted (Methods) and consider the top-ranked prediction only (ranking details are provided in Supplementary Methods 5.9.3).

In Fig. 4a (top row), we plot the chain pair interface-predicted TM (ipTM) score41 (Supplementary Methods 5.9.1) against interface accuracy measures: protein–protein DockQ, protein–nucleic interface LDDT (iLDDT) and protein–ligand success, with success defined as the percentage of examples under thresholded pocket-aligned r.m.s.d. values. In Fig. 4a (bottom row), we plot the average pLDDT per protein, nucleotide or ligand entity against our bespoke LDDT_to_polymer metric (metrics details are provided in the Methods), which is closely related to the training target of the pLDDT predictor.

Fig. 4: AF3 confidences track accuracy.
a, The accuracy of protein-containing interfaces as a function of chain pair ipTM (top). Bottom, the LDDT-to-polymer accuracy was evaluated for various chain types as a function of chain-averaged pLDDT. The box plots show the 25–75% confidence intervals (box limits), the median (centre line) and the 5–95% confidence intervals (whiskers). n values report the number of clusters in each band. b, The predicted structure of PDB 7T82 coloured by pLDDT (orange, 0–50; yellow, 50–70; cyan, 70–90; and blue, 90–100). c, The same prediction coloured by chain. d, DockQ scores for protein–protein interfaces. e, PAE matrix of same prediction (darker is more confident), with chain colouring of c on the side bars. The dashed black lines indicate the chain boundaries.

In Fig. 4b–e, we highlight a single example prediction of 7T82, in which per-atom pLDDT colouring identifies unconfident chain tails, somewhat confident interfaces and otherwise confident secondary structure. In Fig. 4c, the same prediction is coloured by chain, along with DockQ interface scores in Fig. 4d and per-chain colouring displayed on the axes for reference. We see from Fig. 4e that PAE confidence is high for pink–grey and blue–orange residue pairs for which DockQ > 0.7, and least confident about pink–orange and pink–blue residue pairs that have DockQ ≈ 0. A similar PAE analysis of an example with protein and nucleic acid chains is shown in Extended Data Fig. 5c,d.

Model limitations
We note model limitations of AF3 with respect to stereochemistry, hallucinations, dynamics and accuracy for certain targets.

On stereochemistry, we note two main classes of violations. The first is that the model outputs do not always respect chirality (Fig. 5b), despite the model receiving reference structures with correct chirality as input features. To address this in the PoseBusters benchmark, we included a penalty for chirality violation in our ranking formula for model predictions. Despite this, we still observe a chirality violation rate of 4.4% in the benchmark. The second class of stereochemical violations is a tendency of the model to occasionally produce overlapping (clashing) atoms in the predictions. This sometimes manifests as extreme violations in homomers in which entire chains have been observed to overlap (Fig. 5e). Penalizing clashes during ranking (Supplementary Methods 5.9.3) reduces the occurrence of this failure mode but does not eliminate them. Almost all remaining clashes occur for protein–nucleic complexes with both greater than 100 nucleotides and greater than 2,000 residues in total.

Fig. 5: Model limitations.
a, Antibody prediction quality increases with the number of model seeds. The quality of top-ranked, low-homology antibody–antigen interface predictions as a function of the number of seeds. Each datapoint shows the mean over 1,000 random samples (with replacement) of seeds to rank over, out of 1,200 seeds. Confidence intervals are 95% bootstraps over 10,000 resamples of cluster scores at each datapoint. Samples per interface are ranked by protein–protein ipTM. Significance tests were performed using by two-sided Wilcoxon signed-rank tests. n = 65 clusters. Exact P values were as follows: 2.0 × 10−5 (percentage correct) and P = 0.009 (percentage very high accuracy). b, Prediction (coloured) and ground-truth (grey) structures of Thermotoga maritima α-glucuronidase and beta-d-glucuronic acid—a target from the PoseBusters set (PDB: 7CTM). AF3 predicts alpha-d-glucuronic acid; the differing chiral centre is indicated by an asterisk. The prediction shown is top-ranked by ligand–protein ipTM and with a chirality and clash penalty. c, Conformation coverage is limited. Ground-truth structures (grey) of cereblon in open (apo, PDB: 8CVP; left) and closed (holo mezigdomide-bound, PDB: 8D7U; right) conformations. Predictions (blue) of both apo (with 10 overlaid samples) and holo structures are in the closed conformation. The dashed lines indicate the distance between the N-terminal Lon protease-like and C-terminal thalidomide-binding domain. d, A nuclear pore complex with 1,854 unresolved residues (PDB: 7F60). The ground truth (left) and predictions from AlphaFold-Multimer v.2.3 (middle) and AF3 (right) are shown. e, Prediction of a trinucleosome with overlapping DNA (pink) and protein (blue) chains (PDB: 7PEU); highlighted are overlapping protein chains B and J and self-overlapping DNA chain AA. Unless otherwise stated, predictions are top-ranked by our global complex ranking metric with chiral mismatch and steric clash penalties (Supplementary Methods 5.9.1).

We note that the switch from the non-generative AF2 model to the diffusion-based AF3 model introduces the challenge of spurious structural order (hallucinations) in disordered regions (Fig. 5d and Extended Data Fig. 1). Although hallucinated regions are typically marked as very low confidence, they can lack the distinctive ribbon-like appearance that AF2 produces in disordered regions. To encourage ribbon-like predictions in AF3, we use distillation training from AF2 predictions, and we add a ranking term to encourage results with more solvent accessible surface area36.

A key limitation of protein structure prediction models is that they typically predict static structures as seen in the PDB, not the dynamical behaviour of biomolecular systems in solution. This limitation persists for AF3, in which multiple random seeds for either the diffusion head or the overall network do not produce an approximation of the solution ensemble.

In some cases, the modelled conformational state may not be correct or comprehensive given the specified ligands and other inputs. For example, E3 ubiquitin ligases natively adopt an open conformation in an apo state and have been observed only in a closed state when bound to ligands, but AF3 exclusively predicts the closed state for both holo and apo systems42 (Fig. 5c). Many methods have been developed, particularly around MSA resampling, that assist in generating diversity from previous AlphaFold models43,44,45 and may also assist in multistate prediction with AF3.

Despite the large advance in modelling accuracy in AF3, there are still many targets for which accurate modelling can be challenging. To obtain the highest accuracy, it may be necessary to generate a large number of predictions and rank them, which incurs an extra computational cost. A class of targets in which we observe this effect strongly is antibody–antigen complexes, similar to other recent work46. Figure 5a shows that, for AF3, top-ranked predictions keep improving with more model seeds, even at as many as 1,000 (Wilcoxon signed-rank test between 5 and 1,000 seeds, P = 2.0 × 10−5 for percentage correct and P = 0.009 for percentage very high accuracy; ranking by protein–protein interface ipTM). This large improvement with many seeds is not observed in general for other classes of molecules (Extended Data Fig. 7b). Using only one diffusion sample per model seed for the AF3 predictions rather than five (not illustrated) does not change the results significantly, indicating that running more model seeds is necessary for antibody score improvements, rather than just more diffusion samples.

Discussion
The core challenge of molecular biology is to understand and ultimately regulate the complex atomic interactions of biological systems. The AF3 model takes a large step in this direction, demonstrating that it is possible to accurately predict the structure of a wide range of biomolecular systems in a unified framework. Although there are still substantial challenges to achieve highly accurate predictions across all interaction types, we demonstrate that it is possible to build a deep-learning system that shows strong coverage and generalization for all of these interactions. We also demonstrate that the lack of cross-entity evolutionary information is not a substantial blocker to progress in predicting these interactions and, moreover, substantial improvement in antibody results suggests AlphaFold-derived methods are able to model the chemistry and physics of classes of molecular interactions without dependence on MSAs. Finally, the large improvement in protein–ligand structure prediction shows that it is possible to handle the wide diversity of chemical space within a general deep-learning framework and without resorting to an artificial separation between protein structure prediction and ligand docking.

The development of bottom-up modelling of cellular components is a key step in unravelling the complexity of molecular regulation within the cell, and the performance of AF3 shows that developing the right deep-learning frameworks can massively reduce the amount of data required to obtain biologically relevant performance on these tasks and amplify the impact of the data already collected. We expect that structural modelling will continue to improve not only due to advances in deep learning but also because continuing methodological advances in experimental structure determination, such as the substantial improvements in cryo-electron microscopy and tomography, will provide a wealth of new training data to further the improve the generalization ability of such models. The parallel developments of experimental and computational methods promise to propel us further into an era of structurally informed biological understanding and therapeutic development.

Methods
Full algorithm details
Extensive explanations of the components are available in Supplementary Methods 2–5. Moreover, pseudocode is available in Supplementary Algorithms 1–31, network diagrams in Figs. 1d and  2a–c and Supplementary Fig. 2, input features in Supplementary Table 5 and additional hyperparameters for training in Supplementary Tables 3, 4 and 7.

Training regime
No structural data used during training were released after 30 September 2021 and, for the model used in PoseBusters evaluations, we filtered out PDB32 structures released after 30 September 2021. One optimizer step uses a mini batch of 256 input data samples and during initial training 256 × 48 = 12,288 diffusion samples. For fine-tuning, the number of diffusion samples is reduced to 256 × 32 = 8,192. The model is trained in three stages—the initial training with a crop size of 384 tokens and two sequential fine tuning stages with crop sizes of 640 and 768 tokens. Further details are provided in Supplementary Methods 5.2.

Inference regime
No inference time templates or reference ligand position features were released after 30 September 2021, and in the case of PoseBusters evaluation, an earlier cut-off date of 30 September 2019 was used. The model can be run with different random seeds to generate alternative results, with a batch of diffusion samples per seed. Unless otherwise stated, all results are generated by selecting the top confidence sample from running 5 seeds of the same trained model, with 5 diffusion samples per model seed, for a total of 25 samples to choose from. Standard crystallization aids are excluded from predictions (Supplementary Table 8).

Results are shown for the top-ranked sample and sample ranking depends on whether trying to select the overall best output globally, or the best output for some chain, interface or modified residue. Global ranking uses a mix of pTM and ipTM along with terms to reduce cases with large numbers of clashes and increase rates of disorder; individual chain ranking uses a chain specific pTM measure; interface ranking uses a bespoke ipTM measure for the relevant chain pair; and modified residue ranking uses average pLDDT over the residue of interest (Supplementary Methods 5.9.3).

Metrics
Evaluation compares a predicted structure to the corresponding ground-truth structure. If the complex contains multiple identical entities, assignment of the predicted units to the ground-truth units is found by maximizing LDDT. Assignment in local symmetry groups of atoms in ligands is solved by exhaustive search over the first 1,000 per-residue symmetries as given by RDKit.

We measure the quality of the predictions with DockQ, LDDT or pocket-aligned r.m.s.d. For nucleic–protein interfaces, we measure interface accuracy through iLDDT, which is calculated from distances between atoms across different chains in the interface. DockQ and iLDDT are highly correlated (Extended Data Fig. 9), so the standard cut-offs for DockQ can be translated to equivalent iLDDT cut-offs. Nucleic acid LDDTs (intrachains and interface) were calculated with an inclusion radius of 30 Å compared with the usual 15 Å used for proteins, owing to their larger scale. For confidence calibration assessment, we use a bespoke LDDT (LDDT_to_polymer) metric that considers differences from each atom of a given entity to any Cα or C1′ polymer atom within its inclusion radius. This is closely related to how the confidence prediction is trained (Supplementary Methods 4.3.1).

Pocket-aligned r.m.s.d. is computed as follows: the pocket is defined as all heavy atoms within 10 Å of any heavy atom of the ligand, restricted to the primary polymer chain for the ligand or modified residue being scored, and further restricted to only backbone atoms for proteins. The primary polymer chain is defined variously: for PoseBusters, it is the protein chain with the most atoms within 10 Å of the ligand; for bonded ligand scores, it is the bonded polymer chain; and for modified residues, it is the chain in which the residue is contained (minus that residue). The pocket is used to align the predicted structure to the ground-truth structure with least-squares rigid alignment and then the r.m.s.d. is computed on all heavy atoms of the ligand.

Recent PDB evaluation set
General model evaluation was performed on our recent PDB set consisting of 8,856 PDB complexes released between 1 May 2022 and 12 January 2023. The set contains almost all PDB complexes released during that period that are less than 5,120 model tokens in size (Supplementary Methods 6.1). Single chains and interfaces within each structure were scored separately rather than only looking at full complex scores, and clustering was then applied to chains and interfaces so that scores could be aggregated first within clusters and then across clusters for mean scores, or using a weighting of inverse cluster size for distributional statistics (Supplementary Methods 6.2 and 6.4).

Evaluation on ligands excludes standard crystallization aids (Supplementary Table 8), our ligand exclusion list (Supplementary Table 9) and glycans (Supplementary Table 10). Bonded and non-bonded ligands are evaluated separately. Ions are only included when specifically mentioned (Supplementary Table 11).

The recent PDB set is filtered to a low homology subset (Supplementary Methods 6.1) for some results where stated. Homology is defined as sequence identity to sequences in the training set and is measured by template search (Supplementary Methods 2.4). Individual polymer chains in evaluation complexes are filtered out if the maximum sequence identity to chains in the training set is greater than 40%, where sequence identity is the percentage of residues in the evaluation set chain that are identical to the training set chain. Individual peptide chains (protein chains with less than 16 residues) are always filtered out. For polymer–polymer interfaces, if both polymers have greater than 40% sequence identity to two chains in the same complex in the training set, then the interface is filtered out. For interfaces to a peptide, the interface is filtered out if the non-peptide entity has greater than 40% sequence identity to any chain in the training set.

To compare the quality of prediction of protein–protein interfaces and protein monomers against that of AlphaFold-Multimer (v.2.3)8, and to compare the dependence of single-protein-chain prediction quality on MSA depth, we restrict the low-homology recent PDB set to complexes with fewer than 20 protein chains and fewer than 2,560 tokens. We compare against unrelaxed AlphaFold-Multimer v.2.3 predictions.

To study antibody-antigen interface prediction, we filter the low homology recent PDB set to complexes that contain at least one protein–protein interface where one of the protein chains is in one of the two largest PDB chain clusters (these clusters are representative of antibodies). We further filter to complexes with at most 2,560 tokens and with no unknown amino acids in the PDB to allow extensive comparison against relaxed predictions of AlphaFold-Multimer v2.3. That leaves 71 antibody–antigen complexes, containing 166 antibody–antigen interfaces spanning 65 interface clusters.

MSA depth analysis (Extended Data Fig. 7a) was based on computing the normalized number of effective sequences (Neff) for each position of a query sequence. Per-residue Neff values were obtained by counting the number of non-gap residues in the MSA for this position and weighting the sequences using the Neff scheme49 with a threshold of 80% sequence identity measured on the region that is non-gap in either sequence.

Nucleic acid prediction baseline
For benchmarking performance on nucleic acid structure prediction, we report baseline comparisons to an existing machine learning system for protein–nucleic acid and RNA tertiary structure prediction, RoseTTAFold2NA18. We run the open source RF2NA50 with the same MSAs as those that were used for AF3 predictions. For comparison between AF3 and RF2NA, a subset of our recent PDB set was chosen to meet the RF2NA criteria (<1,000 total residues and nucleotides). As RF2NA was not trained to predict systems with DNA and RNA, analysis is limited to targets with only one nucleic acid type. No system was publicly available at time of writing for baseline comparisons on data with arbitrary combinations of biomolecular types in PDB.

As an additional baseline for RNA tertiary structure prediction, we evaluate AF3 performance on CASP15 RNA targets that were publicly available as of 1 December 2023 (R1116/8S95, R1117/8FZA, R1126 (downloaded from the CASP15 website https://predictioncenter.org/casp15/TARGETS_PDB/R1126.pdb), R1128/8BTZ, R1136/7ZJ4, R1138/[7PTK/7PTL], R1189/7YR7 and R1190/7YR6). We compare the top-1 ranked predictions and, where multiple ground-truth structures exist (R1136), the prediction is scored against the closest state. We display comparisons to RF2NA as a representative machine learning system; AIchemy_RNA2 as the top performing entrant with human intervention; and AIchemy_RNA as the top performing machine learning system. All entrants’ predictions were downloaded from the CASP website and scored internally.

PoseBusters
While other analyses used an AlphaFold model trained on PDB data released before a cut-off of 30 September 2021, our PoseBusters analysis was conducted on a model (with identical architecture and similar training schedule) differing only in the use of an earlier 30 September 2019 cut-off. This analysis therefore did not include training data, inference time templates or ‘ref_pos’ features released after this date.

Inference was performed on the asymmetric unit from specified PDBs, with the following minor modifications. In several PDB files, chains clashing with the ligand of interest were removed (7O1T, 7PUV, 7SCW, 7WJB, 7ZXV, 8AIE). Another PDB entry (8F4J) was too large to inference the entire system (over 5,120 tokens), so we included only protein chains within 20 Å of the ligand of interest. Five model seeds, each with five diffusion samples, were produced per target, resulting in 25 predictions, which were ranked by quality and predicted accuracy: the ranking score was calculated from an ipTM aggregate (Supplementary Methods 5.9.3 (point 3)), then further divided by 100 if the ligand had chirality errors or had clashes with the protein.

For pocket-aligned r.m.s.d., first alignment between the predicted and ground-truth structures was conducted by aligning to the ground-truth pocket backbone atoms (CA, C or N atoms within 10 Å of the ligand of interest) from the primary protein chain (the chain with the greatest number of contacts within 10 Å of the ligand). The PoseBusters Python package v.0.2.751 was used to score r.m.s.d. and violations from the pocket-aligned predictions.

While AlphaFold models are ‘blind’ to the protein pocket, docking is often performed with knowledge of the protein pocket residues. For example, Uni-Mol specifies the pocket as any residue within 6 Å of the heavy atoms in the ligand of interest26. To evaluate the ability of AF3 to dock ligands accurately when given pocket information, we fine-tuned a 30 September 2019 cut-off AF3 model with an additional token feature specifying pocket–ligand pairs (Supplementary Methods 2.8). Specifically, an additional token feature was introduced, set to true for a ligand entity of interest and any pocket residues with heavy atoms within 6 Å of the ligand entity. At training time, a single random ligand entity is chosen to use in this feature. Note that multiple ligand chains with the same entity (CCD code) may be selected. At inference time, the ligand entity was chosen based on the ligand of interest’s CCD code, so again multiple ligand chains were occasionally chosen. The results of this analysis are shown in Extended Data Fig. 4.

Extended data figures and tables
Extended Data Fig. 1 Disordered region prediction.
a, Example prediction for a disordered protein from AlphaFoldMultimer v2.3, AlphaFold 3, and AlphaFold 3 trained without the disordered protein PDB cross distillation set. Protein is DP02376 from the CAID 2 (Critical Assessment of protein Intrinsic Disorder prediction) set. Predictions coloured by pLDDT (orange: pLDDT <= 50, yellow: 50 < pLDDT <= 70, light blue: 70 < pLDDT <= 90, and dark blue: 90 <= pLDDT < 100). b, Predictions of disorder across residues in proteins in the CAID 2 set, which are also low homology to the AF3 training set. Prediction methods include RASA (relative accessible surface area) and pLDDT (N = 151 proteins; 46,093 residues).

Extended Data Fig. 2 Accuracy across training.
Training curves for initial training and fine tuning showing LDDT (local distance difference test) on our evaluation set as a function of optimizer steps. One optimizer step uses a mini batch of 256 trunk samples and during initial training 256 * 48 = 12,288 diffusion samples. For fine tuning the number of diffusion samples is reduced to 256 * 32 = 8,192. The scatter plot shows the raw data points and the lines show the smoothed performance using a median filter with a kernel width of 9 data points. The dashed lines mark the points where the smoothed performance passes 90% and 97% of the initial training maximum for the first time.

Extended Data Fig. 3 AlphaFold 3 predictions of PoseBusters examples for which Vina and Gold were inaccurate.
Predicted protein chains are shown in blue, predicted ligands in orange, and ground truth in grey. a, Human Notum bound to inhibitor ARUK3004556 (PDB ID 8BTI, ligand RMSD: 0.65 Å). b, Pseudomonas sp. PDC86 Aapf bound to HEHEAA (PDB ID 7KZ9, ligand RMSD: 1.3 Å). c, Human Galectin-3 carbohydrate-recognition domain in complex with compound 22 (PDB ID 7XFA, ligand RMSD: 0.44 Å).

Extended Data Fig. 4 PoseBusters analysis.
a, Comparison of AlphaFold 3 and baseline method protein-ligand binding success on the PoseBusters Version 1 benchmark set (V1, August 2023 release). Methods classified by the extent of ground truth information used to make predictions. Note all methods that use pocket residue information except for UMol and AF3 also use ground truth holo protein structures. b, PoseBusters Version 2 (V2, November 2023 release) comparison between the leading docking method Vina and AF3 2019 (two-sided Fisher exact test, N = 308 targets, p = 2.3 * 10−8). c, PoseBusters V2 results of AF3 2019 on targets with low, moderate, and high protein sequence homology (integer ranges indicate maximum sequence identity with proteins in the training set). d, PoseBusters V2 results of AF3 2019 with ligands split by those characterized as “common natural” ligands and others. “Common natural” ligands are defined as those which occur greater than 100 times in the PDB and which are not non-natural (by visual inspection). A full list may be found in Supplementary Table 15. Dark bar indicates RMSD < 2 Å and passing PoseBusters validity checks (PB-valid). e, PoseBusters V2 structural accuracy and validity. Dark bar indicates RMSD < 2 Å and passing PoseBusters validity checks (PB-valid). Light hashed bar indicates RMSD < 2 Å but not PB valid. f, PoseBusters V2 detailed validity check comparison. Error bars indicate exact binomial distribution 95% confidence intervals. N = 427 targets for RoseTTAFold All-Atom and 428 targets for all others in Version 1; 308 targets in Version 2.

Extended Data Fig. 5 Nucleic acid prediction accuracy and confidences.
a, CASP15 RNA prediction accuracy from AIChemy_RNA (the top AI-based submission), RoseTTAFold2NA (the AI-based method capable of predicting proteinRNA complexes), and AlphaFold 3. Ten of the 13 targets are available in the PDB or via the CASP15 website for evaluation. Predictions are downloaded from the CASP website for external models. b, Accuracy on structures containing low homology RNA-only or DNA-only complexes from the recent PDB evaluation set. Comparison between AlphaFold 3 and RoseTTAFold2NA (RF2NA) (RNA: N = 29 structures, paired Wilcoxon signed-rank test, p = 1.6 * 10−7; DNA: N = 63 structures, paired two-sided Wilcoxon signed-rank test, p = 5.2 * 10−12). Note RF2NA was only trained and evaluated on duplexes (chains forming at least 10 hydrogen bonds), but some DNA structures in this set may not be duplexes. Box, centerline, and whiskers boundaries are at (25%, 75%) intervals, median, and (5%, 95%) intervals. c Predicted structure of a mycobacteriophage immunity repressor protein bound to double stranded DNA (PDB ID 7R6R), coloured by pLDDT (left; orange: 0–50, yellow: 50–70, cyan 70–90, and blue 90–100) and chain id (right). Note the disordered N-terminus not entirely shown. d, Predicted aligned error (PAE) per token-pair for the prediction in c with rows and columns labelled by chain id and green gradient indicating PAE.

Extended Data Fig. 6 Analysis and examples for modified proteins and nucleic acids.
a, Accuracy on structures. containing common phosphorylation residues (SEP, TPO, PTR, NEP, HIP) from the recent PDB evaluation set. Comparison between AlphaFold 3 with phosphorylation modelled, and AlphaFold 3 without modelling phosphorylation (N = 76 clusters, paired two-sided Wilcoxon signed-rank test, p = 1.6 * 10−4). Note, to predict a structure without modelling phosphorylation, we predict the parent (standard) residue in place of the modification. AlphaFold 3 generally achieves better backbone accuracy when modelling phosphorylation. Error bars indicate exact binomial distribution 95% confidence intervals. b, SPOC domain of human SHARP in complex with phosphorylated RNA polymerase II C-terminal domain (PDB ID 7Z1K), predictions coloured by pLDDT (orange: 0–50, yellow: 50–70, cyan 70–90, and blue 90–100). Left: Phosphorylation modelled (mean pocket-aligned RMSDCα 2.104 Å). Right: Without modelling phosphorylation (mean pocketaligned RMSDCα 10.261 Å). When excluding phosphorylation, AlphaFold 3 provides lower pLDDT confidence on the phosphopeptide. c, Structure of parkin bound to two phospho-ubiquitin molecules (PDB ID 7US1), predictions similarly coloured by pLDDT. Left: Phosphorylation modelled (mean pocket-aligned RMSDCα 0.424 Å). Right: Without modelling phosphorylation (mean pocket-aligned RMSDCα 9.706 Å). When excluding phosphorylation, AlphaFold 3 provides lower pLDDT confidence on the interface residues of the incorrectly predicted ubiquitin. d, Example structures with modified nucleic acids. Left: Guanosine monophosphate in RNA (PDB ID 7TNZ, mean pocket-aligned modified residue RMSD 0.840 Å). Right: Methylated DNA cytosines (PDB ID 7SDW, mean pocket-aligned modified residue RMSD 0.502 Å). Welabel residues of the predicted structure for reference. Ground truth structure in grey; predicted protein in blue, predicted RNA in purple, predicted DNA in magenta, predicted ions in orange, with predicted modifications highlighted via spheres.

Extended Data Fig. 7 Model accuracy with MSA size and number of seeds.
a, Effect of MSA depth on protein prediction accuracy. Accuracy is given as single chain LDDT score and MSA depth is computed by counting the number of non-gap residues for each position in the MSA using the Neff weighting scheme and taking the median across residues (see Methods for details on Neff). MSA used for AF-M 2.3 differs slightly from AF3; the data uses the AF3 MSA depth for both to make the comparison clearer. The analysis uses every protein chain in the low homology Recent PDB set, restricted to chains in complexes with fewer than 20 protein chains and fewer than 2,560 tokens (see Methods for details on Recent PDB set and comparisons to AF-M 2.3). The curves are obtained through Gaussian kernel average smoothing (window size is 0.2 units in log10(Neff)); the shaded area is the 95% confidence interval estimated using bootstrap of 10,000 samples. b, Increase in ranked accuracy with number of seeds for different molecule types. Predictions are ranked by confidence, and only the most confident per interface is scored. Evaluated on the low homology recent PDB set, filtered to less than 1,536 tokens. Number of clusters evaluated: dna-intra = 386, protein-intra = 875, rnaintra = 78, protein-dna = 307, protein-rna = 102, protein-protein (antibody = False) = 697, protein-protein (antibody = True) = 58. Confidence intervals are 95% bootstraps over 1,000 samples.

Extended Data Fig. 8 Relationship between confidence and accuracy for protein interactions with ions, bonded ligands and bonded glycans.
Accuracy is given as the percentage of interface clusters under various pocket-aligned RMSD thresholds, as a function of the chain pair ipTM of the interface. The ions group includes both metals and nonmetals. N values report the number of clusters in each band. For a similar analysis on general ligand-protein interfaces, see Fig. 4 of main text.

Extended Data Fig. 9 Correlation of DockQ and iLDDT for protein-protein interfaces.
One data point per cluster, 4,182 clusters shown. Line of best fit with a Huber regressor with epsilon 1. DockQ categories correct (>0.23), and very high accuracy (>0.8) correspond to iLDDTs of 23.6 and 77.6 respectively.   

References

Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583–589 (2021).


Kreitz, J. et al. Programmable protein delivery with a bacterial contractile injection system. Nature 616, 357–364 (2023).


Lim, Y. et al. In silico protein interaction screening uncovers DONSON’s role in replication initiation. Science 381, eadi3448 (2023).


Mosalaganti, S. et al. AI-based structure prediction empowers integrative structural analysis of human nuclear pores. Science 376, eabm9506 (2022).


Anand, N. & Achim, T. Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. Preprint at arXiv https://doi.org/10.48550/arXiv.2205.15019 (2022).
Yang, Z., Zeng, X., Zhao, Y. & Chen, R. AlphaFold2 and its applications in the fields of biology and medicine. Signal Transduct. Target. Ther. 8, 115 (2023).


Evans, R. et al. Protein complex prediction with AlphaFold-Multimer. Preprint at bioRxiv https://doi.org/10.1101/2021.10.04.463034 (2022).
Židek, A. AlphaFold v.2.3.0 Technical Note. GitHub https://github.com/google-deepmind/alphafold/blob/main/docs/technical_note_v2.3.0.md (2022).
Isert, C., Atz, K. & Schneider, G. Structure-based drug design with geometric deep learning. Curr. Opin. Struct. Biol. 79, 102548 (2023).
Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. Science 379, 1123–1130 (2023).
Article ADS MathSciNet CAS PubMed Google Scholar 

Baek, M. et al. Accurate prediction of protein structures and interactions using a three-track neural network. Science https://doi.org/10.1126/science.abj8754 (2021).
Wu, R. et al. High-resolution de novo structure prediction from primary sequence. Preprint at bioRxiv https://doi.org/10.1101/2022.07.21.500999 (2022).
Bryant, P., Pozzati, G. & Elofsson, A. Improved prediction of protein-protein interactions using AlphaFold2. Nat. Commun. 13, 1265 (2022).


Moriwaki, Y. Post on X. X https://x.com/Ag_smith/status/1417063635000598528?lang=en-GB (2021).
Baek, M. Post on X. X https://x.com/minkbaek/status/1417538291709071362?lang=en (2021).
Qiao, Z. et al. State-specific protein–ligand complex structure prediction with a multiscale deep generative model. Nat. Mach. Intell. 6, 195–208 (2024).
Nakata, S., Mori, Y. & Tanaka, S. End-to-end protein–ligand complex structure generation with diffusion-based generative models. BMC Bioinform. 24, 233 (2023).


Baek, M. et al. Accurate prediction of protein–nucleic acid complexes using RoseTTAFoldNA. Nat. Methods 21, 117–121 (2024).
Townshend, R. J. L. et al. Geometric deep learning of RNA structure. Science 373, 1047–1051 (2021).


Jiang, D. et al. InteractionGraphNet: a novel and efficient deep graph representation learning framework for accurate protein-ligand interaction predictions. J. Med. Chem. 64, 18209–18232 (2021).


Jiang, H. et al. Predicting protein–ligand docking structure with graph neural network. J. Chem. Inf. Model. https://doi.org/10.1021/acs.jcim.2c00127 (2022).
Corso, G., Stärk, H., Jing, B., Barzilay, R. & Jaakkola, T. DiffDock: diffusion steps, twists, and turns for molecular docking. Preprint at arXiv https://doi.org/10.48550/arXiv.2210.01776 (2022).
Stärk, H., Ganea, O., Pattanaik, L., Barzilay, D. & Jaakkola, T. EquiBind: Geometric deep learning for drug binding structure prediction. In Proc. 39th International Conference on Machine Learning (eds Chaudhuri, K. et al.) 20503–20521 (PMLR, 2022).
Liao, Z. et al. DeepDock: enhancing ligand-protein interaction prediction by a combination of ligand and structure information. In Proc. 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) 311–317 (IEEE, 2019).
Lu, W. et al. TANKBind: trigonometry-aware neural networks for drug-protein binding structure prediction. Adv. Neural Inf. Process. Syst. 35, 7236–7249 (2022).


Zhou, G. et al. Uni-Mol: a universal 3D molecular representation learning framework. Preprint at ChemRxiv https://chemrxiv.org/engage/chemrxiv/article-details/6402990d37e01856dc1d1581 (2023).
Shen, T. et al. E2Efold-3D: end-to-end deep learning method for accurate de novo RNA 3D structure prediction. Preprint at arXiv https://arxiv.org/abs/2207.01586 (2022).
van Dijk, M. & Bonvin, A. M. J. J. Pushing the limits of what is achievable in protein–DNA docking: benchmarking HADDOCK’s performance. Nucleic Acids Res. 38, 5634–5647 (2010).


Krishna, R. et al. Generalized biomolecular modeling and design with RoseTTAFold All-Atom. Science 384, eadl2528 (2024).


Buttenschoen, M., Morris, G. M. & Deane, C. M. PoseBusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences. Chem. Sci. 15, 3130–3139 (2024).
Das, R. et al. Assessment of three-dimensional RNA structure prediction in CASP15. Proteins 91, 1747–1770 (2023).


Berman, H. M. et al. The Protein Data Bank. Nucleic Acids Res. 28, 235–242 (2000).


Karras, T., Aittala, M., Aila, T. & Laine, S. Elucidating the design space of diffusion-based generative models. Adv. Neural Inf. Process. Syst. 35, 26565–26577 (2022).
Wang, Y., Elhag, A. A., Jaitly, N., Susskind, J. M. & Bautista, M. A. Generating molecular conformer fields. Preprint at arXiv https://doi.org/10.48550/arXiv.2311.17932 (2023).
Ji, Z., et al. Survey of hallucination in natural language generation. ACM Comput. Surv. 55, 248 (2023).
Del Conte, A. et al. Critical assessment of protein intrinsic disorder prediction (CAID)—results of round 2. Proteins 91, 1925–1934 (2023).


Trott, O. & Olson, A. J. AutoDock Vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading. J. Comput. Chem. 31, 455–461 (2010).


Miller, E. B. et al. Reliable and accurate solution to the induced fit docking problem for protein–ligand binding. J. Chem. Theory Comput. https://doi.org/10.1021/acs.jctc.1c00136 (2021).
Chen, K., Zhou, Y., Wang, S. & Xiong, P. RNA tertiary structure modeling with BRiQ potential in CASP15. Proteins 91, 1771–1778 (2023).


Basu, S. & Wallner, B. DockQ: a quality measure for protein-protein docking models. PLoS ONE 11, e0161879 (2016).


Zhang, Y. & Skolnick, J. Scoring function for automated assessment of protein structure template quality. Proteins 57, 702–710 (2004).


Watson, E. R. et al. Molecular glue CELMoD compounds are regulators of cereblon conformation. Science 378, 549–553 (2022).


Wayment-Steele, H. K. et al. Predicting multiple conformations via sequence clustering and AlphaFold2. Nature 625, 832–839 (2024).
del Alamo, D., Sala, D., Mchaourab, H. S. & Meiler, J. Sampling alternative conformational states of transporters and receptors with AlphaFold2. eLife https://doi.org/10.7554/eLife.75751 (2022).
Heo, L. & Feig, M. Multi-state modeling of G-protein coupled receptors at experimental accuracy. Proteins 90, 1873–1885 (2022).


Wallner, B. AFsample: improving multimer prediction with AlphaFold using massive sampling. Bioinformatics 39, btad573 (2023).
Mariani, V., Biasini, M., Barbato, A. & Schwede, T. lDDT: a local superposition-free score for comparing protein structures and models using distance difference tests. Bioinformatics 29, 2722–2728 (2013).


Zemla, A. LGA: A method for finding 3D similarities in protein structures. Nucleic Acids Res. 31, 3370–3374 (2003).
Wu, T., Hou, J., Adhikari, B. & Cheng, J. Analysis of several key factors influencing deep learning-based inter-residue contact prediction. Bioinformatics 36, 1091–1098 (2020).


DiMaio, F. RF2NA v.0.2. GitHub https://github.com/uw-ipd/RoseTTAFold2NA/releases/tag/v0.2 (2023).
Buttenschoen, M. PoseBusters v.0.2.7. GitHub https://github.com/maabuu/posebusters/releases/tag/v0.2.7 (2023).
Werel, L. et al. Structural basis of dual specificity of Sinorhizobium meliloti Clr, a cAMP and cGMP receptor protein. MBio 14, e0302822 (2023).


Wang, C. et al. Antigenic structure of the human coronavirus OC43 spike reveals exposed and occluded neutralizing epitopes. Nat. Commun. 13, 2921 (2022).


Lapointe, C. P. et al. eIF5B and eIF1A reorient initiator tRNA to allow ribosomal subunit joining. Nature 607, 185–190 (2022).


Wilson, L. F. L. et al. The structure of EXTL3 helps to explain the different roles of bi-domain exostosins in heparan sulfate synthesis. Nat. Commun. 13, 3314 (2022).


Liu, X. et al. Highly active CAR T cells that bind to a juxtamembrane region of mesothelin and are not blocked by shed mesothelin. Proc. Natl Acad. Sci. USA 119, e2202439119 (2022).


Liu, Y. et al. Mechanisms and inhibition of Porcupine-mediated Wnt acylation. Nature 607, 816–822 (2022).


Kurosawa, S. et al. Molecular basis for enzymatic aziridine formation via sulfate elimination. J. Am. Chem. Soc. 144, 16164–16170 (2022).


Boffey, H. K. et al. Development of selective phosphatidylinositol 5-phosphate 4-kinase γ inhibitors with a non-ATP-competitive, allosteric binding mode. J. Med. Chem. 65, 3359–3370 (2022).


Buckley, P. T. et al. Multivalent human antibody-centyrin fusion protein to prevent and treat Staphylococcus aureus infections. Cell Host Microbe 31, 751–765 (2023).


Mohapatra, S. B. & Manoj, N. Structural basis of catalysis and substrate recognition by the NAD(H)-dependent α-d-glucuronidase from the glycoside hydrolase family 4. Biochem. J. 478, 943–959 (2021).


Gao, X. et al. Structural basis for Sarbecovirus ORF6 mediated blockage of nucleocytoplasmic transport. Nat. Commun. 13, 4782 (2022).


Atkinson, B. N. et al. Designed switch from covalent to non-covalent inhibitors of carboxylesterase Notum activity. Eur. J. Med. Chem. 251, 115132 (2023).


Luo, S. et al. Structural basis for a bacterial Pip system plant effector recognition protein. Proc. Natl Acad. Sci. USA 118, e2019462118 (2021).


Liu, C. et al. Identification of monosaccharide derivatives as potent, selective, and orally bioavailable inhibitors of human and mouse galectin-3. J. Med. Chem. 65, 11084–11099 (2022).


Dombrowski, M., Engeholm, M., Dienemann, C., Dodonova, S. & Cramer, P. Histone H1 binding to nucleosome arrays depends on linker DNA length and trajectory. Nat. Struct. Mol. Biol. 29, 493–501 (2022).


Vecchioni, S. et al. Metal-mediated DNA nanotechnology in 3D: structural library by templated diffraction. Adv. Mater. 35, e2210938 (2023).


Wang, W. & Pyle, A. M. The RIG-I receptor adopts two different conformations for distinguishing host from viral RNA ligands. Mol. Cell 82, 4131–4144 (2022).


McGinnis, R. J. et al. A monomeric mycobacteriophage immunity repressor utilizes two domains to recognize an asymmetric DNA sequence. Nat. Commun. 13, 4105 (2022).


Dietrich, M. H. et al. Nanobodies against Pfs230 block Plasmodium falciparum transmission. Biochem. J. 479, 2529–2546 (2022).


Appel, L.-M. et al. The SPOC domain is a phosphoserine binding module that bridges transcription machinery with co- and post-transcriptional regulators. Nat. Commun. 14, 166 (2023).


Yin, Y. et al. Impact of cytosine methylation on DNA binding specificities of human transcription factors. Science 356, eaaj2239 (2017).


Jolma, A. et al. DNA-dependent formation of transcription factor pairs alters their binding specificity. Nature 527, 384–388 (2015).





Supplementary Information
Supplementary Information 1 (notation), 2 (data pipeline), 3 (model architecture), 4 (auxiliary heads), 5 (training and inference), 6 (evaluation), 7 (differences to AlphaFold2 and AlphaFold-Multimer), 8 (Supplementary Results) and 9 (Appendix, including CCD Code and PDB ID tables).

1 Notation
The notation used below largely follows the convention from the AlphaFold 2 paper [1], which we summarize here for
convenience to the reader.
We denote the number of tokens – the fundamental sequential unit entering the Pairformer – by Ntoken (cropped
during training), the number of templates used in the model by Ntempl, the number of MSA rows used in the model by
Nmsa. Concrete values for these parameters are given in the data pipeline (section 2) and training details (section 5).
On the model side, we also denote the number of blocks in Pairformer-like stacks by Nblock (subsection 3.6), and the
number of recycling iterations by Ncycle.
We present architectural details in Algorithms, where we use the following conventions. We use capitalized operator
names when they encapsulate learnt parameters, e.g. we use Linear for a linear transformation with a weights matrix W
and a bias vector b, and LinearNoBias for the linear transformation without the bias vector. Note that when we have
multiple outputs from the Linear operator at the same line of an algorithm, we imply different trainable weights for
each output. A subscript index at the linear operator, e.g., LinearNoBiasa indicates also set of different trainable
weights, where a selects the weight matrix to be used for that sample (used in Algorithm 31). We use LayerNorm
for the layer normalization [2] operating on the channel dimensions with learnable per-channel gains and biases. We
also use capitalized names for random operators, such as generators for random augmentations. For functions without
parameters we use lower case operator names, e.g. sigmoid, softmax, stopgrad. We use ⊙ for the element-wise
multiplication, ⊗ for the outer product, ⊕ for the outer sum, and a
⊤b for the dot product of two vectors. Indices i, j, k
always operate on the token dimension, indices l, m on the flat atom dimension, indices s, t on the sequence dimension
(e.g. indexing MSA sequences or diffusion’s time dimension), and index h on the attention heads dimension. The
channel dimension is implicit and we type the channel-wise vectors in bold, e.g. zij . Algorithms operate on sets of
such vectors, e.g. we use {zij} to denote all pair representations. Atomic positions (i.e. vectors in Euclidean space)
are specified as ⃗x ∈ R3
. A full atomic structure is represented as a flat atom list {⃗xl} where l indexes the atom.
The mapping between tokens and their corresponding atoms is denoted i(l) ∈ N which maps the flat-atom index
l to the token index i and a = token_atom_idx(l) that maps the flat-atom index l to the within-token atom index
a ∈ {1, . . . , Nmax_atoms_per_token}.
2 Data pipeline
The data pipeline is the first step when running AlphaFold. It takes an input an mmCIF file and produces input features
for the model.
2.1 Parsing
The data pipeline operates on mmCIF format files. In addition to parsing the _atom_site section, we extract basic metadata (resolution, release date, method) and some non-coordinate information (bioassembly details, chemical
component details, chain names and sequences, and covalent bond information).
To simplify later code, the parser performs some basic structure cleanup. Alternative locations for atoms/residues are
resolved by taking the one with the largest occupancy, MSE residues are converted to MET residues, waters are removed,
and arginine naming ambiguities are fixed (ensuring NH1 is always closer to CD than NH2). The first bioassembly is
then expanded, to encourage the model to predict biologically relevant complexes.
During inference a dummy mmCIF is used as input with all atom coordinates zeroed out. This can either be constructed from an mmCIF deposited in PDB[3], or from a collection of CCD code sequences and SMILES strings
provided by the user.
2.2 Genetic search
We have 5 databases available to search for protein chains (see Table 1)
Jackhmmer searches use the following additional flags: -N 1 -E 0.0001 –incE 0.0001 –F1 0.0005 –F2 0.00005 –F3
0.0000005.
HHBlits searches use: -n 3 -e 0.001 -realign_max 100000 -maxfilt 100000 -min_prefilter_hits 1000 -p 20 -Z 500.
A further 3 databases are available for RNA chains (see Table 2). These databases are pre-processed by filtering to
RNA entries only if necessary, then clustering with settings: --min-seq-id 0.9 -c 0.8. We search the cluster representative
sequences.
Nhmmer searches use flags: -E 0.001 --incE 0.001 --rna --watson --F3 0.00005 (--F3 0.02 for sequences shorter than
50 nucleotides). RNA hit sequences are realigned to the query with hmmalign.
Supplementary information for AlphaFold 3 2
Table 1 Genetics databases for protein chains
Database Search tool Database-specific flags Max sequences
UniRef90 [4] jackhmmer [5]
∗
--seq_limit 100000 10,000
UniProt [6] jackhmmer --seq_limit 500000 50,000
Uniclust30 [7] + BFD [8] HHBlits v3.0-beta.3 [9] None
Reduced BFD [10] jackhmmer --seq_limit 50000 5,000
MGnify [11] jackhmmer --seq_limit 50000 5,000
∗All programs from the HMMER suite [12] are v3.3.
Table 2 Genetics databases for RNA chains
Database Clustering tool Search tool Max sequences
Rfam [13] mmseqs easy-cluster [14] nhmmer [15] 10,000
RNACentral [16] mmseqs easy-linclust nhmmer 10,000
Nucleotide collection [17] mmseqs easy-cluster nhmmer 10,000
The databases are used as follows:
• For training the models, we search UniRef90 (up to v2022_05), UniProt (v2020_05) with flag -Z 119222328,
Reduced BFD or the combination Uniclust30 (v2018_08) + BFD, MGnify (up to v2022_05), Rfam (v14.9),
RNACentral (v21.0), and Nucleotide collection (2023-02-23).
• For inference on recent PDB, we search UniRef90 (v2022_05), UniProt (v2021_04) with flag -Z 138515945,
Reduced BFD, MGnify (v2022_05), Rfam (v14.9), RNACentral (v21.0), and Nucleotide collection (2023-02-
23).
• For inference on the PoseBusters set, we search UniRef90 (v2020_01), UniProt (v2020_05),
Uniclust30 (v2018_08) + BFD, and MGnify (v2018_12).
Individual MSA results are cropped to the maximum number of sequences stated above. If no hits are found, a
length-1 MSA containing only the query sequence is returned.
The UniProt search result is kept separate and used to provide cross-chain genetic information, as in [18]. All other
results are stacked in the order listed and deduplicated to form the main MSA. During training, the main MSA for each
sequence is subsampled from size n to size k = Uniform[1, n] by selecting sequences to keep at random.
2.3 MSA processing
We construct an MSA with up to 16,384 rows (Nmsa ≤ 16,384). The first row is the query sequence, the next rows (up
to 8,191) are constructed by pairing the UniProt MSA based on species, as described in the AlphaFold-Multimer paper
[18]. Unlike in AlphaFold-Multimer, the rest of the MSA is constructed in a dense fashion, so if there are n paired rows
then, for each chain, the last (16384 − n) rows in the output MSA are the first rows from the original MSA constructed
in subsection 2.2.
2.4 Template search
Template search is performed only for individual protein chains, we do not provide any multi-chain templates. The
template search takes as input the deduplicated UniRef90 MSA described in subsection 2.2, which in training is cropped
to the first 300 sequences. We then use hmmbuild to convert the MSA into an HMM, followed by hmmsearch with flags
--noali --F1 0.1 --F2 0.1 --F3 0.1 --E 100 --incE 100 --domE 100 --incdomE 100. Hmmsearch retrieves template hits
from a fasta file of all protein PDB sequences.
Template hits are filtered based on their release date. For PDB-based training datasets, the template release date must
be no less than 60 days before that of the example, while for distillation sets the max template date is 2018-04-30. At
inference time the max template date is our training cutoff of 2021-09-30 unless otherwise stated. We also remove
templates that contain the exact query sequence with greater than 95% coverage as well as short templates less than 10
residues or covering less than 10% of the query.
Supplementary information for AlphaFold 3 3
Template structures are retrieved from the corresponding PDB mmCIF file, and we attempt to locate the template
sequence reported by hmmsearch. This should be a subsequence of the chain returned by hmmsearch, but we fall
back to checking other chains for a match, and if necessary realigning the query to the reported template chain with
Kalign v.0.2.0 [19]. Once the relevant residues have been identified, that part of the template structure can be featurized
similarly to the training example itself (see subsection 2.8).
Templates are sorted by e-value. At most 20 templates can be returned by our search, and the model uses up to 4
(Ntempl ≤ 4). At inference time we take the first 4. At training time we choose k random templates out of the available
n, where k ∼ min(Uniform[0, n], 4). This reduces the efficacy of simply copying the template.
Note that templates are strictly single chain; when predicting a complex, we make no attempt to select templates from
the same PDB file in order to gain information about inter-chain interactions.
2.5 Training data
We train on a mixture of five structural datasets, as summarized in Table 3 below. Selecting an example for the model
to train on proceeds as follows:
1. Sample a dataset according to a set of dataset weights (See the weights column of Table 3).
2. Draw an example from the selected dataset. The examples from PDB-based datasets are drawn according to our
weighting sampling procedure (described in subsubsection 2.5.1 below); examples from other datasets are drawn
uniformly.
3. Sample a structural crop from the selected example (described in subsection 2.7 below).
Table 3 Sampling of training examples: Examples are drawn from a mixture of datasets with associated weight fractions (weight column). Within a dataset, examples are drawn either according to our weighted procedure or uniformly
(Sampling strategy column).
Name Description Sampl. strategy Weight
Weighted PDB Ground truth PDB structures weighted 0.5
Disordered protein PDB distillation Proteins with unresolved residues weighted 0.02
Protein monomer distillation Protein monomer predictions from MGnify uniform 0.495
Short protein monomer distillation Protein short monomer predictions from MGnify uniform 0.005
RNA distillation RNA monomer predictions from Rfam uniform 0.05
Transcription factor negatives MGnify protein + random DNA uniform 0.011
Transcription factor positives DNA+protein predictions from JASPAR uniform 0.021
2.5.1 Weighted PDB dataset
For training on PDB data, we sample single chains and chain-pair interfaces with a bespoke weighting meant to both
redundancy-reduce (via clustering) and stratify molecule types that exist in the underlying data (via weight factors).
First, a list of chains and interfaces are collected from our filtered PDB training set (filtering described in subsubsection 2.5.4), with interfaces defined as pairs of chains with minimum heavy atom (i.e. non-hydrogen) separation less
than 5 Å. Each item from this dataset is then either a chain or interface and is sampled with a weight w dependent upon
its item type r ∈ {chain, interface}, chain count per type n{prot,nuc,ligand}
2
, and cluster size Nclust:
w ∝
βr
Nclust
(αprot ∗ nprot + αnuc ∗ nnuc + αligand ∗ nligand). (1)
We chose parameters βchain = 0.5, βinterface = 1, αprot = 3, αnuc = 3, αligand = 1. The cluster size Nclust is determined
from our clustering procedure, described in subsubsection 2.5.3 below.
The selected chain or interface is then used to bias the cropping procedure to select crops near the selected chain or
interface (see subsection 2.7).
1Transcription factor training is only applied during fine tuning.
2
For example, for a DNA or RNA chain item we would have nprot = 0, nnuc = 1, nligand = 0, and for a protein-protein interface item we would
have nprot = 2, nnuc = 0, nligand = 0.
Supplementary information for AlphaFold 3 4
2.5.2 Distillation datasets
Other than PDB ground truth structures, our four other datasets were obtained via AlphaFold distillation, three of which
coming from AlphaFold 2 predictions and one from AlphaFold 3 predictions.
1. Protein monomer distillation: AlphaFold 2 predictions of MGnify sequences with greater than 200 residues (same
as used in AlphaFold-Multimer).
2. Short protein monomer distillation: AlphaFold 2 predictions of MGnify sequences between 4 and 200 residues
(same inference run as for protein monomer distillation). N≈41,000,000 structures between both the long and
short protein monomer distillation sets.
3. Disordered protein PDB distillation: AlphaFold-Multimer v2.3 predictions of PDB proteins from the training set
with ground truth nucleic acids and small molecules inserted after the prediction is aligned to the ground truth
protein. Predictions are filtered to having at least 40 unresolved residues, at least 60 GDT with respect to the
ground truth and no atom clashes after inserting nucleic acids and small molecules. The resulting distillation
structures therefore include all resolved entities in the ground truth PDB structure, with predicted coordinates for
all protein residues (including unresolved ones) and ground truth coordinates for all non-protein entities. This
distillation set was used to ensure predictions of unstructured regions are consistent with disordered strands seen
in AlphaFoldDB predictions. Only the first bioassembly was used for this distillation set. N≈25,000 structures.
4. RNA distillation: We clustered Rfam (v14.9) using MMseqs2 with 90% sequence identity and 80% coverage,
taking one sequence per cluster (the cluster representative). AlphaFold 3 predictions of sequences from this set
were then used as an RNA distillation training set with a minimum of 10 residues and maximum average predicted
distance error (PDE) of 2. N≈65,000 structures after filtering.
5. Transcription factor positive distillation: We constructed a dataset of positive protein-DNA examples in the following way. We first find transcription factors profiles from the JASPAR 9 database [20] which have gene ids
matching those used in two high-throughput SELEX datasets [21, 22]. Next, for each profile in this filtered dataset
we assign a protein sequence in two ways: 1) by taking the canonical sequence under the profile’s Uniprot ID; 2)
by searching across sequences used in at least one of two SELEX datasets mentioned above for the sequence with
highest similarity to the Uniprot sequence and matching gene id. Sequence similarity is computed as the number
of non-gap matches between aligned sequences (aligned via Kalign v2.0 [19]), divided by the minimum length of
the two pre-aligned sequences. We next clustered the transcription factor sequences with 10% sequence identity
and 60% coverage and randomly select 50% of clusters for training. Next we generate positive DNA sequences
which should bind each transcription factor. Specifically, for each protein sequence, we sample 10 random singlestranded motifs from corresponding JASPAR profile’s position frequency matrix (PFM) and remove any identical
samples. For each random motif, the protein sequence, the DNA strand, and the DNA strand’s reverse complement are combined to form a single distillation example. This gives a total of 16,439 protein-DNA complexes
containing 1,165 unique protein sequences. Predictions on this set were made with AlphaFold 3 (trained without
positive transcription factor distillation). These predictions were further filtered to only those with a minimum
protein-DNA predicted distance error (PDE) of 5 Å, the majority of the predictions have much lower PDE (99%
less than 3 Å).
During training the DNA helices in this set are randomly extended with idealized DNA helices, this is done with
the following steps:
a) Remove DNA leaving atoms (OP3) from the original prediction.
b) Sample Nnew ∼ U{0, 100 − Norig}, where Norig is the number of base pairs in the original prediction.
c) Sample Nstart ∼ U{0, Nnew} and set Nend ← Nnew − Nstart.
d) Construct two idealized B-DNA helices with random complementary base pairs of length Nstart and Nend.
e) Align the last base pairs of one of the random helices to the first base pair in the original prediction, and
align the first base pair in the other random helix to the last base pair in the original prediction.
f) If the new extended structure has any new clashes or violations, then we return the original structure without
modifications. A clash is defined as having two heavy atoms within 1.5 Å, excluding neighbouring residues.
A violation is defined if the bond angle differs by more than 20 degrees to the RDKit reference conformer,
or if the bond lengths differ by more than 0.2 Å to the reference conformer.
6. Transcription factor negatives distillation: We constructed a dataset of negative protein-DNA examples. It was
generated with the following steps:
Supplementary information for AlphaFold 3 5
a) Sample a DNA length N ∼ U{60, 100} with 0.3 probability, and from the empirical distribution of DNA
lengths in PDB with 0.7 probability.
b) Randomly generate a DNA sequence with N bases and its reverse complement.
c) Generate a prediction with AlphaFold 3 for the dsDNA.
d) Sample a random protein prediction from the protein monomer distillation set.
e) Centre and randomly rotate the protein and DNA, then translate them such that the minimum distance
between the two is at least 25 Å + N (2, 3) Å.
2.5.3 Training set clustering
In order to reduce bias in the training and evaluation sets, clustering was performed on PDB chains and interfaces, as
follows.
• Chain-based clustering occur at 40% sequence homology for proteins, 100% homology for nucleic acids, 100%
homology for peptides (<10 residues) and according to CCD identity for small molecules (i.e. only identical
molecules share a cluster).
• Chain-based clustering of polymers with modified residues is first done by mapping the modified residues to
a standard residue using SCOP [23, 24] convention (https://github.com/biopython/biopython/
blob/5ee5e69e649dbe17baefe3919e56e60b54f8e08f/Bio/Data/SCOPData.py). If the modified residue could not be found as a mapping key or was mapped to a value longer than a single character, it was
mapped to type unknown.
• Interface-based clustering is a join on the cluster IDs of the constituent chains, such that interfaces I and J are
in the same interface cluster C
interface only if their constituent chain pairs {I1, I2}, {J1, J2} have the same chain
cluster pairs {Cchain
1
, C
chain
2
}.
2.5.4 Filtering
The PDB data is filtered with the following constraints.
Filtering of targets:
• The structure must have been released to the PDB before the cutoff date of 2021-09-30.
• The structure must have a reported resolution of 9 Å or less.
• The maximum number of polymer chains in a considered structure is 300 for training and 1000 for evaluation.
• Any polymer chain containing fewer than 4 resolved residues is filtered out.
Filtering of bioassemblies:
• Hydrogens are removed.
• Polymer chains with all unknown residues are removed.
• Clashing chains are removed. Clashing chains are defined as those with >30% of atoms within 1.7 Å of an atom
in another chain. If two chains are clashing with each other, the chain with the greater percentage of clashing
atoms will be removed. If the same fraction of atoms are clashing, the chain with fewer total atoms is removed.
If the chains have the same number of atoms, then the chain with the larger chain id is removed.
• For residues or small molecules with CCD codes, atoms outside of the CCD code’s defined set of atom names are
removed.
• Leaving atoms (ligand atom or groups of atoms that detach when bonds form) for covalent ligands are filtered
out.
• Protein chains with consecutive Cα
atoms >10 Å apart are filtered out.
• For bioassemblies with greater than 20 chains, we select a random interface token (with a centre atom <15 Å to
the centre atom of a token in another chain) and select the closest 20 chains to this token based on minimum
distance between any tokens centre atom.
• Crystallization aids are removed if the mmCIF method information indicates that crystallography was used (see
Table 9).
Supplementary information for AlphaFold 3 6
2.6 Tokenization
In AlphaFold 2, the protein residue was the fundamental sequential unit entering the Evoformer single and pair stacks.
In this work we have generalized the tokenization scheme so as to accommodate a wider variety of molecular types. We
used the following procedure.
• A standard amino acid residue (Table 13) is represented as a single token.
• A standard nucleotide residue (Table 13) is represented as a single token.
• A modified amino acid or nucleotide residue is tokenized per-atom (i.e. N tokens for an N-atom residue)
• All ligands are tokenized per-atom
For each token we also designate a token centre atom, used in various places below:
• C
α
for standard amino acids
• C1′
for standard nucleotides
• For other cases take the first and only atom as they are tokenized per-atom.
2.7 Cropping
Our cropping procedure is largely equivalent to that found in AlphaFold-Multimer [18], with the caveat that before
the cropping was applied to protein residues and now it is applied to tokens. We have three main cropping strategies,
described below, that are randomly selected from, with dataset-specific selection weights (see Table 4).
Table 4 Weights for sampling cropping strategies
Dataset Cropping weight
Contiguous Spatial Spatial interface
Weighted PDB 0.20 0.40 0.40
Disordered protein PDB complex 0.20 0.40 0.40
Monomer distillation 0.25 0.75 0.00
Short protein distillation 0.25 0.75 0.00
RNA distillation 0.25 0.75 0.00
2.7.1 Contiguous cropping
Here contiguous sequences of polymer residues and/or ligand atoms are selected for each chain. For details see section
7.2.1 and Algorithm 1 from the AlphaFold-Multimer paper [18].
2.7.2 Spatial Cropping
In this procedure, polymer residues and ligand atoms are selected that are within close spatial distance of a reference
atom. The reference atom is selected at random from the set of token centre atoms (defined in subsection 2.6). For
examples coming out of the Weighted PDB or Disordered protein PDB complex datasets, where a preferred chain or
interface is provided (subsection 2.5), the reference atom is selected at random from token centre atoms that exist within
this chain or interface.
Once the reference atom is selected, the final crop is determined via Algorithm 2 of AlphaFold-Multimer [18].
2.7.3 Spatial Interface Cropping
In this procedure, polymer residues and ligand atoms are selected that are within close spatial distance of an interface
atom. The interface atom is selected at random from the set of token centre atoms (defined in subsection 2.6) with a
distance under 15 Å to another chain’s token centre atom. For examples coming out of the Weighted PDB or Disordered
protein PDB complex datasets, where a preferred chain or interface is provided (subsection 2.5), the reference atom is
selected at random from interfacial token centre atoms that exist within this chain or interface.
Once the interface atom is selected, the final crop is determined via Algorithm 2 of AlphaFold-Multimer [18].
Supplementary information for AlphaFold 3 7
2.8 Featurisation and model inputs
Table 5 lists the main model input features, which fall into the following categories:
• Token features. Composed of per-token features, such as position indexes (token_index); chain identifiers
(asym_id); masks (is_protein).
• Reference features. Features derived from a residue, nucleotide or ligand’s reference conformer. Given an input
CCD code or SMILES string, the conformer is typically generated with RDKit v.2023_03_3 [25] using ETKDGv3
[26]. On error, we fall back to using the CCD ideal coordinates, or finally the representative coordinates if they
are from before our training date cut-off (2021-09-30 unless otherwise stated). At the end, any atom coordinates
still missing are set to zeros.
• Msa features. Features derived from genetics search, e.g. the MSA itself, deletion matrix, and profile.
• Template features. Features derived from template search, e.g. the template atom positions.
• Bond features. Features providing bond information, e.g. the expected locations of polymer-ligand bonds, and
of intra-/inter-ligand bonds.
In the algorithms, features are referred as “f” with the feature name in superscript and the element indices in subscript,
e.g. the one-hot encoded msa feature with shape [Nmsa, Ntoken, 32] is denoted as f
msa
si ∈ N32
Table 5 Input features to the model. Feature dimensions: Ntoken is the number of tokens, Nmsa is the number of MSA
sequences, Ntempl is the number of templates, Natom is the number of atoms, Nchains is the number of chains, Nbonds is
the number of bonds, Nperm the number of atom permutations.
Feature & Shape Description
residue_index
[Ntoken]
Residue number in the token’s original input chain.
token_index
[Ntoken]
Token number. Increases monotonically; does not restart at 1 for new
chains.
asym_id
[Ntoken]
Unique integer for each distinct chain.
entity_id
[Ntoken]
Unique integer for each distinct sequence.
sym_id
[Ntoken]
Unique integer within chains of this sequence. E.g. if chains A, B and
C share a sequence but D does not, their sym_ids would be [0, 1, 2, 0].
restype
[Ntoken, 32]
One-hot encoding of the sequence. 32 possible values: 20 amino acids
+ unknown, 4 RNA nucleotides + unknown, 4 DNA nucleotides + unknown, and gap. Ligands represented as “unknown amino acid”.
is_protein / rna / dna / ligand
[Ntoken]
4 masks indicating the molecule type of a particular token.
ref_pos
[Natom, 3]
Atom positions in the reference conformer, with a random rotation and
translation applied. Atom positions are given in Å.
ref_mask
[Natom]
Mask indicating which atom slots are used in the reference conformer.
ref_element
[Natom, 128]
One-hot encoding of the element atomic number for each atom in the
reference conformer, up to atomic number 128.
ref_charge
[Natom]
Charge for each atom in the reference conformer.
Supplementary information for AlphaFold 3 8
Feature & Shape Description
ref_atom_name_chars
[Natom, 4, 64]
One-hot encoding of the unique atom names in the reference conformer.
Each character is encoded as ord(c) − 32, and names are padded to
length 4.
ref_space_uid
[Natom]
Numerical encoding of the chain id and residue index associated with
this reference conformer. Each (chain id, residue index) tuple is assigned an integer on first appearance.
msa
[Nmsa, Ntoken, 32]
One-hot encoding of the processed MSA, using the same classes as
restype.
has_deletion
[Nmsa, Ntoken]
Binary feature indicating if there is a deletion to the left of each position
in the MSA.
deletion_value
[Nmsa, Ntoken]
Raw deletion counts (the number of deletions to the left of each MSA
position) are transformed to [0, 1] using 2
π
arctan d
3
.
profile
[Ntoken, 32]
Distribution across restypes in the main MSA. Computed before MSA
processing (subsection 2.3).
deletion_mean
[Ntoken]
Mean number of deletions at each position in the main MSA. Computed
before MSA processing (subsection 2.3).
template_restype
[Ntempl, Ntoken]
One-hot encoding of the template sequence, see restype.
template_pseudo_beta_mask
[Ntempl, Ntoken]
Mask indicating if the Cβ
(Cα
for glycine) has coordinates for the template at this residue.
template_backbone_frame_mask
[Ntempl, Ntoken]
Mask indicating if coordinates exist for all atoms required to compute
the backbone frame (used in the template_unit_vector feature).
template_distogram
[Ntempl, Ntoken, Ntoken, 39]
A one-hot pairwise feature indicating the distance between Cβ
atoms
(Cα
for glycine). Pairwise distances are discretized into 38 bins of equal
width between 3.25 Å and 50.75 Å; one more bin contains any larger
distances.
template_unit_vector
[Ntempl, Ntoken, Ntoken, 3]
The unit vector of the displacement of the Cα
atom of all residues
within the local frame of each residue. Local frames are computed
as in [1].
token_bonds
[Ntoken, Ntoken]
A 2D matrix indicating if there is a bond between any atom in token i
and token j, restricted to just polymer-ligand and ligand-ligand bonds
and bonds less than 2.4 Å during training.
3 Model architecture
The model architecture is broadly based on AlphaFold 2; however, we made a number of changes to enable the model to
predict a wider range of molecules than proteins and also to further increase the accuracy in protein structure predictions.
The model is a conditional diffusion model, where unlike most other diffusion models most of the computation
is happening in the conditioning. The conditioning part of the model is similar in overall architecture to the trunk
(Template Module, MSA Module, and Pairformer) of AlphaFold 2, with a number of key differences. The algorithm
corresponding to the architecture is shown in Algorithm 1 and in Main Article Fig. 1d. We introduce a more general
tokenization scheme, where each amino acid residue corresponds to one token as in AlphaFold 2 and each nucleotide
corresponds to one token, while for other molecules we encode each heavy atom as its own token. In order to get the
initial trunk input feature we have a more sophisticated input feature embedder that performs attention over all atoms
Supplementary information for AlphaFold 3 9
in order to encode the information about the chemical structure of all the molecules, leading to a single representation
representing all the tokens.
Given the input features, we build a pair representation in a manner similar to AlphaFold 2. This pair representation
and the single representation are then fed into the main part of the conditioning network, which is recycled multiple
times.
The main part of the conditioning network consists of a TemplateEmbedder in a similar style to AlphaFold 2 which
encodes information about provided templates into the pair representation. This is followed by an MSA Module that
extracts information from the MSA and encodes it into the pair representation. Here we use MSAs for both protein
sequences as well as RNA sequences. The resulting pair representation is then used as an input for the main PairformerStack, which also receives the single representation as an input and processes the representation further forming the
main loop of the model.
The resulting single and pair embeddings are then used to condition a diffusion process. Here the diffusion is
parametrized by a Diffusion Module, which is a considerably cheaper sub-network that encodes a single denoising
step. Notably it scales quadratically in the number of tokens rather than cubically. The resulting output structure from
the Diffusion Module is then passed to a confidence head, which uses the pair and single representation together with
the structure to provide confidence measures. The components of the model are described in the subsequent sections.
Algorithm 1 Main Inference Loop
def MainInferenceLoop({f
*}, Ncycle = 4, cs = 384, cz = 128) :
1: {s
inputs
i
} = InputFeatureEmbedder({f
*})
2: s
init
i = LinearNoBias(s
inputs
i
) s
init
i ∈ Rcs
3: z
init
ij = LinearNoBias(s
inputs
i
) + LinearNoBias(s
inputs
j
) z
init
ij ∈ Rcz
4: z
init
ij += RelativePositionEncoding({f
*})
5: z
init
ij += LinearNoBias(f token_bonds
ij )
6: {zˆij}, {ˆsi} = 0, 0
7: for all c ∈ [1, . . . , Ncycle] do
8: zij = z
init
ij + LinearNoBias(LayerNorm(zˆij )) zij ∈ Rcz
9: {zij} += TemplateEmbedder({f
*}, {zij})
10: {zij} += MsaModule({f
msa
Si }, {zij}, {s
inputs
i
})
11: si = s
init
i + LinearNoBias(LayerNorm(ˆsi)) si ∈ Rcs
12: {si}, {zij} = PairformerStack({si}, {zij})
13: {ˆsi}, {zˆij} ← {si}, {zij}
14: end for
15: {⃗x
pred
l
} = SampleDiffusion({f
*}, {s
inputs
i
}, {si}, {zij})
16: {p
plddt
l
}, {p
pae
ij }, {p
pde
ij }, {p
resolved
l
} = ConfidenceHead({s
inputs
i
}, {si}, {zij}, {⃗x
pred
l
})
17: p
distogram
ij = DistogramHead(zij )
18: return {⃗x
pred
l
}, {p
plddt
l
}, {p
pae
ij }, {p
pde
ij }, {p
resolved
l
}, {p
distogram
ij }
3.1 Input embeddings
Any bonds provided by the user (via the token_bonds feature) are linearly embedded in Algorithm 1, the embedding of
the other user inputs, along with the RDKit reference conformer are described below.
3.1.1 Input embedder
The residue type, reference conformer and MSA summary features (profile and deletion_mean) are embedded in Algorithm 2. The reference conformer is embedded in a permutation invariant way using the AtomAttentionEncoder
(Algorithm 5).
Supplementary information for AlphaFold 3 10
Algorithm 2 Construct an initial 1D embedding
def InputFeatureEmbedder({f
*}) :
# Embed per-atom features.
1: {ai}, _, _, _ = AtomAttentionEncoder({f
*}, ∅, ∅, ∅, catom = 128, catompair = 16, ctoken = 384)
# Concatenate the per-token features.
2: si = concat(ai
,f
restype
i
,f
profile
i
, f
deletion_mean
i
)
3: return {si}
3.1.2 Relative position encoding
As in AlphaFold 2 and AlphaFold-Multimer, relative encodings are used to break symmetries across identical residues
(a
rel_pos
ij ) and chains (a
rel_chain
ij ). In AlphaFold 3 we introduce a relative token encoding, applied to tokens within the
same residue (see Algorithm 3). The relative position and token indices are clipped between [rmin, rmax], with rmax =
32. Relative chain indices are clipped between [smin, smax], with smax = 2.
Algorithm 3 Relative position encoding
def RelativePositionEncoding({f
*}, rmax, smax, cz = 128) :
1: b
same_chain
ij = (f asym_id
i == f asym_id
j
)
2: b
same_residue
ij = (f residue_index
i == f residue_index
j
)
3: b
same_entity
ij = (f entity_id
i == f entity_id
j
)
4: d
residue
ij =



clip(f residue_index
i − f
residue_index
j + rmax, 0, 2 · rmax) if b
same_chain
ij
2 · rmax + 1 else
d
residue
ij ∈ N
5: a
rel_pos
ij = one_hot(d
residue
ij , [0, . . . , 2 · rmax + 1])
6: d
token
ij =



clip(f token_index
i − f
token_index
j + rmax, 0, 2 · rmax) if b
same_chain
ij and b
same_residue
ij
2 · rmax + 1 else
d
token
ij ∈ N
7: a
rel_token
ij = one_hot(dij , [0, . . . , 2 · rmax + 1])
8: d
chain
ij =



clip(f sym_id
i − f
sym_id
j + smax, 0, 2 · smax) if not b
same_chain
ij
2 · smax + 1 else
d
chain
ij ∈ N
9: a
rel_chain
ij = one_hot(d
chain
ij , [0, . . . , 2 · smax + 1])
10: pij = LinearNoBias(concat([a
rel_pos
ij , a
rel_token
ij , bsame_entity
ij , a
rel_chain
ij ])) pij ∈ R cz
11: return {pij}
Supplementary information for AlphaFold 3 11
Algorithm 4 One-hot encoding with nearest bin
def one_hot(x, vbins) : x ∈ R, vbins ∈ RNbins
1: p = 0 p ∈ RNbins
2: b = arg min(|x − vbins|)
3: pb = 1
4: return p
3.2 Sequence-local atom attention
The “sequence-local atom attention” represents the whole structure as a flat list of atoms and allows all atoms to “talk”
directly to each other within a certain sequence neighbourhood. E.g., each subset of 32 atoms attends to the subset of the
nearby 128 atoms (nearby in the sequence space). This gives the network the capacity to learn general rules about local
atom constellations, independently from the coarse-grained tokenization where each standard residue is represented
with a single token only.
The restriction to a certain sequence neighbourhood is sub-optimal, but was necessary to keep the memory and
compute costs within reasonable bounds. The resulting attention pattern is equivalent to computing the full affinity
matrix and applying a neighbourhood mask that contains the rectangular blocks along the diagonal (see Suppl. Fig. 1).
flat atoms
flat atoms
Supplementary Figure 1 Sequence-local atom attention. Each subset of atoms (rows) attends to a larger subset
of atoms (columns). The blue area depicts the theoretical full Natoms × Natoms attention matrix. The yellow rectangles
represent the attentions that are realized.
The conversion of the atom indexing by token index i ∈ Ntokens and atom name a ∈ Satom names, e. g. {⃗x
a
i
} to a flat
indexing with atom index l ∈ Natoms is denoted by the mapping of flat atom index to the token index i = tok_idx(l) .
Supplementary information for AlphaFold 3 12
Algorithm 5 Atom attention encoder
def AtomAttentionEncoder({f
*}, {rl}, {s
trunk
i
}, {zij}, catom, catompair, ctoken) :
# Create the atom single conditioning: Embed per-atom meta data
1: cl = LinearNoBias(concat(⃗f
ref_pos
l
, f
ref_charge
l
, f
ref_mask
l
,f
ref_element
l
,f
ref_atom_name_chars
l
)) cl ∈ Rcatom
l ∈ {1, . . . , Natoms}
# Embed offsets between atom reference positions
2: ⃗dlm = ⃗f
ref_pos
l −⃗f
ref_pos
m
⃗dlm ∈ R3
3: vlm = (f ref_space_uid
l == f ref_space_uid
m ) vlm ∈ R
4: plm = LinearNoBias(⃗dlm) · vlm plm ∈ Rcatompair
# Embed pairwise inverse squared distances, and the valid mask.
5: plm += LinearNoBias 
1/

1 + ∥
⃗dlm∥
2

· vlm
6: plm += LinearNoBias(vlm) · vlm
# Initialise the atom single representation as the single conditioning.
7: ql = cl ql ∈ Rcatom
# If provided, add trunk embeddings and noisy positions.
8: if {rl} ̸= ∅ then
# Broadcast the single and pair embedding from the trunk.
9: cl += LinearNoBias(LayerNorm(s
trunk
tok_idx(l)
))
10: plm += LinearNoBias(LayerNorm(ztok_idx(l) tok_idx(m)
))
# Add the noisy positions.
11: ql += LinearNoBias (rl)
12: end if
# Add the combined single conditioning to the pair representation.
13: plm += LinearNoBias(relu(cl)) + LinearNoBias(relu(cm))
# Run a small MLP on the pair activations.
14: plm += LinearNoBias(relu(LinearNoBias(relu(LinearNoBias(relu(plm))))))
# Cross attention transformer.
15: {ql} = AtomTransformer({ql}, {cl}, {plm}, Nblock = 3, Nhead = 4)
# Aggregate per-atom representation to per-token representation
16: ai = mean
l∈{1,...,Natoms}
tok_idx(l)=i
(relu(LinearNoBias(ql))) ai ∈ Rctoken
17: q
skip
l
, c
skip
l
, p
skip
lm = ql
, cl
, plm
18: return {ai}, {q
skip
l
}, {c
skip
l
}, {p
skip
lm }
Supplementary information for AlphaFold 3 13
Algorithm 6 Atom attention decoder
def AtomAttentionDecoder({ai}, {q
skip
l
}, {c
skip
l
}, {p
skip
lm }) :
# Broadcast per-token activiations to per-atom activations and add the skip connection
1: ql = LinearNoBias(atok_idx(l)
) + q
skip
l
# Cross attention transformer.
2: {ql} = AtomTransformer({ql}, {c
skip
l
}, {p
skip
lm }, Nblock = 3, Nhead = 4)
# Map to positions update.
3: r
update
l = LinearNoBias(LayerNorm(ql))
4: return {r
update
l
}
Algorithm 7 Atom Transformer
def AtomTransformer({ql}, {cl}, {plm}, Nblock = 3, Nhead,
Nqueries = 32, Nkeys = 128, Ssubset centres = {15.5, 47.5, 79.5, . . . }) :
# sequence-local atom attention is equivalent to self attention within rectangular blocks along the diagonal.
1: βlm =



0 if |l − c| < Nqueries/2 ∧ |m − c| < Nkeys/2 ∀ c ∈ Ssubset centres
−1010 else
2: {ql}= DiffusionTransformer({ql}, {cl}, {plm}, {βlm}, Nblock, Nhead)
3: return {ql}
Supplementary information for AlphaFold 3 14
3.3 MSA Module
The MSA Module (Suppl. Fig. 2, Algorithm 8) in AlphaFold 3 fullfills a similar role to the Extra MSA Stack in
AlphaFold 2 and hence has a fairly similar network architecture to AlphaFold-Multimer in the block. It samples a new
iid random subset of the MSA for each recycling iteration, the MSA sequences and input features then get embedded
into representation msi for each token in each sequence in the msa. The Network architecture of the MSA Module then
consists of 4 homogeneous blocks which repeatedly process and combine the pair representation zij and the msa. The
final pair representation then gets passed on to the Pairformer Stack.
The overall structure of the block is very similar to the Pairformer Stack, where the MSA representation fullfills a
role similar to the single representation. The individual blocks here are similar to the extra MSA stack in AlphaFold 2,
the difference here is that the attention is independently performed for each row of the MSA and that attention weights
are entirely projected from the pair representation, i.e. there is no key-query based attention. This also means that each
row of the MSA combines information via attention in the same way, this reduces computation and memory usage in
the attention. The MSA attention layer employs the same gating mechanism as the other attention layers. Otherwise
this part of the model works the same as AlphaFold 2, meaning the pair representation gets passed through Triangular
Multiplicative Update and Triangular self-attention layers and a transition block. In all the transition blocks we use
SwiGLU instead of ReLU.
Conceptually a difference to AlphaFold 2 is that here we do not combine information across different rows of the
MSA directly, but rather all information has to flow via the pair representation. The motivation behind this is that the
pair representation should contain as much information as possible about the proteins or nucleic acids as it forms the
backbone for the rest of the network.
pair
representation
(n,n,c)
pairweighted
averaging
+
transition
Outer
product
mean
+
triangle
update
using
"outgoing"
edges
triangle
selfattention
around
starting
node
triangle
selfattention
around
ending
node
triangle
update
using
"incoming"
edges
transition
pair
representation
(n,n,c)
+
+ + + + +
4 blocks
subsample
and
embed
+
inputs rep. (n,c)
MSA
representation
(s,n,c)
HHSSGLVPRGSHMSGKIQHK
HHSSGLVPRGSHMPSYTLHy
HHSSGLVPRGSHMILKFDHi
HHSSGLVPRGSHMRDPTQFE
HHSSGLVPRGSHNMTQFEER
HHSSGLVPRGSSGAFEDRDp
HHSSGLVPRGSHMAAPIRFf
HHSSGLVPRGSHMDLE---K
Supplementary Figure 2 MSA Module.
Supplementary information for AlphaFold 3 15
Algorithm 8 MSA Module
def MsaModule({f
*}, {zij}, {s
inputs
i
}, Nblock = 4, cm = 64) :
1: mSi = concat(f
msa
Si , f
has_deletion
Si , f
deletion_value
Si )
2: {s} = SampleRandomWithoutReplacement({S})
3: msi ← LinearNoBias(msi) msi ∈ Rcm
4: msi+= LinearNoBias({s
inputs
i
})
5: for all l ∈ [1, . . . , Nblock] do
# Communication
6: {zij} += OuterProductMean({msi})
# MSA stack
7: {msi} += DropoutRowwise0.15(MSAPairWeightedAveraging({msi}, {zij}, c = 8))
8: {msi} += Transition({msi})
# Pair stack
9: {zij} += DropoutRowwise0.25(TriangleMultiplicationOutgoing({zij}))
10: {zij} += DropoutRowwise0.25(TriangleMultiplicationIncoming({zij}))
11: {zij} += DropoutRowwise0.25(TriangleAttentionStartingNode({zij}))
12: {zij} += DropoutColumnwise0.25(TriangleAttentionEndingNode({zij}))
13: {zij} += Transition({zij})
14: end for
15: return {zij}
Algorithm 9 Outer product mean
def OuterProductMean({msi}, c = 32, cz = 128) :
1: msi ← LayerNorm(msi)
2: asi, bsi = LinearNoBias(msi) asi, bsi ∈ Rc
3: oij = flatten
means(asi ⊗ bsj )

oij ∈ Rc·c
4: zij = Linear(oij ) zij ∈ Rcz
5: return {zij}
Algorithm 10 MSA pair weighted averaging with gating
def MSAPairWeightedAveraging({msi}, {zij}, c = 32, Nhead = 8) : msi ∈ Rcm
# Input projections
1: msi ← LayerNorm(msi)
2: v
h
si = LinearNoBias(msi) v
h
si ∈ Rc
, h ∈ {1, . . . , Nhead}
3: b
h
ij = LinearNoBias(LayerNorm(zij ))
4: g
h
si = sigmoid
LinearNoBias(msi)

g
h
si ∈ Rc
# Weighted average with gating
5: w
h
ij = softmaxj

b
h
ij
6: o
h
si = g
h
si ⊙
P
j w
h
ijv
h
sj
# Output projection
7: m˜ si = LinearNoBias 
concath(o
h
si)

m˜ si ∈ Rcm
8: return {m˜ s
Supplementary information for AlphaFold 3 16
Algorithm 11 Transition layer
def Transition(x, n = 4) : x ∈ Rc
1: x ← LayerNorm(x)
2: a = LinearNoBias(x) a ∈ Rn·c
3: b = LinearNoBias(x) b ∈ Rn·c
4: x ← LinearNoBias(swish(a) ⊙ b) x ∈ Rc
5: return x
3.4 Triangle updates of the pair representation
AlphaFold 3 uses the same update / attention scheme for the pair representation as AlphaFold 2 which establishes a
direct communication between the edges that connect 3 nodes (interpreting the pair representation as edge features of
a fully connected graph where the tokens are the nodes). This allows the network to easily detect inconsistencies in its
current belief about the spatial relationship and to update them accordingly. For more details, see [1].
Algorithm 12 Triangular multiplicative update using “outgoing” edges
def TriangleMultiplicationOutgoing({zij}, c = 128) : zij ∈ cz
1: zij ← LayerNorm(zij )
2: aij , bij = sigmoid
LinearNoBias(zij )

⊙ LinearNoBias(zij ) aij , bij ∈ Rc
3: gij = sigmoid
LinearNoBias(zij )

gij ∈ Rcz
4: z˜ij = gij ⊙ LinearNoBias(LayerNorm(P
k
aik ⊙ bjk)) z˜ij ∈ Rcz
5: return {z˜ij}
Algorithm 13 Triangular multiplicative update using “incoming” edges (differences to Algorithm 12 highlighted)
def TriangleMultiplicationIncoming({zij}, c = 128) : zij ∈ cz
1: zij ← LayerNorm(zij )
2: aij , bij = sigmoid
LinearNoBias(zij )

⊙ LinearNoBias(zij ) aij , bij ∈ Rc
3: gij = sigmoid
LinearNoBias(zij )

gij ∈ Rcz
4: z˜ij = gij ⊙ LinearNoBias(LayerNorm(P
k
aki ⊙ bkj )) z˜ij ∈ Rcz
5: return {z
Supplementary information for AlphaFold 3 17
Algorithm 14 Triangular gated self-attention around starting node
def TriangleAttentionStartingNode({zij}, c = 32, Nhead = 4) : zij ∈ cz
# Input projections
1: zij ← LayerNorm(zij )
2: q
h
ij , k
h
ij , v
h
ij = LinearNoBias(zij ) q
h
ij , k
h
ij , v
h
ij ∈ Rc
, h ∈ {1, . . . , Nhead}
3: b
h
ij = LinearNoBias(zij )
4: g
h
ij = sigmoid
LinearNoBias(zij )

g
h
ij ∈ Rc
# Attention
5: a
h
ijk = softmaxk

√
1
c
q
h
ij
⊤
k
h
ik + b
h
jk
6: o
h
ij = g
h
ij ⊙
P
k
a
h
ijkv
h
ik
# Output projection
7: z˜ij = LinearNoBias 
concath(o
h
ij )

z˜ij ∈ Rcz
8: return {z˜ij}
Algorithm 15 Triangular gated self-attention around ending node (differences to Algorithm 14 highlighted)
def TriangleAttentionEndingNode({zij}, c = 32, Nhead = 4) : zij ∈ cz
# Input projections
1: zij ← LayerNorm(zij )
2: q
h
ij , k
h
ij , v
h
ij = LinearNoBias(zij ) q
h
ij , k
h
ij , v
h
ij ∈ Rc
, h ∈ {1, . . . , Nhead}
3: b
h
ij = LinearNoBias(zij )
4: g
h
ij = sigmoid
LinearNoBias(zij )

g
h
ij ∈ Rc
# Attention
5: a
h
ijk = softmaxk

√
1
c
q
h
ij
⊤
k
h
kj + b
h
ki 
6: o
h
ij = g
h
ij ⊙
P
k
a
h
ijk v
h
kj
# Output projection
7: z˜ij = LinearNoBias 
concath

o
h
ij
z˜ij ∈ Rcz
8: return {z˜i
Supplementary information for AlphaFold 3 18
3.5 Template embedding
The template embedding (Algorithm 16) combines all raw template features to a pair representation, and processes it
together with the given pair representation zij (produced in the previous recyling iteration). This allows the network to
attend to specific regions in the template based on its current belief about the structure.
The template_backbone_frame_mask, template_distogram, template_restype, template_pseudo_beta_mask and template_unit_vector features from Table 5 are concatenated into a 2D feature. In the main recycling loop (Algorithm 1)
the pairwise embeddings zij are passed into the template embedder (Algorithm 16). Each template is processed independently with a PairformerStack, and the resulting activations are averaged to obtain a pairwise embedding of all
templates.
Algorithm 16 Template embedder
def TemplateEmbedder({f
*}, {zij}, Nblock = 2, c = 64) :
1: b
template_backbone_frame_mask
ij = f template_backbone_frame_mask
ti · f
template_backbone_frame_mask
tj
2: b
template_pseudo_beta_mask
ij = f template_pseudo_beta_mask
ti · f
template_pseudo_beta_mask
tj
3: atij = concat(f
template_distogram
tij , btemplate_backbone_frame_mask
ij ,f
template_unit_vector
tij , btemplate_pseudo_beta_mask
ij )
4: atij ← atij ⊙ (f asym_id
i == f asym_id
j
)
5: atij ← concat(atij ,f
template_restype
ti ,f
template_restype
tj )
6: uij = 0 uij ∈ Rc
7: for all t ∈ [1, . . . , Ntemplates] do
8: vij = LinearNoBias(LayerNorm(zij )) + LinearNoBias(atij ) vij ∈ Rc
9: {vij} += PairformerStack({vij}, Nblock)
10: uij += LayerNorm(vij )
11: end for
12: uij ←
uij
Ntemplates
13: uij ← LinearNoBias(ReLU(uij )) uij ∈ Rc
14: return {uij}
3.6 Pairformer stack
The Pairformer Stack (Algorithm 17, Main Article Fig. 2a) fulfills a similar role to the Evoformer stack in AlphaFold 2,
the Pairformer stack uses just a single representation si
, rather than a representation for a subset of the MSA. Here the
single representation plays a role similar to the privileged first row in the Evoformer in AlphaFold 2.
As a consequence of this change there is no column-wise attention in the stack. The single attention with pair bias is
the same as the row-wise attention used in AlphaFold 2, but only applied to a single sequence, which corresponds to the
single represenation si
.
Furthermore unlike in AlphaFold 2 the single representation does not influence the pair representation, the pair representation is used to control information flow in the single representation by biasing the Attention logits.
All Transition blocks use SwiGLU.
Supplementary information for AlphaFold 3 19
Algorithm 17 Pairformer stack
def PairformerStack({si}, {zij}, Nblock = 48) :
1: for all l ∈ [1, . . . , Nblock] do
# Pair stack
2: {zij} += DropoutRowwise0.25(TriangleMultiplicationOutgoing({zij}))
3: {zij} += DropoutRowwise0.25(TriangleMultiplicationIncoming({zij}))
4: {zij} += DropoutRowwise0.25(TriangleAttentionStartingNode({zij}))
5: {zij} += DropoutColumnwise0.25(TriangleAttentionEndingNode({zij}))
6: {zij} += Transition({zij})
7: {si} += AttentionPairBias({si}, ∅, {zij}, βij = 0, Nhead = 16)
8: {si} += Transition({si})
9: end for
10: return {si}, {zij}
3.7 Diffusion Module
In AlphaFold 2 the final structure was realised using a Structure Module using invariant point attention. For AlphaFold 3
we replaced it with a relatively standard non-equivariant point-cloud diffusion model over all atoms (Algorithm 18 and
Main Article Fig. 2b). During training, we train a denoiser to remove Gaussian noise from the positions of all heavy
atoms conditioned on the features from the main trunk. The denoiser is based on a modern transformer, but with several
modifications to make it more amenable to the task. The main changes are:
• We incorporate conditioning from the trunk in several ways: we initialise the activations from the single embedding, use a variant of Adaptive Layernorm [27] for the single conditioning and logit biasing for the pair
conditioning.
• We use standard modern transformer tricks (e.g. SwiGLU [28]) and methods used in AlphaFold 2 (gating).
• We use a two-level architecture, working first on atoms, then tokens, then atoms again.
Notably, the transformer only uses a single linear layer to embed all atom positions and and a single linear layer to
project the updates at the end, there are no geometric biases involved (e.g. locality or SE(3) invariance). This is in
contrast to contemporary trends of using stronger domain specific inductive biases.
The details of the architecture are outlined in Algorithm 20.
Supplementary information for AlphaFold 3 20
Algorithm 18 Sample Diffusion
def SampleDiffusion( {f
*}, {s
inputs
i
}, {s
trunk
i
}, {z
trunk
ij }, Noise Schedule [c0, c1, . . . , cT ],
γ0 = 0.8, γmin = 1.0, noise scale λ = 1.003, step scale η = 1.5 ):
1: ⃗xl ∼ c0 · N (⃗0, I3) ⃗xl ∈ R3
2: for all cτ ∈ [c1, . . . , cT ] do
3: {⃗xl} ← CentreRandomAugmentation({⃗xl})
4: γ = γ0 if cτ > γmin else 0
5: tˆ= cτ−1(γ + 1)
6: ⃗ξl = λ
q
tˆ2 − c
2
τ−1
· N (⃗0, I3)
⃗ξl ∈ R3
7: ⃗x
noisy
l = ⃗xl + ⃗ξl
8: {⃗x
denoised
l
} = DiffusionModule({⃗x
noisy
l
},t,ˆ {f
*}, {s
inputs
i
}, {s
trunk
i
}, {z
trunk
ij })
9: ⃗δl = (⃗xl − ⃗x
denoised
l
)/tˆ
10: dt = cτ − tˆ
11: ⃗xl ← ⃗x
noisy
l + η · dt ·
⃗δl
12: end for
13: return {⃗xl}
Algorithm 19 CentreRandomAugmentation
def CentreRandomAugmentation({⃗xl}, strans = 1 Å) :
1: ⃗xl ← ⃗xl − mean
l
⃗xl
2: R = UniformRandomRotation()
3: ⃗t ∼ strans · N (⃗0, I3)
4: ⃗xl ← R · ⃗xl +⃗t
5: return {⃗xl}
Supplementary information for AlphaFold 3 21
Algorithm 20 Diffusion Module
def DiffusionModule({⃗x
noisy
l
},t,ˆ {f
*}, {s
inputs
i
}, {s
trunk
i
}, {z
trunk
ij },
σdata = 16, catom = 128, catompair = 16, ctoken = 768) :
# Conditioning
1: {si}, {zij}= DiffusionConditioning(t,ˆ {f
*}, {s
inputs
i
}, {s
trunk
i
}, {z
trunk
ij }, σdata)
# Scale positions to dimensionless vectors with approximately unit variance.
2: r
noisy
l = ⃗x
noisy
l
/
q
tˆ2 + σ
2
data r
noisy
l ∈ R3
# Sequence-local Atom Attention and aggregation to coarse-grained tokens
3: {ai}, {q
skip
l
}, {c
skip
l
}, {p
skip
lm } = AtomAttentionEncoder({f
*}, {r
noisy
l
}, {s
trunk
i
}, {zij}, catom, catompair, ctoken)
ai ∈ Rctoken
# Full self-attention on token level.
4: ai+= LinearNoBias(LayerNorm(si))
5: {ai} ← DiffusionTransformer({ai}, {si}, {zij}, βij = 0, Nblock = 24, Nhead = 16)
6: ai ← LayerNorm(ai)
# Broadcast token activations to atoms and run Sequence-local Atom Attention
7: {r
update
l
} = AtomAttentionDecoder({ai}, {q
skip
l
}, {c
skip
l
}, {p
skip
lm })
# Rescale updates to positions and combine with input positions
8: ⃗x
out
l = σ
2
data/(σ
2
data + tˆ2
) · ⃗x
noisy
l + σdata · t/ˆ
q
σ
2
data + tˆ2 · r
update
l
9: return {⃗x
out
l
}
Algorithm 21 Diffusion Conditioning
def DiffusionConditioning(t,ˆ {f
*}, {s
inputs
i
}, {s
trunk
i
}, {z
trunk
ij }, σdata, cz = 128, cs = 384) :
# Pair conditioning
1: zij = concat([z
trunk
ij , RelativePositionEncoding({f
*})])
2: zij ← LinearNoBias(LayerNorm(zij )) zij ∈ Rcz
3: for all b ∈ [1, 2] do
4: zij+= Transition(zij , n = 2)
5: end for
# Single conditioning
6: si = concat([s
trunk
i
, s
inputs
i
])
7: si ← LinearNoBias(LayerNorm(si)) si ∈ Rcs
8: n = FourierEmbedding(
1
4
log(t/σ ˆ data), 256)
9: si+= LinearNoBias(LayerNorm(n))
10: for all b ∈ [1, 2] do
11: si+= Transition(si
, n = 2)
12: end for
13: return {si}, {zij}
Supplementary information for AlphaFold 3 22
Algorithm 22 Fourier Embedding
def FourierEmbedding(t, c ˆ )
# Randomly generate weight/bias once before training
1: w, b ∼ N (⃗0, Ic) w, b ∈ Rc
# Compute embeddings
2: return cos(2π(tˆw + b))
Algorithm 23 Diffusion Transformer
def DiffusionTransformer({ai}, {si}, {zij}, {βij}, Nblock, Nhead):
1: for all n ∈ [1, . . . , Nblock] do
2: {bi} = AttentionPairBias({ai}, {si}, {zij}, {βij}, Nhead)
3: ai ← bi + ConditionedTransitionBlock(ai
, si)
4: end for
5: return {ai}
Algorithm 24 DiffusionAttention with pair bias and mask
def AttentionPairBias({ai}, {si}, {zij}, {βij}, Nhead) : ai ∈ Rca
, c = ca/Nhead
# Input projections
1: if {si} ̸= ∅ then
2: ai ← AdaLN(ai
, si)
3: else
4: ai ← LayerNorm(ai)
5: end if
6: q
h
i = Linear(ai) q
h
i ∈ Rc
, h ∈ {1, . . . , Nhead}
7: k
h
i
, v
h
i = LinearNoBias(ai) k
h
i
, v
h
i ∈ Rc
, h ∈ {1, . . . , Nhead}
8: b
h
ij ← LinearNoBias(LayerNorm(zij )) + βij
9: g
h
i ← sigmoid
LinearNoBias(ai)

g
h
i ∈ Rc
# Attention
10: Ah
ij ← softmaxj

√
1
c
q
h
i
⊤
k
h
j + b
h
ij
11: ai ← LinearNoBias 
concath

g
h
i ⊙
P
j Ah
ijv
h
j

ai ∈ Rca
# Output projection (from adaLN-Zero [27])
12: if {si} ̸= ∅ then
13: ai ← sigmoid(Linear(si
, biasinit=-2.0)) ⊙ ai
14: end if
15: return {ai
Supplementary information for AlphaFold 3 23
Algorithm 25 Conditioned Transition Block
# SwiGLU transition block with adaptive layernorm
def ConditionedTransitionBlock(a, s, n = 2) : a ∈ Rc
1: a ← AdaLN(a, s)
2: b ← swish(LinearNoBias(a)) ⊙ LinearNoBias(a) b ∈ Rn·c
# Output projection (from adaLN-Zero [27])
3: a ← sigmoid(Linear(s, biasinit=-2.0)) ⊙ LinearNoBias(b) a ∈ Rc
4: return a
Algorithm 26 Adaptive LayerNorm
def AdaLN(a, s) :
1: a ← LayerNorm(a, scale=False, offset=False)
2: s ← LayerNorm(s, offset=False)
3: a ← sigmoid(Linear(s)) ⊙ a + LinearNoBias(s)
4: return a
3.7.1 Diffusion Training
Our diffusion training methodology largely follows [29]. With some notable differences, mostly to the loss.
To improve training efficiency we train the Diffusion Module with a larger batch size than the trunk (see Main Article
Fig. 2c). To realise this, we run the trunk once and then create 48 versions of the input structure by randomly rotating
and translating according to Algorithm 19 and adding independent noise to each structure. We then train the Diffusion
Module over all of them in parallel. This is efficient since the Diffusion Module is much cheaper than the model trunk.
We apply a weighted aligned MSE loss to the denoised structure output from the Diffusion Module. We first perform
a rigid alignment of the ground truth ⃗x
GT
l
on to the denoised structure ⃗xl as
{⃗x
GT-aligned
l
} = weighted_rigid_align({⃗x
GT
l
}, {⃗xl)}, {wl}) (2)
with weights wl provided in Equation 4. We then compute a weighted MSE
LMSE =
1
3
mean
l

wl
||⃗xl − ⃗x
GT-aligned
l
||2

, (3)
with upweighting of nucleotide and ligand atoms as
wl = 1 + f is_dna
l
α
dna + f is_rna
l
α
rna + f is_ligand
l
α
ligand (4)
and hyperparameters α
dna = α
rna = 5, and α
ligand = 10.
To ensure that the bonds for bonded ligands (including bonded glycans) have the correct length, we introduce an
auxiliary loss during fine tuning as
Lbond = mean
(l,m)∈B 

⃗xl − ⃗xm

 −


⃗x
GT
l − ⃗x
GT
m



2
, (5)
where B is the set of tuples (start atom index, end atom index) defining the bond between the bonded ligand and its
parent chain.
We also apply an auxiliary structure-based loss based on smooth LDDT, as described in Algorithm 27. The final loss
from the Diffusion Module is then:
Ldiffusion = (tˆ2 + σ
2
data)/(tˆ+ σdata)
2
· (LMSE + αbond · Lbond) + Lsmooth_lddt (6)
Where tˆis the sampled noise level, σdata is a constant determined by the variance of the data (set to 16) and αbond is
0 for regular training, and 1 for both fine tuning stages. Prior to computing these losses, we apply an optimal ground
truth chain assignment as described in subsection 4.2.
Supplementary information for AlphaFold 3 24
During training the noise level is sampled from σdata · exp(−1.2 + 1.5 · N (0, 1)), during inference the noise schedule
is defined as
tˆ= σdata · (s
1/p
max + t · (s
1/p
min − s
1/p
max))p
(7)
where smax = 160, smin = 4 · 10−4
, p = 7 and t is distributed uniformly between [0, 1] with a step size of 1
200 .
Algorithm 27 Smooth LDDT Loss
def SmoothLDDTLoss({⃗xl}, {⃗x
GT
l
}, {f
is_dna
l
}, {f
is_rna
l
}) :
# Compute distances between all pairs of atoms
1: δxlm ← ||⃗xl − ⃗xm||
2: δxGT
lm ← ||⃗x
GT
l − ⃗x
GT
m ||
# Compute distance difference for all pairs of atoms
3: δlm ← abs(δxGT
lm − δxlm)
4: ϵlm ← 1
4
h
sigmoid( 1
2 − δlm) + sigmoid(1 − δlm)) + sigmoid(2 − δlm) + sigmoid(4 − δlm)
i
# Restrict to bespoke inclusion radius
5: f
is_nucleotide
l ← f
is_dna
l + f is_rna
l
6: clm ← (δxGT
lm < 30 Å)f
is_nucleotide
l +
(δxGT
lm < 15 Å)(1 − f
is_nucleotide
l
)
# Compute mean, avoiding self term
7: lddt = mean
l̸=m
(clmϵlm)/ mean
l̸=m
(clm)
8: return 1 − lddt
Supplementary information for AlphaFold 3 25
Algorithm 28 Weighted Rigid Align
def weighted_rigid_align({⃗xl}, {⃗x
GT
l
}, {wl}) :
# Mean-centre positions
1: ⃗µ ← mean
l
(wl⃗xl)/ mean
l
(wl)
2: ⃗µ
GT ← mean
l
(wl⃗x
GT
l
)/ mean
l
(wl)
3: ⃗xl ← ⃗xl − ⃗µ
4: ⃗x
GT
l ← ⃗x
GT
l − ⃗µ
GT
# Find optimal rotation from singular value decomposition
5: U, V ← svd(P
l wl⃗x
GT
l ⊗ ⃗xl)
6: R ← UV
# Remove reflection
7: if det(R) < 0 then
8: F ←


1 0 0
0 1 0
0 0 −1


9: R ← UF V
10: end if
# Apply alignment
11: ⃗x
align
l = R⃗xl + ⃗µ
12: return stop_gradient(⃗x
align
l
)
4 Auxiliary heads
4.1 Mini diffusion rollout
Several of the heads require predicted coordinates, therefore at training time we do a short rollout of the Diffusion
Module from pure noise with 20 steps (Main Article Fig. 2c). No gradients are applied to this mini-rollout.
4.2 Chain permutation and symmetry resolution
The assignment of names (chain-ids, like ‘A’, ‘B’, ‘C’, ...) to chains with identical sequences in a physical structure is
arbitrary, and every permutation of these names is equally correct. The same applies to multiple ligands of the same
type. It is very likely that AlphaFold will use a different permutation than the authors who deposited the ground truth
in the PDB. To compare a predicted structure to the ground truth in the confidence head (subsubsection 4.3.1) or to
create the noised version of the ground truth for diffusion training (subsection 3.7), the ground truth chains must be
renamed such that they match the predicted structure. We use the method described in [18] section 7.3 “Multi-Chain
Permutation Alignment” to find a good assignment of the predicted chains to the ground truth chains. The predicted
structure used for this alignment is the output of the mini-diffusion rollout (subsection 4.2). The same method is applied
to ligands as well, with the extension that ligands covalently bonded to polymer chains will be permuted in sync with
the corresponding chains. This is achieved by grouping all covalently bonded components together and assigning them
the same entity id.
Finally, we resolve atom naming ambiguities within each chemical component (ligand/residue): we generate permutations (up to 1000) using RDKit and permute the ground truth such that it has minimal RMSD to the prediction.
4.3 Model confidence prediction
The model is trained to predict three confidence metrics, a per-atom confidence, predicted local distance difference
(pLDDT), a pairwise atom-atom aligned error (PAE), and a pairwise atom-atom distance confidence, predicted distance
error (PDE). It is also trained to predict whether an atom is experimentally resolved. The confidence losses are only
Supplementary information for AlphaFold 3 26
applied for the PDB training set (i.e. it is not applied for any of the distillation sets), with a further filter that the
resolution of the ground truth structure is between 0.1 and 4. The details of the confidence head and losses are outlined
below.
4.3.1 Predicted local distance difference test (pLDDT)
The per atom confidence is trained to predict a version of LDDT that takes into account distances from all atoms to
polymer residues (e.g. for a ligand atom the confidence only takes into account interactions between the ligand atom
and proteins/nucleic acids, no intra-ligand terms are included), it is defined for atom l as:
lddtl =
X
m∈R
1
4
X
c∈{0.5,1,2,4}
dlm < c (8)
Where dlm is the distance between atom l and atom m in the mini-rollout prediction (subsection 4.1), l encompasses
all atoms and the set of atoms m ∈ R is defined as:
• Atoms such that the distance in the ground truth between atom l and atom m is less than 15 Å if m is a protein
atom or less than 30 Å if m is a nucleic acid atom.
• Only atoms in polymer chains.
• One atom per token - Cα
for standard protein residues and C1′
for standard nucleic acid residues.
The single embedding (si) in the confidence head (Algorithm 31) is linearly projected into [Nmax_atoms_per_token, 50]
values, where Nmax_atoms_per_token is the maximum number of atoms per token. This gives per atom confidences with 50
bins between 0 and 1 and probabilities p
b
l
obtained via a softmax. The loss is then defined as:
Lplddt = −
1
Natom
X
l
X
50
b=1
lddtb
l
log p
b
l
(9)
A single value of pLDDT per-atom is obtained by taking the expectation across the 50 binned probabilities. Here
lddtb
l
denotes a binned version of lddtl
, i.e. lddtb
l
is 1 if lddtl falls within bin b and 0 otherwise.
4.3.2 Predicted aligned error (PAE)
It is useful to have a pairwise confidence between atoms, to determine confidence of interfaces or specific interactions
between atoms. To this end we follow AlphaFold 2 and predict a pairwise alignment error (PAE), an estimate of the
error of one token when aligned according to the frame of another.
We associate a reference frame to each token i using a set of three atoms which we denote as (ai
, bi
, ci). Let
Φi = (⃗ai
,
⃗bi
,⃗ci) denote the coordinates of these frame atoms. An atom position ⃗x is expressed in a basis defined by
Φi according to Algorithm 29, which takes ⃗bi as its origin and applies to ⃗x a rotation defined by Φi
. To compute an
aligned error eij between frame i and token j, Φi and Φ
true
i
are built from the predicted and ground truth frame atom
coordinates for token i, and representative token atom coordinates from the prediction ⃗xj and ground truth ⃗x
true
j
are
expressed in the bases of Φi and Φ
true
i
, respectively. The error eij is then defined as the Euclidean distance between
these two alignments. See Algorithm 30 for pseudocode.
The atoms (ai
, bi
, ci) used to construct token i’s frame depend on the chain type of i: Protein tokens use their residue’s
backbone (N, C
α
, C), while DNA and RNA tokens use (C1
′
, C3′
, C4′
) atoms of their residue. All other tokens (small
molecules, glycans, ions) contain only one atom per token. The token atom is assigned to bi
, the closest atom to the
token atom is ai
, and the second closest atom to the token atom is ci
. If this set of three atoms is close to colinear (less
than 25 degree deviation), or if three atoms do not exist in the chain (e.g. a sodium ion), then the frame is marked as
invalid. The PAE head does not train on invalid frames, nor do ranking and confidence metrics consider them.
When computing the alignment error we use the predicted coordinates taken from the mini-rollout structure.
Supplementary information for AlphaFold 3 27
Algorithm 29 Express coordinates in frame
def expressCoordinatesInFrame(⃗x, Φ) : ⃗x ∈ R3
# Extract frame atoms
1: (⃗a,
⃗b,⃗c) = Φ ⃗a,
⃗b,⃗c ∈ R3
2: w⃗ 1 = (⃗a − ⃗b)/


⃗a − ⃗b



3: w⃗ 2 = (⃗c − ⃗b)/


⃗c − ⃗b



# Build orthonormal basis
4: ⃗e1 = (w⃗ 1 + w⃗ 2)/

w⃗ 1 + w⃗ 2


5: ⃗e2 = (w⃗ 2 − w⃗ 1)/

w⃗ 2 − w⃗ 1


6: ⃗e3 = ⃗e1 × ⃗e2
# Project onto frame basis
7: ⃗d = ⃗x − ⃗b
8: ⃗x
transformed = concat(⃗d · ⃗e1,
⃗d · e⃗2,
⃗d · ⃗e3)
9: return ⃗x
transformed ⃗x
transformed ∈ R3
Algorithm 30 Compute alignment error
def computeAlignmentError({⃗xi}, {⃗x
true
i
}, {Φi}, {Φ
true
i
}, ϵ = 1e
−8 Å
2
) :
1: ˆ⃗xij = expressCoordinatesInFrame(⃗xj , Φi)
2: ˆ⃗x
true
ij = expressCoordinatesInFrame(⃗x
true
j
, Φ
true
i
)
3: eij =
q
∥
ˆ⃗xij − ˆ⃗x
true
ij ∥
2 + ϵ
4: return {eij}
To predict the alignment error, the pair embedding (zij ) in the confidence head (Algorithm 31) is linearly projected
into 64 distance bins b, and probabilities p
b
ij are obtained via a softmax. There are 64 bins equally spaced from 0 Å
to 32 Å in 0.5 Å increments. During training the final bin also captures larger errors. The prediction targets are the
alignment errors eij . The loss is then defined as:
Lpae = −
1
N2
token
X
i,j
X
64
b=1
e
b
ij log p
b
ij (10)
Here e
b
ij denotes a binned version of eij , i.e. e
b
ij is 1 if eij falls within bin b and 0 otherwise.
A single value of PAE per token-pair is obtained by taking the expectation across the 64 binned probabilities:
PAEij =
X
64
b=1
∆b p
b
ij (11)
where ∆b are the distance bin centers.
We also use the probabilities p
b
ij to compute pTM, an estimate of the TM-Score [30] as well as an ipTM, an interfacial
variant that only considers (i, j) pairs from different chains. For full description of pTM and ipTM we refer the reader
to Sec. 1.9.7 of [1] and Sec. 7.9 of [18], respectively. As previously, we find pTM and ipTM to be useful as confidence
metrics (main text Figure 4) and for sample ranking (subsection 5.9).
4.3.3 Predicted distance error (PDE)
In addition to an alignment error, the model is also trained to predict the error in absolute distances between atoms.
The pair embedding (zij ) in the confidence head (Algorithm 31) is linearly projected into 64 distance bins b, and
probabilities p
b
ij are obtained via a softmax. There are 64 bins equally spaced from 0 Å to 32 Å in 0.5 Å increments.
Supplementary information for AlphaFold 3 28
During training the final bin also captures larger errors. The prediction targets eij are defined as eij = |d
pred
ij − d
gt
ij |,
where d
pred
ij is the distance between representative token atoms i and j in the mini-rollout prediction (subsection 4.1),
and d
gt
ij is the corresponding distance in the ground truth. The loss is then defined as:
Lpde = −
1
N2
token
X
i,j
X
64
b=1
e
b
ij log p
b
ij (12)
Here e
b
ij denotes a binned version of eij , i.e. e
b
ij is 1 if eij falls within bin b and 0 otherwise.
A single value of PDE per token-pair is obtained by taking the expectation across the 64 binned probabilities.
PDEij =
X
64
b=1
∆b p
b
ij (13)
where ∆b are the distance bin centers.
4.3.4 Experimentally resolved prediction
The single embedding (si) in the confidence head is linearly projected into per-atom values (in the same way as subsubsection 4.3.1) with 2 bins. Then a softmax is used to predict whether the atom is resolved in the ground truth (yl), and
loss defined as:
Lresolved = −
1
Natom
X
l
X
2
b=1
y
b
l
log p
b
l
(14)
4.3.5 Confidence head architecture
The pLDDT, PAE, PDE and experimentally resolved outputs are projected from the same head, the architecture of this
head is outlined in Algorithm 31. The pairwise distance between the representative atoms for each token are extracted,
and combined with the single ({si}) and pair ({zij}) embeddings from the network trunk. lrep(i) provides the flat-atom
index l of the representative atom for the given token i. During training, a stop gradient is applied to the single and pair
embeddings and predicted structure.
Algorithm 31 Confidence head
def ConfidenceHead({s
inputs
i
}, {si}, {zij}, {⃗x
pred
l
}, Nblock = 4) :
1: zij += LinearNoBias(s
inputs
i
) + LinearNoBias(s
inputs
j
)
# Embed pair distances of representative atoms:
2: dij =


⃗x
pred
lrep(i) − ⃗x
pred
lrep(j)



3: zij += LinearNoBias(one_hot(dij , vbins = [3
3
⁄8 Å, 5
1
⁄8 Å, . . . , 213
⁄8 Å]))
4: {si}, {zij} += PairformerStack({si}, {zij}, Nblock)
5: p
pae
ij = softmax(LinearNoBias(zij )) p
pae
ij ∈ Rbpae
6: p
pde
ij = softmax(LinearNoBias(zij + zji)) p
pde
ij ∈ Rbpde
7: p
plddt
l = softmax(LinearNoBiastoken_atom_idx(l)
(si(l)
)) p
plddt
l ∈ Rbplddt
8: p
resolved
l = softmax(LinearNoBiastoken_atom_idx(l)
(si(l)
)) p
resolved
l ∈ R2
9: return {p
plddt
l
}, {p
pae
ij }, {p
pde
ij }, {p
resolved
l
}
4.4 Distogram prediction
The model is also trained to predict binned distances between all pairs of tokens (a distogram). This head and loss
are identical to AlphaFold 2 [1], where the pairwise token distances use the representative atom for each token: Cβ
for
protein residues (Cα
for glycine), C4 for purines and C2 for pyrimidines. All ligands already have a single atom per
token.
Supplementary information for AlphaFold 3 29
5 Training and inference
5.1 Structure filters
For both training and inference, standard crystallization agents (Table 9) are removed from the structure. Bonds for
structures with homomeric subcomplexes lacking the corresponding homomeric symmetry are also removed – e.g. if a
certain bonded ligand only exists for some of the symmetric copies, but not for all, we remove the corresponding bond
information from the input. In consequence the model has to learn to infer these bonds by itself.
5.2 Training stages
Training occurs in four stages with differences highlighted in Table 6. We train a randomly initialized model first with a
sequence crop size of 384, and then fine tune from the stage one weights at a crop size of 640, followed by a second fine
tuning stage with crop size 768. In the first stage of fine tuning, smooth lddt loss was turned off and the weight of the
disordered protein PDB distillation set was reduced, but now with unmasked diffusion loss on non-protein chains in that
distillation set. For the second stage of fine tuning, in addition, transcription factor distillation sets were turned on and
the weight of the disordered protein PDB distillation set was increased back to its original level. Protein residues from
the DNA motif distillation set with predicted probability of being experimentally resolved lower than 0.9 were treated
as unresolved residues. In the third, final fine-tuning stage, we trained the PAE head while removing all structure-based
losses (diffusion and distogram) from training and increased the maximum number of chains to 50. All other settings
were kept as in the previous fine-tuning stage.
The model trained for PoseBusters was trained similarly to the primary model, but without PDB structures released
after 2021-09-30, without RNA MSA or RNA distillation sets, and without the predicted experimentally resolved filter
on the DNA motif distillation set during fine tuning.
Table 6 Training stages
Initial training Fine tuning 1 Fine tuning 2 Fine tuning 3
Sequence crop size Ntoken 384 640 768 768
Parameters initialized from Random Initial training Fine tuning 1 Fine tuning 2
Sampling weight for disorder PDB distillation 0.02 0.01 0.02 0.02
Train on transcription factor distillation sets False False True True
Masked diffusion loss for non-protein in disorder
PDB distillation
True False False False
Train structure and distogram True True True False
Train PAE head False False False True
Diffusion batch size 48 32 32 32
Training samples (·106
) ≈20 ≈1.5 ≈1.5 ≈1.8
Training times (days on 256 A100s) ≈10 ≈3 ≈5 ≈2
Polymer-ligand bond loss weight 0 1 1 1
Max number of chains 20 20 20 50
5.3 Loss
The details of the losses from each module are given in the relevant sections, these are then combined into the final loss:
Lloss = αconfidence · (Lplddt + Lpde + Lresolved + αpae · Lpae) + αdiffusion · Ldiffusion + αdistogram · Ldistogram (15)
Where αconfidence = 10−4
, αdiffusion = 4, αdistogram = 3 · 10−2
and αpae = 0 for all except for the final training stage,
where it is set to 1.
5.4 Optimization
For training we use the Adam [31] optimizer with parameters β1 = 0.9, β2 = 0.95, ϵ = 10−8
. The base learning rate
is 1.8 · 10−3
, which is linearly increased from 0 over the first 1,000 steps. The learning rate is then decreased by a factor
of 0.95 every 5 · 104
steps.
The model is trained with a batch size of 256, and we apply gradient clipping if the global norm is greater than 10.
Supplementary information for AlphaFold 3 30
5.5 Dropout
Dropout is applied during training in the MsaModule (Algorithm 8), in the Pairformer stack (Algorithm 17) and in the
template embedder (Algorithm 16). See the respective algorithms for the dropout rates.
5.6 Inference Setup
For inference, we use an exponential moving average of the trained parameters [32] with the decay rate of 0.999.
Structures are inferenced without any cropping.
5.7 Model selection
For model selection during training stages, performance over a validation set was tracked during training, with all
structures in the validation set released after 2021-09-30 and before 2023-01-13 (the maximum release date of training
date structures was 2021-09-30). Details of the construction of the validation set are given in subsection 5.8.
A single model selection metric was computed from single chain, interface and full complex scores as follows:
1. Scores were computed for all five samples across all low homology chains and interfaces in the validation set
structures, as well as certain full complex metrics.
2. Scores were aggregated across samples via arithmetic mean of top1 (top-ranked) and top5 (best-of-5) predictions,
according to global PDE ranking:
gPDE =
P
ij pijPDEij
P
ij pij
, (16)
where the weight pij is the probability of contact of token pair i, j under the distogram (i.e. the sum of distogram
probabilities corresponding to distogram bins under 8 Å).
3. A final scalar was computed via a weighted mean across score types (see Table 7 for weights and description of
metrics below), averaged over all targets.
For each interface (chain pair) type and single chain type, LDDTs (local distance difference test) [33] were aggregated. For each pair of low homology chains, interface LDDT was calculated from distances between atoms across
different chains in the interface. Intra-chain LDDTs were calculated from distances within a single low homology
chain. Nucleic acid LDDTs (intra-chains and interface) were calculated with an inclusion radius of 30 Å compared to
the usual 15 Å used for proteins, due to the larger size of nucleotides compared to amino acids. An additional metric,
unresolved protein RASA (relative solvent accessible surface area) [34] was calculated for unresolved protein residues
in our evaluation set.
Table 7 Weighting of accuracy metrics for model selection
Metric Initial training Fine tuning
Protein-protein interface LDDT 20 20
DNA-protein interface LDDT 10 10
Protein-RNA interface LDDT 10 2
DNA-ligand interface LDDT 5 5
Ligand-protein interface LDDT 10 10
Ligand-RNA interface LDDT 5 2
Protein intra-chain LDDT 20 20
DNA intra-chain LDDT 4 4
RNA intra-chain LDDT 16 16
Ligand intra-chain LDDT 20 20
Modified residue intra-chain LDDT 10 0
Unresolved protein RASA 10 10
Supplementary information for AlphaFold 3 31
5.8 Validation set
The validation set for model selection during training was composed of a all low homology chains and interfaces from
a subset of all PDB targets released after 2021-09-30 and before 2023-01-13, with maximum length 2048 tokens.
The process for selecting these targets was broken up into two separate stages. The first was for selecting multimers,
the second for selecting monomers. Multimer selection proceeded as follows:
1. Take all targets released after 2021-09-30 and before 2023-01-13 and remove targets with total number of tokens
greater than 2560, more than one thousand chains or resolution greater than 4.5, then generate a list of all interface
chain pairs for all remaining targets.
2. Filter to only low homology interfaces, which are defined as those where no target in the training set contains
two chains with high homology to the chains involved in the interface, where high homology here means >
40% sequence identity for polymers or > 0.85 tanimoto similarity for ligands. Additionally filter out interfaces
involving a ligand with ranking model fit less than 0.5 or with multiple residues.
3. Assign interfaces to clusters as per subsubsection 2.5.3, other than for polymer-ligand interfaces which use cluster
ID (polymer_cluster, CCD-code) and sample one interface per cluster.
4. Take the following interface types only, possibly reducing number of clusters by sampling a subset of clusters
(number of samples given in brackets if reduced): protein-protein (600), protein-DNA (100), DNA-DNA (100),
Protein-ligand (600), DNA-ligand (50), ligand-ligand (200), protein-RNA, RNA-RNA, DNA-RNA, RNA-ligand.
5. Take the set of all PDB targets containing the remaining interfaces with a final additional restriction of max total
tokens 2048 and make the set of scored chains and interfaces equal to all low homology chains and interfaces in
those targets.
6. Manually exclude a small set of targets (11 in our case) where alignment for scoring took too long to be practical
for generating validation scores during experiments.
Monomer selection proceeded similarly:
1. Take all polymer monomer targets released after 2021-09-30 and before 2023-01-13 (can include monomer polymers with ligand chains) and remove targets with total number of tokens greater than 2560 or resolution greater
than 4.5
2. Filter to only low homology polymers.
3. Assign polymers to clusters as per subsubsection 2.5.3.
4. Sample 40 protein monomers and take all DNA and RNA monomers.
5. Add a final additional restriction of max total tokens 2048 and make the set of scored chains and interfaces equal
to all low homology chains and interfaces in the remaining targets.
6. Manually exclude a set of RNA monomers (8 in our case) that all come from one over represented cluster.
The end result was 1,220 PDB targets containing 2,333 low homology interfaces and 2,099 low homology chains.
5.9 Confidence measures and sample ranking
5.9.1 Alignment-based confidence measures
To capture confidence from an alignment perspective we employ pTM and ipTM, PAE-based predictors that have been
developed previously [1, 18]. The pTM is a predictor of TM-Score [30], an alignment-based measure of structural
accuracy (see Sec. 1.9.7 of [1] for original description). For a given set of tokens D, the pTM is computed as
pTM(D) = max
i∈D
has_frame(i)
1
|D|
X
j∈D
N
Xbins
b=1
p
b
ij


1
1 + 
∆b
d0(|D|)
2

 . (17)
Here p
b
ij is the PAE probability of error bin b for token j aligned to frame i, ∆b are the associated error bin centers, and
Nbins is the number of bins (see subsubsection 4.3.2 for PAE description). The has_frame notation indicates that invalid
Supplementary information for AlphaFold 3 32
frames are not considered when computing pTM (see subsubsection 4.3.2 for description of invalid frames). Finally,
the factor d0(N) = 1.24p3 maximum(N, 19) − 15 − 1.8 is the TM-score normalization constant.
With the above equation we can compute multiple confidence measures: for a whole-structure confidence we compute
a whole complex pTM, where D includes all tokens. For single chain confidence we compute a "chain pTM", where D
is restricted to tokens in the chain of interest.
The ipTM (originally described in [18]) is an interface variant of pTM that only considers interactions between
different chains,
ipTM(D) = max
i∈D
has_frame(i)
1
|D-chain(i)
|
X
j∈D-chain(i)
N
Xbins
b=1
p
b
ij


1
1 + 
∆b
d0(|D|)
2

 , (18)
where D-chain(i)
is the subset of D excluding all tokens in the chain of token i. We compute a full structure ipTM by
evaluating Equation 18 on a set D containing all tokens. We also compute a "chain pair ipTM" confidence, where D is
restricted to the set of tokens within a pair of chains defining the interface.
5.9.2 Clashes
We sometimes employ a clash penalty in ranking (see subsubsection 5.9.3), whereby structures with heavily clashing
polymer chains are down-weighted. The number of clashes between two chains is defined as
clashes(A, B) =
X
i∈A
X
j∈B
dij < 1.1 Å
where dij is the distance between atom i and atom j. A structure is marked as having a clash (has_clash) if for any
two polymer chains A, B in the prediction clashes(A, B) > 100 or clashes(A, B)/min(NA, NB) > 0.5 where NA is the
number of atoms in chain A. See subsubsection 5.9.3 for how this is used when ranking predictions.
5.9.3 Sample ranking
Sample ranking of the recent PDB evaluation set uses a combination of pTM, ipTM, and pLDDT-based summaries. The
ranking depends on whether the metric is single-chain (e.g. protein intra) or an interface (e.g. DNA-protein), as well as
on the chain types being scored:
1. Full complex ranking is similar to [18] according to a weighted average of the full-complex pTM and ipTM. We
also include terms that penalize predictions with lots of clashes, and very slightly up weight predictions with more
disorder (as defined by the relative solvent accessible surface area) to further reduce issues with hallucinations.
(subsubsection 5.9.1)
0.8 · ipTM + 0.2 · pTM + 0.5 · disorder − 100 · has_clash. (19)
Where disorder is defined as 1
NP
P
i∈P
(rasai > 0.581), P being the set of all protein atoms. The rasa metric,
and associated cutoff 0.581 were chosen based on previous work on predicting disorder with AlphaFold [34].
2. Ranking of single-chain metrics (e.g. protein intra) is according to the chain pTM (see subsubsection 5.9.1),
where the token subset is restricted to members of the chain.
3. Interface metrics (e.g. protein-protein) are ranked according to a bespoke ipTM aggregate representing the two
chains in the interface. We first compute a [Nchains, Nchains] matrix M of chain pair ipTM values (see subsubsection 5.9.1) for all chain pairs in the prediction. For each chain c ∈ [A, B] in our interface we then compute its
average interaction with all other chains: R(c) = mean
ij
(Mij ) restricted to i = c or j = c, i ̸= j, and chain i
having at least one valid frame. Finally, we rank according to
1
2

R(A) + R(B)

(20)
Interfaces containing a small molecule, ion, or bonded ligand chain are ranked differently. Denoting this chain as
C
∗
, the interface is ranked according to R(C
∗
) only.
4. Modified residue scores are ranked according to the average pLDDT of the modified residue.
Sample ranking during the model selection phase was performed differently from above. See subsection 5.7 for details.
Supplementary information for AlphaFold 3 33
5.10 Inference Time
Example inference times using 10 trunk recycles on 16 NVIDIA A100 GPUs are shown in Table 8. This includes only
GPU wallclock time, and MSA construction, data processing, compilation, and post-processing is not included.
Table 8 Inference time in seconds by complex size, using 16 A100
Number of tokens Inference time (seconds)
1024 22
2048 71
3072 126
4096 228
5120 347
6 Evaluation
6.1 Recent PDB evaluation set
The recent PDB evaluation set construction started by taking all 10,192 PDB entries released between 2022-05-01 and
2023-01-12, a date range falling after any data in our training set which had a maximum release date of 2021-09-30.
Each entry in the date range was expanded from the asymmetric unit to Biological Assembly 1, then two filters were
applied:
• Filtering to non-NMR entries with resolution better than 4.5 Å, leaving 9,636 complexes.
• Filtering to complexes with less than 5,120 tokens under our tokenization scheme (see subsection 2.6), leaving
8,856 complexes.
For each complex we generated a list of all individual entities and a list of all entity pairs where the minimal distance
between heavy atoms in the two entities was less than 5 Å and at least one entity in the pair was a polymer. The
procedures for determining homology and clusters for these entities and interfaces are described in later sections.
Predictions on the recent PDB set were made on the full post-assembly complex, but crystallization aids (Table 9)
were removed from the complex for prediction and scoring, along with all bonds for structures with homomeric subcomplexes lacking the corresponding homomeric symmetry.
Not every entity and interface was included:
• Peptide-peptide interfaces, peptide monomers and modified residues within peptides (where a peptide here is
defined as a protein with less than 16 residues) were not included in scoring as their homology to the training set
was not determined.
• The system can predict other entities like DNA/RNA hybrids, Peptide Nucleic Acids (PNA) and (D) polypeptides,
but these entities and interfaces involving them were not scored as they are too rare to get meaningful results on.
• Eight structures were removed from the test set as matching predicted chains to ground truth chains took too long,
due to large numbers of individual entities in the structure.
• Four structures were removed from the set for technical reasons - three where all chains had fewer than 4 residues
and one with bad metadata in the source file.
• Eleven ligand-protein or ion-protein interfaces failed to score due to RMSD calculation errors.
In addition to the full evaluation set described in the section above we create a “low homology” subset that is filtered
on homology to this training set.
Evaluation is done either on individual chains, or on specific interfaces extracted from the full complex prediction.
For intra-chain metrics, we keep polymers that have less than 40% sequence identity to the training set. Here we define
sequence identity as the percent of residues in the evaluation set chain that are identical to the training set chain. For
interface metrics the following filters are applied:
Supplementary information for AlphaFold 3 34
• Polymer-polymer interfaces: If both polymers have greater than 40% sequence identity to two chains in the same
complex in the training set, then this interface is filtered out.
• Peptide-polymer: For interfaces to a peptide (<16 residues), the similarity of the non-peptide entity has to be
novel (less than 40% sequence identity to anything in the training set).
6.2 Evaluation set clustering
The evaluation data was clustered to allow for redundancy reduction. Individual polymer chains were clustered at a
40% sequence similarity clustering for proteins with more than 9 residues and 100% similarity for nucleic acids and
protein with less than or equal to 9 residues. Ligands, ions, and metal entities were clustered according to CCD identity
(only used for Main Article Fig. 4). When assigning a cluster ID to an interface:
• Polymer-polymer interfaces are given a cluster ID of (polymer1_cluster, polymer2_cluster).
• Polymer-ligand interfaces are given a cluster ID of the polymer_cluster only.
• Polymer-modified_residue interfaces are given a cluster ID (polymer_cluster, CCD-code).
6.3 Evaluation metrics
The evaluation procedure compares a predicted structure to the corresponding ground truth structure. If the complex
contains multiple identical entities, the optimal assignment (maximising LDDT) of the predicted units to the ground
truth units is found by either an exhaustive search over all permutations (for groups up to 8 members) or a simulated annealing optimization (for larger groups). After the chain assignment is found, the assignment in local symmetry groups
of atoms in ligands is solved by exhaustive search over the first 1000 per-residue symmetries as given by RDKit [25].
We measure the quality of the predictions with DockQ, LDDT (local distance difference test) [33] or pocket-aligned
RMSD (root mean square deviation). For nucleic-protein interfaces we measure interface accuracy via interface LDDT
(iLDDT), which is calculated from distances between atoms across different chains in the interface. Nucleic acid
LDDTs (intra-chains and interface) were calculated with an inclusion radius of 30 Å compared to the usual 15 Å used
for proteins, owing to their larger scale.
If not stated differently, the pocket-aligned RMSD is computed as follows: the pocket is defined as all heavy atoms
within 10 Å of any heavy atom of the ligand in the ground truth structure. In ligand-protein interfaces the Cα
atoms
within the pocket are used to align the predicted structure to the ground truth structure by least squares rigid alignment,
and then RMSD is computed on all heavy atoms of the ligand. In ligand-nucleic chain interfaces all heavy atoms are
used for the alignment due to the larger spacing of the backbone atoms and the more rigid structure inside.
We used three categories of evaluation metrics:
1. Complex level metrics (e.g. full complex LDDT score). For these we used the prediction with the highest whole
complex confidence over the 5 model seeds.
2. Individual entity metrics (e.g. intra-chain LDDT). For these we used the prediction with the highest entity confidence for the entity being evaluated.
3. Interface metrics (e.g. pocket-aligned RMSD for ligand-protein interfaces). For these we used the prediction with
the highest interface confidence for the interface being evaluated.
6.4 Aggregation of scores
To avoid the overrepresentation of similar polymer chains or interfaces that were deposited under different PDB codes,
we cluster those together (see subsection 6.2) and aggregate the individual scores such that each cluster gets the same
weight. For mean values this is achieved by first computing the mean value per-cluster and then averaging over clusters.
For median values and other percentiles we weight each sample by 1/(Nci
), where Nci
is the size of the cluster the
sample belongs to, and compute a weighted percentile.
6.5 Baselines
As part of our ligand accuracy evaluation, we ran docking onto AlphaFold-Multimer v2.3 structures with AutoDock
Vina 1.1 [35, 36], using Gypsum-DL [37] and Propka [38] for ligand and protein preparation. The pocket used for
docking onto the AlphaFold structures was determined after a rigid alignment with the ground truth structures. We
Supplementary information for AlphaFold 3 35
verified the performance of our docking protocol by recovering the published accuracy on ground truth structures from
the PoseBusters set [39]. All other reported baseline numbers in the ligand section are from published literature.
For benchmarking performance on nucleic acid structure prediction, we report baseline comparisons to an existing
machine learning system for protein-nucleic acid and RNA tertiary structure prediction, RoseTTAFold2NA [40]. We
run the open source RF2NA [41] with the same multiple sequence alignments (MSAs) as were used for AlphaFold 3
predictions. For comparison between AlphaFold 3 and RF2NA, a subset of our recent PDB evaluation set targets are
chosen to meet the RF2NA criteria (<1000 total residues and nucleotides). Also as RF2NA was not trained to predict
systems with DNA and RNA, this analysis was limited to targets with only one nucleic acid type. Note, that AlphaFold
3 is capable of predicting systems with any combination of protein, DNA, RNA, and ligands.
As an additional baseline for RNA tertiary structure prediction, we evaluate AlphaFold 3 performance on CASP15
RNA targets that are currently publicly available (R1116/8S95, R1117/8FZA, R11263
, R1128/8BTZ, R1136/7ZJ4,
R1138/[7PTK/7PTL], R1189/7YR7, and R1190/7YR6). We compare top-1 ranked predictions, and where multiple
ground truth structures exist (R1136) the prediction is scored against the closest state. We display comparisons to
RF2NA as a representative machine learning system and AIchemy_RNA2 as the top performing entrant. Both the
RF2NA (CASP15 entry BAKER) and AIchemy_RNA2 predictions were downloaded from the CASP website.
Independent work on RoseTTAFold All-Atom [42] was concurrently released that performs structure prediction
across a wide range of biomolecular systems. This system is not available for baselining at the time of writing, but
the RoseTTAFold All-Atom paper indicates their accuracy is below specialist predictors in almost all categories. The
RoseTTAFold All-Atom PoseBusters benchmark score was included since it uses a comparable methodology.
7 Differences to AlphaFold 2 and AlphaFold-Multimer
AlphaFold 3 differs in a number of ways from the previous versions of AlphaFold. A non-exhaustive list of key differences is discussed below.
Centrally, AlphaFold 3 is capable of modeling general molecules; in order to achieve this, we changed the inputs to
allow specifying e.g. ligands and modified residues and the way coordinates are represented in the model.
In AlphaFold 2, the protein structure was internally represented by associating a rigid body frame to each amino acid
(relating the Cα
atoms) and the side chains were parameterized by χ-angles. These representations together were used
to deduce the coordinates of all the atoms in the amino acid.
That approach was handcrafted for proteins and does not generalize naturally to arbitrary molecules – therefore, in
AlphaFold 3 we instead model the system as a collection of atoms, which each have independent global coordinates
without any rigid constraints between them. The model learns to respect the chemical structure of the molecules as a
result of training, not because the output space is parameterized to enforce it.
In order to allow for efficient computation, we group atoms into tokens. For standard nucleic acids and standard
amino acids, tokens correspond to entire residues or nucleotides; for others, each token corresponds to a single heavy
atom.
Further, in contrast to AlphaFold 2 and much other recent work, the structure prediction part of the model is not
equivariant under spatial transformations. In fact, we employ very limited spatial inductive bias, simply embedding the
positions with a single matrix multiplication.
To predict the structure, we employ a generative diffusion-based head, that proceeds by simply adding Gaussian noise
to the atom coordinates and training the head to remove the noise, driven by a simple MSE loss. Other than a bond loss
to enforce the locations of bonded ligands and glycans (see Equation 5), no violation or clash losses are applied. The
relax postprocessing step is rarely needed when using AlphaFold 3.
While we no longer use rigid frames for the core prediction task, we do use frames for the PAE confidence head
(subsubsection 4.3.2). This frame construction is largely the same as AlphaFold 2, with exceptions that now we have
frames for non-protein entities as well and that the frames are constructed via a different combination of the atom
position vectors (compare Algorithm 28 to Algorithm 21 of [1]).
Furthermore, we simplified the core pieces of the architecture. For the main trunk of the model, we moved to a new
simplified architecture called “Pairformer” and we removed the MSA representation from the core part of the model
and instead replace it with a single representation. As a direct consequence, there is also no column-wise attention.
In Pairformer, we removed the outer product mean that the Evoformer had, meaning there is no information flow from
the single to the pair representation. We also simplified the Evoformer stack. The Evoformer stack from AlphaFold 2
operated over both the MSA and residue pairs, and is now replaced with a Pairformer stack that operates over just the
token pairs.
3Downloaded from https://predictioncenter.org/casp15/TARGETS_PDB/R1126.pdb
Supplementary information for AlphaFold 3 36
In AlphaFold-Multimer, unpaired MSA sequences across chains were presented to the network in a block-diagonal
fashion. In AlphaFold 3 these “unpaired” sequences are presented in a dense fashion - so that the network sees many
more MSA sequences per-chain.
Lastly, we replaced the ReLU activation in the model’s transition blocks function by SwiGLU, which has been
observed to yield better results [28]. ReLU is present in the atom attention (Algorithm 5).
When it comes to data, we introduced new distillation sets, and the data gets clustered and re-balanced at the interface
level, as opposed to just clustered at the individual chain level. For the protein monomer distillation set, we removed
the restrictions on the pLDDT being above 80 and the length of the protein being above 200 residues.
8 Supplemental Results
8.1 Selected examples
In Main Article Fig. 3 we highlight examples of predicted macromolecular complexes that are now possible with AF3,
spanning a range of targets and molecule types, varying in size, complexity, and function.
• Protein-RNA complex: Human 40S ribosomal subunit is a large complex with >7,000 residues, critical to the
initiation of human protein synthesis. eIF1A and eIF5B proteins remodel the complex to orient the Met-tRNAi
Met
into a conformation compatible with ribosomal subunit joining. AF3 is able to predict the full ≈7,600 residue
complex (Main Article Fig. 3a). The eIF1A protein interaction with the ribosomal subunit, as well as the eIF5B
intra-residue interactions, are modelled correctly. The complex is composed of a set of 43 chains, each of which
has high sequence identity to the training set, but represents a novel combination of them as a whole (PDB ID =
7TQL).
• PTM: Proteins harboring heparan sulfate (HS) chains play crucial roles in tissue homeostasis and signal transduction. Exostosin-like 3 (EXTL3) is a glycosyltransferase forming a homocomplex responsible for HS chain
synthesis initiation. EXTL3 has 9.4% sequence identity to the training set. AF3 correctly predicts (Main Article
Fig. 3b) the protein homocomplex and two N-glycans on each of the protomers (bonded glycan accuracy: 0.99 Å
mean pocked-aligned RMSD, where the pocket is defined only using residues from the bonded protein chain,
PDB ID = 7AU2).
• Antibody-antigen: Mesothelin (MSLN) is a cell-surface protein that is a popular target for antibody-based therapies. A new antibody binding to a section of the juxtamembrane region inhibits MSLN shedding and results in
more active CAR-T therapies. No similar sequences could be found for the antigen (residues 584-598 of MSLN),
the antibodies have 88% (Fab heavy chain) and 97% (Fab light chain) sequence identity to the training set. AF3
predicts (Main Article Fig. 3c) the antibody-antigen interaction to high accuracy (0.82 DockQ, PDB ID = 7U8C).
• Integral membrane protein: Wnt signalling is essential for adult tissue homeostasis, and requires palmitoleoylation by PORCN, an endoplasmic reticulum-resident membrane-bound O-acyltransferase. AF3 correctly predicts
(Main Article Fig. 3d) the PORCN complex with LGK974 and the WNT3A peptide, providing a structural
rationale for the mechanism of action of the clinical stage molecule (PDB ID = 7URD).
• Unique fold: Natural products containing an aziridine ring exhibit antitumor activities. The AziU3/U2 protein
complex catalyses the formation of aziridine rings at a novel catalytic site formed by a complex with a novel
fold. AF3 correctly predicts (Main Article Fig. 3e) this novel protein complex bound to the substrate (PDB ID =
7WUX).
• Allosteric site: PI5P4Kγ is a lipid kinase expressed to regulate cellular levels of the PI5P substrate and the
PI(4,5)P2 product, implicated in cancer and immunological disorders. AF3 correctly predicts (Main Article
Fig. 3f) the novel allosteric binding mode of a novel inhibitor (PDB ID = 7QIE).
In Extended Data Fig. 6 we considered ligand docking cases studies chosen from the PoseBusters set. Here we compare AF3 ligand pose predictions to the best Vina or Gold docking poses reported in the PoseBusters paper [39], using
the pocket-aligned ligand heavy atom RMSD (ligand RMSD). The three examples selected have AF3 ligand RMSD
< 2 Å, and the best docking ligand RMSD > 3 Å. We measure ligand Tanimoto similarity using RDKit v.2023_03_3
Morgan fingerprints (radius 2,2048 bits)
• Notum is a serine hydrolase enzyme that catalyses the delipidation of Wnt proteins, and a well-established drug
target. While Notum has 99% sequence identity to the training set, the ligand (ARUK3004556) is novel, originally
Supplementary information for AlphaFold 3 37
discovered by a virtual screen, not sharing the Murcko scaffold with any Notum ligand in the PDB, and maximum
Tanimoto similarity to any ligand in the training set less than 0.4. AF3 predicts the docked ligand (Extended Data
Fig. 6a) substantially better than both Gold and Vina docking tools as presented in the PoseBusters benchmark
(AF3 ligand RMSD = 1.0 Å, best docking ligand RMSD = 5.2 Å, PDB ID = 8BTI).
• An ethanolamine derivative, HEHEAA, is a potent effector of PipR-mediated gene regulation in plant-associated
bacteria. The first step in the response to HEHEAA is the binding to the AapF protein. It is a relatively novel
protein (28% sequence identity to the training set), while the maximum Tanimoto similarity of HEHEAA to any
ligand in the training set is 0.43. AF3 predicts the docked ligand (Extended Data Fig. 6b) substantially better
than the best docking algorithm (AF3 ligand RMSD = 1.4 Å, best docking ligand RMSD = 5.4 Å, PDB ID =
7KZ9).
• Galectin-3 is a protein that binds β-galactosides through carbohydrate-recognition domains, and a well-known
human drug target. This protein has 91% sequence identity to the training set, and while the ligand (compound
22) shares the core with a known scaffold, the phenyl-1,2,4 triazole side chain is novel. AF3 predicts a more
accurate ligand pose (Extended Data Fig. 6c) than the best docking algorithm (AF3 ligand RMSD = 0.30 Å,
Best docking ligand RMSD = 7.2 Å, PDB ID = 7XFA).
9 Appendix: CCD code and PDB ID tables
Table 9 Crystallization aids
SO4, GOL, EDO, PO4, ACT, PEG, DMS, TRS, PGE, PG4, FMT, EPE, MPD, MES, CD, IOD
Table 10 Ligand exclusion list
144, 15P, 1PE, 2F2, 2JC, 3HR, 3SY, 7N5, 7PE, 9JE, AAE, ABA, ACE, ACN, ACT, ACY, AZI, BAM, BCN, BCT, BDN, BEN, BME, BO3, BTB, BTC, BU1, C8E, CAD, CAQ, CBM, CCN, CIT, CL, CLR,
CM, CMO, CO3, CPT, CXS, D10, DEP, DIO, DMS, DN, DOD, DOX, EDO, EEE, EGL, EOH, EOX, EPE, ETF, FCY, FJO, FLC, FMT, FW5, GOL, GSH, GTT, GYF, HED, IHP, IHS, IMD, IOD, IPA, IPH,
LDA, MB3, MEG, MES, MLA, MLI, MOH, MPD, MRD, MSE, MYR, N, NA, NH2, NH4, NHE, NO3, O4B, OHE, OLA, OLC, OMB, OME, OXA, P6G, PE3, PE4, PEG, PEO, PEP, PG0, PG4, PGE, PGR,
PLM, PO4, POL, POP, PVO, SAR, SCN, SEO, SEP, SIN, SO4, SPD, SPM, SR, STE, STO, STU, TAR, TBU, TME, TPO, TRS, UNK, UNL, UNX, UPL, URE
Table 11 CCD codes defining glycans
045, 05L, 07E, 07Y, 08U, 09X, 0BD, 0H0, 0HX, 0LP, 0MK, 0NZ, 0UB, 0V4, 0WK, 0XY, 0YT, 10M, 12E, 145, 147, 149, 14T, 15L, 16F, 16G, 16O, 17T, 18D, 18O, 1CF, 1FT, 1GL, 1GN, 1LL, 1S3, 1S4,
1SD, 1X4, 20S, 20X, 22O, 22S, 23V, 24S, 25E, 26O, 27C, 289, 291, 293, 2DG, 2DR, 2F8, 2FG, 2FL, 2GL, 2GS, 2H5, 2HA, 2M4, 2M5, 2M8, 2OS, 2WP, 2WS, 32O, 34V, 38J, 3BU, 3DO, 3DY, 3FM,
3GR, 3HD, 3J3, 3J4, 3LJ, 3LR, 3MG, 3MK, 3R3, 3S6, 3SA, 3YW, 40J, 42D, 445, 44S, 46D, 46Z, 475, 48Z, 491, 49A, 49S, 49T, 49V, 4AM, 4CQ, 4GC, 4GL, 4GP, 4JA, 4N2, 4NN, 4QY, 4R1, 4RS,
4SG, 4UZ, 4V5, 50A, 51N, 56N, 57S, 5GF, 5GO, 5II, 5KQ, 5KS, 5KT, 5KV, 5L3, 5LS, 5LT, 5MM, 5N6, 5QP, 5SP, 5TH, 5TJ, 5TK, 5TM, 61J, 62I, 64K, 66O, 6BG, 6C2, 6DM, 6GB, 6GP, 6GR, 6K3,
6KH, 6KL, 6KS, 6KU, 6KW, 6LA, 6LS, 6LW, 6MJ, 6MN, 6PZ, 6S2, 6UD, 6YR, 6ZC, 73E, 79J, 7CV, 7D1, 7GP, 7JZ, 7K2, 7K3, 7NU, 83Y, 89Y, 8B7, 8B9, 8EX, 8GA, 8GG, 8GP, 8I4, 8LR, 8OQ, 8PK,
8S0, 8YV, 95Z, 96O, 98U, 9AM, 9C1, 9CD, 9GP, 9KJ, 9MR, 9OK, 9PG, 9QG, 9S7, 9SG, 9SJ, 9SM, 9SP, 9T1, 9T7, 9VP, 9WJ, 9WN, 9WZ, 9YW, A0K, A1Q, A2G, A5C, A6P, AAL, ABD, ABE, ABF,
ABL, AC1, ACR, ACX, ADA, AF1, AFD, AFO, AFP, AGL, AH2, AH8, AHG, AHM, AHR, AIG, ALL, ALX, AMG, AMN, AMU, AMV, ANA, AOG, AQA, ARA, ARB, ARI, ARW, ASC, ASG, ASO, AXP, AXR,
AY9, AZC, B0D, B16, B1H, B1N, B2G, B4G, B6D, B7G, B8D, B9D, BBK, BBV, BCD, BDF, BDG, BDP, BDR, BEM, BFN, BG6, BG8, BGC, BGL, BGN, BGP, BGS, BHG, BM3, BM7, BMA, BMX, BND,
BNG, BNX, BO1, BOG, BQY, BS7, BTG, BTU, BW3, BWG, BXF, BXP, BXX, BXY, BZD, C3B, C3G, C3X, C4B, C4W, C5X, CBF, CBI, CBK, CDR, CE5, CE6, CE8, CEG, CEZ, CGF, CJB, CKB, CKP,
CNP, CR1, CR6, CRA, CT3, CTO, CTR, CTT, D1M, D5E, D6G, DAF, DAG, DAN, DDA, DDL, DEG, DEL, DFR, DFX, DG0, DGO, DGS, DGU, DJB, DJE, DK4, DKX, DKZ, DL6, DLD, DLF, DLG, DNO,
DO8, DOM, DPC, DQR, DR2, DR3, DR5, DRI, DSR, DT6, DVC, DYM, E3M, E5G, EAG, EBG, EBQ, EEN, EEQ, EGA, EMP, EMZ, EPG, EQP, EQV, ERE, ERI, ETT, EUS, F1P, F1X, F55, F58, F6P,
F8X, FBP, FCA, FCB, FCT, FDP, FDQ, FFC, FFX, FIF, FK9, FKD, FMF, FMO, FNG, FNY, FRU, FSA, FSI, FSM, FSW, FUB, FUC, FUD, FUF, FUL, FUY, FVQ, FX1, FYJ, G0S, G16, G1P, G20, G28,
G2F, G3F, G3I, G4D, G4S, G6D, G6P, G6S, G7P, G8Z, GAA, GAC, GAD, GAF, GAL, GAT, GBH, GC1, GC4, GC9, GCB, GCD, GCN, GCO, GCS, GCT, GCU, GCV, GCW, GDA, GDL, GE1, GE3, GFP,
GIV, GL0, GL1, GL2, GL4, GL5, GL6, GL7, GL9, GLA, GLC, GLD, GLF, GLG, GLO, GLP, GLS, GLT, GM0, GMB, GMH, GMT, GMZ, GN1, GN4, GNS, GNX, GP0, GP1, GP4, GPH, GPK, GPM, GPO,
GPQ, GPU, GPV, GPW, GQ1, GRF, GRX, GS1, GS9, GTK, GTM, GTR, GU0, GU1, GU2, GU3, GU4, GU5, GU6, GU8, GU9, GUF, GUL, GUP, GUZ, GXL, GXV, GYE, GYG, GYP, GYU, GYV, GZL,
H1M, H1S, H2P, H3S, H53, H6Q, H6Z, HBZ, HD4, HNV, HNW, HSG, HSH, HSJ, HSQ, HSX, HSY, HTG, HTM, HVC, IAB, IDC, IDF, IDG, IDR, IDS, IDU, IDX, IDY, IEM, IN1, IPT, ISD, ISL, ISX, IXD,
J5B, JFZ, JHM, JLT, JRV, JSV, JV4, JVA, JVS, JZR, K5B, K99, KBA, KBG, KD5, KDA, KDB, KDD, KDE, KDF, KDM, KDN, KDO, KDR, KFN, KG1, KGM, KHP, KME, KO1, KO2, KOT, KTU, L0W, L1L,
L6S, L6T, LAG, LAH, LAI, LAK, LAO, LAT, LB2, LBS, LBT, LCN, LDY, LEC, LER, LFC, LFR, LGC, LGU, LKA, LKS, LM2, LMO, LNV, LOG, LOX, LRH, LTG, LVO, LVZ, LXB, LXC, LXZ, LZ0, M1F, M1P,
M2F, M3M, M3N, M55, M6D, M6P, M7B, M7P, M8C, MA1, MA2, MA3, MA8, MAB, MAF, MAG, MAL, MAN, MAT, MAV, MAW, MBE, MBF, MBG, MCU, MDA, MDP, MFB, MFU, MG5, MGC, MGL, MGS,
MJJ, MLB, MLR, MMA, MN0, MNA, MQG, MQT, MRH, MRP, MSX, MTT, MUB, MUR, MVP, MXY, MXZ, MYG, N1L, N3U, N9S, NA1, NAA, NAG, NBG, NBX, NBY, NDG, NFG, NG1, NG6, NGA, NGC,
NGE, NGK, NGR, NGS, NGY, NGZ, NHF, NLC, NM6, NM9, NNG, NPF, NSQ, NT1, NTF, NTO, NTP, NXD, NYT, OAK, OI7, OPM, OSU, OTG, OTN, OTU, OX2, P53, P6P, P8E, PA1, PAV, PDX, PH5,
PKM, PNA, PNG, PNJ, PNW, PPC, PRP, PSG, PSV, PTQ, PUF, PZU, QDK, QIF, QKH, QPS, QV4, R1P, R1X, R2B, R2G, RAE, RAF, RAM, RAO, RB5, RBL, RCD, RER, RF5, RG1, RGG, RHA, RHC,
RI2, RIB, RIP, RM4, RP3, RP5, RP6, RR7, RRJ, RRY, RST, RTG, RTV, RUG, RUU, RV7, RVG, RVM, RWI, RY7, RZM, S7P, S81, SA0, SCG, SCR, SDY, SEJ, SF6, SF9, SFU, SG4, SG5, SG6, SG7,
SGA, SGC, SGD, SGN, SHB, SHD, SHG, SIA, SID, SIO, SIZ, SLB, SLM, SLT, SMD, SN5, SNG, SOE, SOG, SOL, SOR, SR1, SSG, SSH, STW, STZ, SUC, SUP, SUS, SWE, SZZ, T68, T6D, T6P,
T6T, TA6, TAG, TCB, TDG, TEU, TF0, TFU, TGA, TGK, TGR, TGY, TH1, TM5, TM6, TMR, TMX, TNX, TOA, TOC, TQY, TRE, TRV, TS8, TT7, TTV, TU4, TUG, TUJ, TUP, TUR, TVD, TVG, TVM, TVS,
TVV, TVY, TW7, TWA, TWD, TWG, TWJ, TWY, TXB, TYV, U1Y, U2A, U2D, U63, U8V, U97, U9A, U9D, U9G, U9J, U9M, UAP, UBH, UBO, UDC, UEA, V3M, V3P, V71, VG1, VJ1, VJ4, VKN, VTB,
W9T, WIA, WOO, WUN, WZ1, WZ2, X0X, X1P, X1X, X2F, X2Y, X34, X6X, X6Y, XDX, XGP, XIL, XKJ, XLF, XLS, XMM, XS2, XXM, XXR, XXX, XYF, XYL, XYP, XYS, XYT, XYZ, YDR, YIO, YJM, YKR,
YO5, YX0, YX1, YYB, YYH, YYJ, YYK, YYM, YYQ, YZ0, Z0F, Z15, Z16, Z2D, Z2T, Z3K, Z3L, Z3Q, Z3U, Z4K, Z4R, Z4S, Z4U, Z4V, Z4W, Z4Y, Z57, Z5J, Z5L, Z61, Z6H, Z6J, Z6W, Z8H, Z8T, Z9D,
Z9E, Z9H, Z9K, Z9L, Z9M, Z9N, Z9W, ZB0, ZB1, ZB2, ZB3, ZCD, ZCZ, ZD0, ZDC, ZDO, ZEE, ZEL, ZGE, ZMR
Table 12 Ions
118, 119, 1AL, 1CU, 2FK, 2HP, 2OF, 3CO, 3MT, 3NI, 3OF, 4MO, 4PU, 4TI, 543, 6MO, AG, AL, ALF, AM, ATH, AU, AU3, AUC, BA, BEF, BF4, BO4, BR, BS3, BSY, CA, CAC, CD, CD1, CD3, CD5, CE,
CF, CHT, CO, CO5, CON, CR, CS, CSB, CU, CU1, CU2, CU3, CUA, CUZ, CYN, DME, DMI, DSC, DTI, DY, E4N, EDR, EMC, ER3, EU, EU3, F, FE, FE2, FPO, GA, GD3, GEP, HAI, HG, HGC, HO3,
IN, IR, IR3, IRI, IUM, K, KO4, LA, LCO, LCP, LI, LU, MAC, MG, MH2, MH3, MMC, MN, MN3, MN5, MN6, MO, MO1, MO2, MO3, MO4, MO5, MO6, MOO, MOS, MOW, MW1, MW2, MW3, NA2, NA5,
NA6, NAO, NAW, NET, NI, NI1, NI2, NI3, NO2, NRU, O4M, OAA, OC1, OC2, OC3, OC4, OC5, OC6, OC7, OC8, OCL, OCM, OCN, OCO, OF1, OF2, OF3, OH, OS, OS4, OXL, PB, PBM, PD, PER,
PI, PO3, PR, PT, PT4, PTN, RB, RH3, RHD, RU, SB, SE4, SEK, SM, SMO, SO3, T1A, TB, TBA, TCN, TEA, TH, THE, TL, TMA, TRA, V, VN3, VO4, W, WO5, Y1, YB, YB2, YH, YT3, ZCM, ZN, ZN2,
ZN3, ZNO, ZO3, ZR
Supplementary information for AlphaFold 3 38
Table 13 Standard residues
ALA, ARG, ASN, ASP, CYS, GLN, GLU, GLY, HIS, ILE, LEU, LYS, MET, PHE, PRO, SER, THR, TRP, TYR, VAL, UNK, A, G, C, U, DA, DG, DC, DT, N, DN
Table 14 Recent PDB test set with nucleic acid complexes
7B0C, 7BCA, 7BJQ, 7EDS, 7EOF, 7F3J, 7F8Z, 7F9H, 7M4L, 7MKT, 7MWH, 7MZ0, 7MZ1, 7MZ2, 7N5U, 7N5V, 7N5W, 7NQF, 7NRP, 7OGS, 7OOO, 7OOS, 7OOT, 7OUE, 7OWF, 7OY7, 7OZZ, 7P0W,
7P3F, 7P8L, 7P9J, 7P9Z, 7PSX, 7PTQ, 7PZA, 7PZB, 7Q3O, 7Q4N, 7Q94, 7QAZ, 7QP2, 7R6R, 7R6T, 7R8G, 7R8H, 7R8I, 7RCC, 7RCD, 7RCE, 7RCF, 7RCG, 7RCU, 7RGU, 7RSR, 7RSS, 7S03,
7S68, 7S9J, 7S9K, 7S9L, 7S9M, 7S9N, 7S9O, 7S9P, 7S9Q, 7SOP, 7SOS, 7SOT, 7SOU, 7SOV, 7SOW, 7SUM, 7SUV, 7SVB, 7SX5, 7SXE, 7T18, 7T19, 7T1A, 7T1B, 7T8K, 7TDW, 7TDX, 7TEA,
7TEC, 7TO1, 7TO2, 7TQW, 7TUV, 7TXC, 7TZ1, 7TZR, 7TZS, 7TZT, 7TZU, 7TZV, 7U76, 7U79, 7U7A, 7U7B, 7U7C, 7U7F, 7U7G, 7U7I, 7U7J, 7U7K, 7U7L, 7UBL, 7UBU, 7UCR, 7UPZ, 7UQ6, 7UR5,
7URI, 7URM, 7UU4, 7UXD, 7UZ0, 7V2Z, 7VE5, 7VFT, 7VG8, 7VKI, 7VKL, 7VN2, 7VNV, 7VNW, 7VO9, 7VOU, 7VOV, 7VOX, 7VP1, 7VP2, 7VP3, 7VP4, 7VP5, 7VP7, 7VSJ, 7VTI, 7WM3, 7WQ5,
7X5E, 7X5F, 7X5G, 7X5L, 7X5M, 7XHV, 7XI3, 7XQ5, 7XRC, 7XS4, 7YHO, 7YZE, 7YZF, 7YZG, 7Z0U, 7Z5A, 7ZHH, 7ZVN, 7ZVX, 8A1C, 8A4I, 8AMG, 8AMI, 8AMJ, 8AMK, 8AML, 8AMM, 8AMN,
8B0R, 8CSH, 8CTZ, 8CU0, 8CZQ, 8D28, 8D2A, 8D2B, 8D5L, 8D5O, 8DVP, 8DVR, 8DVS, 8DVU, 8DVY, 8DW0, 8DW1, 8DW4, 8DW8, 8DWM, 8DZK, 8E2P, 8E2Q, 8EDJ, 8EF9, 8EFC, 8EFK, 8GMS,
8GMT, 8GMU
Table 15 PoseBusters V2 Common Natural Ligands
2BA, 5AD, A3P, ACP, ADP, AKG, ANP, APC, APR, ATP, BCN, BDP, BGC, C5P, CDP, CTP, DGL, DSG, F15, FAD, FDA, FMN, GSH, GSP, GTP, H4B, IPE, MFU, MTA, MTE, NAD, NAI, NCA, NGA,
OGA, PGA, PHO, PJ8, PLG, PLP, PRP, SAH, SFG, SIN, SLB, TPP, UD1, UDP, UPG, URI
References
[1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction
with AlphaFold. Nature, 596(7873):583–589, 2021.
[2] Jimmy Ba, Jamie R Kiros, and Geoffrey E Hinton. Layer Normalization. arXiv preprint arXiv:1607.06450, 2016.
[3] H M Berman. The protein data bank. Nucleic Acids Res., 28(1):235–242, January 2000.
[4] Baris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProt Consortium. Uniref
clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics,
31(6):926–932, 2015.
[5] L Steven Johnson, Sean R Eddy, and Elon Portugaly. Hidden markov model speed heuristic and iterative hmm
search procedure. BMC Bioinformatics, 11(1):1–8, 2010.
[6] The UniProt Consortium. Uniprot: the universal protein knowledgebase in 2023. Nucleic Acids Research,
51(D1):D523–D531, 2023.
[7] Milot Mirdita, Lars Von Den Driesch, Clovis Galiez, Maria J Martin, Johannes Söding, and Martin Steinegger.
Uniclust databases of clustered and deeply annotated protein sequences and alignments. Nucleic acids research,
45(D1):D170–D176, 2017.
[8] Martin Steinegger, Milot Mirdita, and Johannes Söding. Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold. Nature methods, 16(7):603–606, 2019.
[9] Michael Remmert, Andreas Biegert, Andreas Hauser, and Johannes Söding. Hhblits: lightning-fast iterative protein sequence searching by hmm-hmm alignment. Nature Methods, 9(2):173–175, 2012.
[10] Kathryn Tunyasuvunakool, Jonas Adler, Zachary Wu, Tim Green, Michal Zielinski, Augustin Žídek, Alex Bridgland, Andrew Cowie, Clemens Meyer, Agata Laydon, et al. Highly accurate protein structure prediction for the
human proteome. Nature, 596(7873):590–596, 2021.
[11] Alex L Mitchell, Alexandre Almeida, Martin Beracochea, Miguel Boland, Josephine Burgin, Guy Cochrane,
Michael R Crusoe, Varsha Kale, Simon C Potter, Lorna J Richardson, Ekaterina Sakharova, Maxim Scheremetjew, Anton Korobeynikov, Alex Shlemov, Olga Kunyavskaya, Alla Lapidus, and Robert D Finn. MGnify: the
microbiome analysis resource in 2020. Nucleic Acids Research, 48(D1):D570–D578, 11 2019.
[12] Sean R Eddy. Accelerated profile hmm searches. PLoS computational biology, 7(10):e1002195, 2011.
[13] Ioanna Kalvari, Eric P Nawrocki, Nancy Ontiveros-Palacios, Joanna Argasinska, Kevin Lamkiewicz, Manja Marz,
Sam Griffiths-Jones, Claire Toffano-Nioche, Daniel Gautheret, Zasha Weinberg, et al. Rfam 14: expanded coverage of metagenomic, viral and microrna families. Nucleic Acids Research, 49(D1):D192–D200, 2021.
[14] Martin Steinegger and Johannes Söding. Clustering huge protein sequence sets in linear time. Nature Communications, 9(1):1–8, 2018.
Supplementary information for AlphaFold 3 39
[15] Travis J Wheeler and Sean R Eddy. nhmmer: Dna homology search with profile hmms. Bioinformatics,
29(19):2487–2489, 2013.
[16] RNAcentral Consortium. Rnacentral 2021: secondary structure integration, improved sequence search and new
member databases. Nucleic acids research, 49(D1):D212–D220, 2021.
[17] Eric W Sayers, Evan E Bolton, J Rodney Brister, Kathi Canese, Jessica Chan, Donald C Comeau, Catherine M
Farrell, Michael Feldgarden, Anna M Fine, Kathryn Funk, et al. Database resources of the national center for
biotechnology information in 2023. Nucleic acids research, 51(D1):D29–D38, 2023.
[18] Richard Evans, Michael O’Neill, Alexander Pritzel, Natasha Antropova, Andrew Senior, Tim Green, Augustin
Žídek, Russ Bates, Sam Blackwell, Jason Yim, et al. Protein complex prediction with AlphaFold-Multimer.
bioRxiv preprint bioRxiv:10.1101/2021.10.04.463034, 2021.
[19] Timo Lassmann, Oliver Frings, and Erik L L Sonnhammer. Kalign2: high-performance multiple alignment of
protein and nucleotide sequences allowing external features. Nucleic Acids Research, 37:858–865, 2009.
[20] Jaime A Castro-Mondragon, Rafael Riudavets-Puig, Ieva Rauluseviciute, Roza Berhanu Lemma, Laura Turchi,
Romain Blanc-Mathieu, Jeremy Lucas, Paul Boddie, Aziz Khan, Nicolás Manosalva Pérez, et al. Jaspar 2022:
the 9th release of the open-access database of transcription factor binding profiles. Nucleic acids research,
50(D1):D165–D173, 2022.
[21] Yimeng Yin, Ekaterina Morgunova, Arttu Jolma, Eevi Kaasinen, Biswajyoti Sahu, Syed Khund-Sayeed,
Pratyush K Das, Teemu Kivioja, Kashyap Dave, Fan Zhong, et al. Impact of cytosine methylation on dna binding
specificities of human transcription factors. Science, 356(6337):eaaj2239, 2017.
[22] Arttu Jolma, Yimeng Yin, Kazuhiro R Nitta, Kashyap Dave, Alexander Popov, Minna Taipale, Martin Enge,
Teemu Kivioja, Ekaterina Morgunova, and Jussi Taipale. Dna-dependent formation of transcription factor pairs
alters their binding specificity. Nature, 527(7578):384–388, 2015.
[23] Alexey G Murzin, Steven E Brenner, Tim Hubbard, and Cyrus Chothia. SCOP: a structural classification of
proteins database for the investigation of sequences and structures. Journal of molecular biology, 247(4):536–
540, 1995.
[24] Peter JA Cock, Tiago Antao, Jeffrey T Chang, Brad A Chapman, Cymon J Cox, Andrew Dalke, Iddo Friedberg,
Thomas Hamelryck, Frank Kauff, Bartek Wilczynski, et al. Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics, 25(11):1422, 2009.
[25] Rdkit: Open-source cheminformatics software. https://www.rdkit.org/.
[26] Shuzhe Wang, Jagna Witek, Gregory A Landrum, and Sereina Riniker. Improving conformer generation for
small rings and macrocycles based on distance geometry and experimental torsional-angle preferences. Journal of
chemical information and modeling, 60(4):2044–2058, 2020.
[27] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748,
2023.
[28] Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020.
[29] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the Design Space of Diffusion-Based
Generative Models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances
in Neural Information Processing Systems, volume 35, pages 26565–26577. Curran Associates, Inc., 2022.
[30] Yang Zhang and Jeffrey Skolnick. Scoring function for automated assessment of protein structure template quality.
Proteins: Structure, Function, and Bioinformatics, 57(4):702–710, 2004.
[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, 2015.
[32] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control
and Optimization, 30(4):838–855, July 1992.
Supplementary information for AlphaFold 3 40
[33] Valerio Mariani, Marco Biasini, Alessandro Barbato, and Torsten Schwede. lddt: A local superposition-free score
for comparing protein structures and models using distance difference tests. Bioinformatics, 29(21):2722–2728,
2013.
[34] Damiano Piovesan, Alexander Miguel Monzon, and Silvio C. E. Tosatto. Intrinsic protein disorder and conditional
folding in AlphaFoldDB. Protein Science, 31(11), October 2022.
[35] Jerome Eberhardt, Diogo Santos-Martins, Andreas F. Tillack, and Stefano Forli. Autodock vina 1.2.0: New
docking methods, expanded force field, and python bindings. Journal of Chemical Information and Modeling,
61(8):3891–3898, Aug 2021.
[36] Oleg Trott and Arthur J. Olson. AutoDock vina: Improving the speed and accuracy of docking with a new scoring
function, efficient optimization, and multithreading. Journal of Computational Chemistry, 31(2):455–461, June
2009.
[37] Patrick J. Ropp, Jacob O. Spiegel, Jennifer L. Walker, Harrison Green, Guillermo A. Morales, Katherine A.
Milliken, John J. Ringe, and Jacob D. Durrant. Gypsum-dl: an open-source program for preparing small-molecule
libraries for structure-based virtual screening. Journal of Cheminformatics, 11(1):34, May 2019.
[38] Mats H. M. Olsson, Chresten R. Søndergaard, Michal Rostkowski, and Jan H. Jensen. Propka3: Consistent treatment of internal and surface residues in empirical pka predictions. Journal of Chemical Theory and Computation,
7(2):525–537, Feb 2011.
[39] Martin Buttenschoen, Garrett M. Morris, and Charlotte M. Deane. Posebusters: AI-based docking methods fail to
generate physically valid poses or generalise to novel sequences. arXiv preprint arXiv:2308.05777, 2023.
[40] Minkyung Baek, Ryan McHugh, Ivan Anishchenko, David Baker, and Frank DiMaio. Accurate prediction of nucleic acid and protein-nucleic acid complexes using RoseTTAFoldNA. bioRxiv preprint
bioRxiv:10.1101/2022.09.09.507333, 2022.
[41] Frank DiMaio, Blake Riley, and Alex Morehead. RoseTTAFold2NA, April 2023.
[42] Rohith Krishna, Jue Wang, Woody Ahern, Pascal Sturmfels, Preetham Venkatesh, Indrek Kalvet, Gyu Rie Lee,
Felix S. Morey-Burrows, Ivan Anishchenko, Ian R. Humphreys, Ryan McHugh, Dionne Vafeados, Xinting Li,
George A. Sutherland, Andrew Hitchcock, C. Neil Hunter, Alex Kang, Evans Brackenbrough, Asim K. Bera,
Minkyung Baek, Frank DiMaio, and David Baker. Generalized biomolecular modeling and design with rosettafold
all-atom. Science, March 2024.